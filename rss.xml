<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>CodingLabs</title>
        <link>http://blog.codinglabs.org</link>
        <description>keep coding, keep foolish</description>
        <lastBuildDate>Mon, 27 May 2013 00:00:00 GMT</lastBuildDate>
        <language>zh-cn</language>
        <item>
<title>时间序列分析基础</title>
<link>http://blog.codinglabs.org/articles/time-series-analysis-foundation.html</link>
<guid>http://blog.codinglabs.org/articles/time-series-analysis-foundation.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Mon, 27 May 2013 00:00:00 GMT</pubDate>
<description>&lt;p&gt;时间序列是现实生活中经常会碰到的数据形式。例如北京市连续一年的日平均气温、某股票的股票价格、淘宝上某件商品的日销售件数等等。时间序列分析的的目的是挖掘时间序列中隐含的信息与模式，并借此对此序列数据进行评估以及对系列的后续走势进行预测。&lt;/p&gt;

&lt;p&gt;由于工作需要，我最近简单学习了时间序列分析相关的基础理论和应用方法，这篇文章可以看做是我的学习笔记。&lt;/p&gt;

&lt;p&gt;文章主要内容会首先描述时间序列分析的基本概念和相关的统计学基础理论，然后着重讲述十分经典和常用的ARIMA模型，在这之后会讲述季节ARIMA模型、干预分析和存在协变量时间序列的分析，最后讲述用于发现隐藏周期性的谱分析基础。&lt;/p&gt;

&lt;p&gt;由于打算以学习笔记的形式写这篇文章，所以我不会一下子写完整篇文章才发布，而是持续更新这篇文章，写的过程中也可能会对前面的内容进行修订。&lt;/p&gt;

&lt;p&gt;文章中会穿插许多实例（兼有模拟数据和数据分析），分析过程中将使用&lt;a href=&quot;http://www.r-project.org/&quot;&gt;R&lt;/a&gt;为分析工具。&lt;/p&gt;</description>
</item>
<item>
<title>算法分析中递推式的一般代数解法</title>
<link>http://blog.codinglabs.org/articles/linear-algebra-for-recursion.html</link>
<guid>http://blog.codinglabs.org/articles/linear-algebra-for-recursion.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Sun, 17 Mar 2013 00:00:00 GMT</pubDate>
<description>&lt;p&gt;算法分析中经常遇到需要求解递推式的情况，即将递推式改写为等价的封闭形式。例如&lt;a href=&quot;http://en.wikipedia.org/wiki/Tower_of_Hanoi&quot;&gt;汉诺塔问题&lt;/a&gt;的时间复杂度递推形式为\(T(n)=2T(n-1)+1 \quad (n \geq 1)\)，可以解出封闭形式为\(T(n)=2^n-1\)（设初始状态\(T(0)=0\)）。&lt;/p&gt;

&lt;p&gt;因为递推式求解的重要性，许多算法书籍对其有专门介绍。&lt;a href=&quot;http://en.wikipedia.org/wiki/Donald_Knuth&quot;&gt;Donald Knuth&lt;/a&gt;在&lt;a href=&quot;http://en.wikipedia.org/wiki/Concrete_Mathematics&quot;&gt;Concrete Mathematics&lt;/a&gt;一书中多个章节都涉及递推式求解方法。&lt;a href=&quot;http://book.douban.com/subject/1885170/&quot;&gt;算法导论&lt;/a&gt;也在第四章中专门论述的这个主题。&lt;/p&gt;

&lt;p&gt;在这些相关论述中，主要介绍了一些启发式方法，这些方法往往需要一些特殊的技巧和灵感才能完成。&lt;/p&gt;

&lt;p&gt;而本文将论述一种纯代数式的方法，这种方法将求解递推式转化为求解一个多项式的根和求解一组线性方程组，这样就使得整个求解过程不依赖于太多技巧，因此具有更好的易用性。&lt;/p&gt;

&lt;p&gt;本文首先会给出两个例子：如何使用纯代数方法求解斐波那契数列和汉诺塔递推式；然后会借助线性代数论述这种方法背后的数学意义，说明线性递推式与线性方程的内在联系以及这种解法的数学原理；最后将例子中的方法推广到一般情况。&lt;/p&gt;

&lt;h1&gt;示例&lt;/h1&gt;

&lt;h2&gt;例1：斐波那契数列&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Fibonacci_number&quot;&gt;斐波那契数列&lt;/a&gt;大家应该很熟悉了，这里不再赘述，直接进入问题。&lt;/p&gt;

&lt;h3&gt;问题&lt;/h3&gt;

&lt;p&gt;设斐波那契数列为由如下递推式定义的数列：&lt;/p&gt;

&lt;p&gt;\[\begin{array}{l l l}
T(0) &amp; = &amp; 0 \\
T(1) &amp; = &amp; 1 \\
T(n) &amp; = &amp; T(n-2)+T(n-1) \quad (n \geq 2)
\end{array}\]&lt;/p&gt;

&lt;p&gt;求解\(T(n)\)的封闭形式（也就是斐波那契数列的通项公式）。&lt;/p&gt;

&lt;h3&gt;求解&lt;/h3&gt;

&lt;p&gt;首先忽略初始条件，考虑递推式\(T(n)=T(n-2)+T(n-1)\)。可以对解的形式进行一个猜测\(T(n)=q^n\)（这个不是瞎猜的，实际上可以证明线性递推式都遵循这种形式）。那么，递推式可以重写为：&lt;/p&gt;

&lt;p&gt;\[\begin{array}{l l}
            &amp; T(n)=T(n-2)+T(n-1) \\
\Rightarrow &amp; q^n=q^{n-2}+q^{n-1} \\
\Rightarrow &amp; q^2=1+q \\
\Rightarrow &amp; q^2-q-1=0
\end{array}\]&lt;/p&gt;

&lt;p&gt;这样问题被转化为一个一元二次方程的求根问题。利用求根公式可得：&lt;/p&gt;

&lt;p&gt;\[q=\frac{1 \pm \sqrt{5}}{2}\]&lt;/p&gt;

&lt;p&gt;因此得到递推式的一个通解：&lt;/p&gt;

&lt;p&gt;\[T(n)=c_1(\frac{1+\sqrt{5}}{2})^n+c_2(\frac{1-\sqrt{5}}{2})^n\]&lt;/p&gt;

&lt;p&gt;即其中\(c_1\)和\(c_2\)为任意实数。下一步要代入初始条件解出\(c_1\)和\(c_2\)。根据n为0和1时的初始条件，可得：&lt;/p&gt;

&lt;p&gt;\[\left\{
\begin{array}{l l l}
c_1+c_2 &amp; = &amp; 0 \\
c_1(\frac{1+\sqrt{5}}{2})+c_2(\frac{1-\sqrt{5}}{2}) &amp; = &amp; 1
\end{array}
\right.\]&lt;/p&gt;

&lt;p&gt;解得\(c_1=\frac{1}{\sqrt{5}}\)，\(c_2=-\frac{1}{\sqrt{5}}\)。因此最终解为：&lt;/p&gt;

&lt;p&gt;\[T(n)=\frac{1}{\sqrt{5}}((\frac{1+\sqrt{5}}{2})^n-(\frac{1-\sqrt{5}}{2})^n)\]&lt;/p&gt;

&lt;h2&gt;例2：汉诺塔&lt;/h2&gt;

&lt;p&gt;汉诺塔的时间复杂度通常使用递归式定义，在这个例子中将使用代数方法求解其封闭形式。&lt;/p&gt;

&lt;h3&gt;问题&lt;/h3&gt;

&lt;p&gt;汉诺塔的时间复杂度为\(T(n)=2T(n-1)+1\)，求解其封闭形式。&lt;/p&gt;

&lt;h3&gt;求解&lt;/h3&gt;

&lt;p&gt;这里并不能直接使用例1中的方法，因为右边除了递推项外，还有一个非递推项1，用线性代数的语言说，这个线性递推式是非齐次的。&lt;/p&gt;

&lt;p&gt;可以回想一下线性代数中求解非齐次方程组通解的方法：1）求解其齐次部分的通解。2）求其一个特解，将特解加到通解上即得非齐次方程组通解。&lt;/p&gt;

&lt;p&gt;我们用类似的方法求解汉诺塔时间复杂度递推式。首先，忽略后面的1，则得到一个齐次线性递推式：&lt;/p&gt;

&lt;p&gt;\[T(n)=2T(n-1)\]&lt;/p&gt;

&lt;p&gt;转化为多项式方程：&lt;/p&gt;

&lt;p&gt;\[\begin{array}{l l}
            &amp; T(n)=2T(n-1) \\
\Rightarrow &amp; q^n=2q^{n-1} \\
\Rightarrow &amp; q=2 \\
\end{array}\]&lt;/p&gt;

&lt;p&gt;因为方程是一次多项式，我们直接得到了其解为2。因此齐次递推式的通解为\(c2^n\)，其中c为任意常数。&lt;/p&gt;

&lt;p&gt;然后我们需要求得\(T(n)=2T(n-1)+1\)的一个特解，解是一个以n为变量的函数。我们可以先从常数试起，设特解为\(T(n)=a\)，带入得\(a=2a+1\)，
解得\(a=-1\)。因此，原递推式的通解为：&lt;/p&gt;

&lt;p&gt;\[T(n)=c2^n-1\]&lt;/p&gt;

&lt;p&gt;最后我们求解常数c。&lt;/p&gt;

&lt;p&gt;将初始条件\(T(0)=0\)带入，得\(0=c-1\)，因此\(c=1\)。代入原通解，得汉诺塔时间复杂度递推式的封闭形式为：&lt;/p&gt;

&lt;p&gt;\[T(n)=2^n-1\]&lt;/p&gt;

&lt;h1&gt;数学原理&lt;/h1&gt;

&lt;p&gt;上面两个例子可能有些同学看的不是很明白，其中提到了一些线性代数术语。在这一章节中，我们分析上述解法的数学原理，看看递推式是如何与线性代数关联起来的。&lt;/p&gt;

&lt;h2&gt;线性递推式的一般化&lt;/h2&gt;

&lt;p&gt;斐波那契数列和汉诺塔递推式可以看成线性递推式的特例，下面给出线性递推式的一般定义：&lt;/p&gt;

&lt;p&gt;我们将满足如下递推关系的递推式称为线性递推式：&lt;/p&gt;

&lt;p&gt;\[T(n)=a_1T(n-1)+a_2T(n-2)+ \dots +a_{k-1}T(n-k+1)+a_kT(n-k)+C(n)\]&lt;/p&gt;

&lt;p&gt;其中\(C(n)\)是只与n有关系的一个函数。如果\(C(n)=0\)，则称递推式为齐次此，否则称为非齐次的。齐次递推式一定有平凡解\(T(n)=0\)。&lt;/p&gt;

&lt;p&gt;注意仅有递推式是不能求得\(T(n)\)的唯一解，因此递推关系式只能给出一个通解。只有当下列初始条件确定后，才有可能给出\(T(n)\)的唯一特解。&lt;/p&gt;

&lt;p&gt;\[\begin{array}{l}
T(0)=b_0 \\
T(1)=b_1 \\
\vdots \\
T(k-1)=b_{k-1}
\end{array}\]&lt;/p&gt;

&lt;h2&gt;齐次递推式求解定理&lt;/h2&gt;

&lt;p&gt;下面先考虑齐次线性递推式的求解。定理如下：&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;设有线性齐次递推式\(T(n)=a_1T(n-1)+a_2T(n-2)+ \dots +a_{k-1}T(n-k+1)+a_kT(n-k)\)&lt;/p&gt;&lt;p&gt;另设多项式方程\(q^k-a_1q^{k-1}- \dots -a_{k-1}q-a_k=0 \quad (q &gt; 0, a_k \neq 0)\)的根是
\(q_1,q_2,\dots ,q_k\)，我们先讨论不存在重根的情况，也就是说k个根互不相等。&lt;/p&gt;&lt;p&gt;则\(T(n)\)的通解为：&lt;/p&gt;&lt;p&gt;\[T(n)=c_1q_1^n+c_2q_2^n+ \dots +c_kq_k^n\]&lt;/p&gt;&lt;p&gt;并且对于任意的初始情况&lt;/p&gt;&lt;p&gt;\[\begin{array}{l}
T(0)=b_0 \\
T(1)=b_1 \\
\vdots \\
T(k-1)=b_{k-1}
\end{array}\]&lt;/p&gt;&lt;p&gt;都存在一组解\(c_1, \dots ,c_k\)使得递推式成立&lt;/p&gt;&lt;/blockquote&gt;

&lt;h2&gt;定理的证明&lt;/h2&gt;

&lt;p&gt;要证明以上定理，主要需要证明两部分。一是证明多项式根的线性组合可以满足递推式，二是证明任意初始条件下总有解。&lt;/p&gt;

&lt;h3&gt;可满足性证明&lt;/h3&gt;

&lt;p&gt;首先来证明\(T(n)=c_1q_1^n+c_2q_2^n+ \dots +c_kq_k^n\)可以满足递推式。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;\(T(n)=a_1T(n-1)+a_2T(n-2)+ \dots +a_{k-1}T(n-k+1)+a_kT(n-k)\)经过变换可以改写为
\(T(n)-a_1T(n-1)-a_2T(n-2)- \dots -a_{k-1}T(n-k+1)-a_kT(n-k)=0\)&lt;/p&gt;&lt;p&gt;假设\(T(n)=q^n\)，因为\(q&gt;0\)，所以两边除以\(q^{n-k}\)，得到\(q^k-a_1q^{k-1}- \dots -a_{k-1}q-a_k=0\)&lt;/p&gt;&lt;p&gt;因此这个多项式和原递推式同解，因此多项式的每个根q的几何级数\(q^n\)都是原递推式的一个解。同时，根的线性组合
\(c_1q_1^n+c_2q_2^n+ \dots +c_kq_k^n\)均满足原递推式（可以带入验证）。&lt;/p&gt;&lt;/blockquote&gt;

&lt;h3&gt;任意初始值有解证明&lt;/h3&gt;

&lt;p&gt;下面要证明对于任意初始条件，均存在适当的常数\(c_1, \dots ,c_k\)。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;将&lt;/p&gt;&lt;p&gt;\[\begin{array}{l}
T(0)=b_0 \\
T(1)=b_1 \\
\vdots \\
T(k-1)=b_{k-1}
\end{array}\]&lt;/p&gt;&lt;p&gt;带入通解公式，得到一个线性方程组&lt;/p&gt;&lt;p&gt;\[\begin{array}{l l l}
c_1+c_2+ \dots +c_k &amp; = &amp; b_0 \\
c_1q_1+c_2q_2+ \dots +c_kq_k &amp; = &amp; b_1 \\
c_1q_1^2+c_2q_2^2+ \dots +c_kq_k^2 &amp; = &amp; b_2 \\
\vdots &amp; &amp; \\
c_1q_1^{k-1}+c_2q_2^{k-1}+ \dots +c_kq_k^{k-1} &amp; = &amp; b_{k-1} \\
\end{array}\]&lt;/p&gt;&lt;p&gt;此时问题转化为证明此方程组对于必然有解，下面就要用到线性代数的知识了。这个方程组的系数行列式为：&lt;/p&gt;&lt;p&gt;\[{det}(V)=\begin{vmatrix}
1 &amp; 1 &amp; \dots &amp; 1 \\
q_1 &amp; q_2 &amp; \dots &amp; q_k \\
q_1^2 &amp; q_2^2 &amp; \dots &amp; q_k^2 \\
\vdots &amp; \vdots &amp; \dots &amp; \vdots \\
q_1^{k-1} &amp; q_2^{k-1} &amp; \dots &amp; q_k^{k-1}
\end{vmatrix}\]&lt;/p&gt;&lt;p&gt;这个行列式就是非常著名&lt;a href=&quot;http://en.wikipedia.org/wiki/Vandermonde_matrix&quot;&gt;Vandermonde行列式&lt;/a&gt;，所以&lt;/p&gt;&lt;p&gt;\[{det}(V)=\prod_{1 \leq i &lt; j \leq k}{(q_i-q_j)}\]&lt;/p&gt;&lt;p&gt;上面我们假设了多项式各个根均互异，因此行列式的值不等于0，这意味着系数矩阵的秩为k。有线性代数知识可知，这表明
对于任意初始值\(b_0, \dots, b_{k-1}\)，方程组均有唯一解。&lt;/p&gt;&lt;p&gt;证毕。&lt;/p&gt;&lt;p&gt;顺便说一下，上面的多项式叫特征多项式。其根叫特征根。&lt;/p&gt;&lt;/blockquote&gt;

&lt;h1&gt;通用解法&lt;/h1&gt;

&lt;p&gt;通过上面的数学分析，我们得到了一个解线性递推式的通用方法。&lt;/p&gt;

&lt;h2&gt;齐次递推式&lt;/h2&gt;

&lt;p&gt;设有齐次递推式&lt;/p&gt;

&lt;p&gt;\[T(n)=a_1T(n-1)+a_2T(n-2)+ \dots +a_{k-1}T(n-k+1)+a_kT(n-k)\]&lt;/p&gt;

&lt;p&gt;我们可以写出其特征多项式方程&lt;/p&gt;

&lt;p&gt;\[q^k-a_1q^{k-1}- \dots -a_{k-1}q-a_k=0 \quad (q &gt; 0, a_k \neq 0)\]&lt;/p&gt;

&lt;p&gt;解出其k个根\(q_1, \dots, q_k\)。如果k个根互异（但可以有复根），则原递推式的通解为&lt;/p&gt;

&lt;p&gt;\[T(n)=c_1q_1^n+c_2q_2^n+ \dots +c_kq_k^n\]&lt;/p&gt;

&lt;p&gt;然后将初始条件\(b_0, \dots, b_{k-1}\)带入组成线性方程组&lt;/p&gt;

&lt;p&gt;\[\begin{array}{l l l}
x_1+x_2+ \dots +x_k &amp; = &amp; b_0 \\
x_1q_1+x_2q_2+ \dots +x_kq_k &amp; = &amp; b_1 \\
x_1q_1^2+x_2q_2^2+ \dots +x_kq_k^2 &amp; = &amp; b_2 \\
\vdots &amp; &amp; \\
x_1q_1^{k-1}+x_2q_2^{k-1}+ \dots +x_kq_k^{k-1} &amp; = &amp; b_{k-1} \\
\end{array}\]&lt;/p&gt;

&lt;p&gt;解线性方程组得唯一解\(\hat{x}_1, \dots, \hat{x}_k\)。带回通解公式则得到递推式的最终解&lt;/p&gt;

&lt;p&gt;\[T(n)=\hat{x}_1q_1^n+\hat{x}_2q_2^n+ \dots +\hat{x}_kq_k^n\]&lt;/p&gt;

&lt;h2&gt;非齐次递推式&lt;/h2&gt;

&lt;p&gt;对于非齐次递推式&lt;/p&gt;

&lt;p&gt;\[T(n)=a_1T(n-1)+a_2T(n-2)+ \dots +a_{k-1}T(n-k+1)+a_kT(n-k)+C(n) \quad (C(n) \neq 0)\]&lt;/p&gt;

&lt;p&gt;可以首先按上面的方法求解其齐次部分的通解\(T_n\)。然后求得其一个特解\(y_n\)，则非齐次递推式的通解为&lt;/p&gt;

&lt;p&gt;\[T_n+y_n\]&lt;/p&gt;

&lt;p&gt;然后用同样的方法带入初始值，通过线性方程组求出个常量参数带回即可（具体可参见例2）。&lt;/p&gt;

&lt;h2&gt;有重根的情况&lt;/h2&gt;

&lt;p&gt;上面的解法只针对特征根互异，如果有重根的话，则上述方法会无效。不过只要经过一定处理也可以有通用方法求解，
因有点复杂，本文不在针对重根情况进行叙述。关于重根情况下的求解，有感兴趣的同学可以参考线性代数或微分方程相关文献。&lt;/p&gt;</description>
</item>
<item>
<title>为什么算法渐进复杂度中对数的底数总为2</title>
<link>http://blog.codinglabs.org/articles/why-logarithm-base-of-asymptotic-time-complexity-always-two.html</link>
<guid>http://blog.codinglabs.org/articles/why-logarithm-base-of-asymptotic-time-complexity-always-two.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Tue, 29 Jan 2013 00:00:00 GMT</pubDate>
<description>&lt;p&gt;在分析各种算法时，经常看到\(O(\log_2n)\)或\(O(n\log_2n)\)这样的渐进复杂度。不知有没有同学困惑过，为什么算法的渐进复杂度中的对数都是以2为底？为什么没有见过\(O(n\log_3n)\)这样的渐进复杂度？本文解释这个问题。&lt;/p&gt;

&lt;h1&gt;三分式归并排序的时间复杂度&lt;/h1&gt;

&lt;p&gt;先看一个小例子。&lt;/p&gt;

&lt;p&gt;大多数人应该对归并排序（merge sort）很熟悉，它的渐进复杂度为\(O(n\log_2n)\)。那么如果我们将归并排序改为均分成三份而不是两份，其算法时间复杂度是否有变化呢？&lt;/p&gt;

&lt;h1&gt;递归分析&lt;/h1&gt;

&lt;p&gt;下面通过递归分析对三分式归并排序的时间复杂度进行分析。因为不管是三分还是二分，对于总共n个数据来说，一遍合并的复杂度为\(O(n)\)，所以三分式归并排序的递归式为：&lt;/p&gt;

&lt;p&gt;\(T(n)=3T(n/3)+O(n)\)&lt;/p&gt;

&lt;p&gt;如果把这个递归式的递归树画出来，很容易得到\(T(n)=O(n\log_3n)\)。如下图所示：&lt;/p&gt;

&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/why-logarithm-base-of-asymptotic-time-complexity-always-two/1.png&quot;/&gt;&lt;/p&gt;

&lt;h1&gt;对数的陷阱&lt;/h1&gt;

&lt;p&gt;那么这是否意味着三分式归并排序在时间复杂度上要优于二分式的归并排序呢？因为直觉上\(n\log_3n\)比\(n\log_2n\)要优一些。&lt;/p&gt;

&lt;p&gt;实际上三分式归并排序的时间复杂度确实是\(T(n)=O(n\log_3n)\)，而且同时也是\(T(n)=O(n\log_2n)\)。&lt;/p&gt;

&lt;p&gt;这看起来似乎是矛盾的，\(n\log_3n\)和\(n\log_2n\)当然在绝大多数情况下是不相等的，但是在渐进复杂度情况下就不同了，因为渐进复杂度是忽略常系数的，但是似乎也看不出来\(n\log_3n\)和\(n\log_2n\)是差一个常系数。关键就在于我们应该在中学学过的一个东西：对数换底公式。&lt;/p&gt;

&lt;p&gt;\(\log_ab = \frac{\log_cb}{\log_ca}\)&lt;/p&gt;

&lt;p&gt;其中a和c均大于0且不等于1。&lt;/p&gt;

&lt;p&gt;根据换底公式可以得出：&lt;/p&gt;

&lt;p&gt;\(\log_3n = \frac{\log_2n}{\log_23}\)&lt;/p&gt;

&lt;p&gt;所以\(n\log_3n\)比\(n\log_2n\)只差一个常系数\(\frac{1}{\log_23}\)。因此，从渐进时间复杂度看，三分式归并并不比二分式归并更优，当然还是有个常系数的差别的。&lt;/p&gt;

&lt;p&gt;更一般的：&lt;/p&gt;

&lt;p&gt;\(\log_an = \frac{\log_2n}{\log_2a}\)&lt;/p&gt;

&lt;p&gt;因此对于大于1的a来说，都与\(O(\log_2n)\)差一个常系数而已，因此为了简便，一般都用\(O(\log_2n)\)表示对数的渐进复杂度，这就解决了本文初始的疑问。当然，以任何大于1的a为底数都是没有问题的。&lt;/p&gt;
</description>
</item>
<item>
<title>解读Cardinality Estimation算法（第四部分：HyperLogLog Counting及Adaptive Counting）</title>
<link>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iv.html</link>
<guid>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iv.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Wed, 09 Jan 2013 00:00:00 GMT</pubDate>
<description>&lt;p&gt;在&lt;a href=&quot;http://www.codinglabs.org/html/algorithms-for-cardinality-estimation-part-iii.html&quot; target=&quot;_blank&quot;&gt;前一篇文章&lt;/a&gt;中，我们了解了LogLog Counting。LLC算法的空间复杂度为\(O(log_2(log_2(N_{max})))\)，并且具有较高的精度，因此非常适合用于大数据场景的基数估计。不过LLC也有自己的问题，就是当基数不太大时，估计值的误差会比较大。这主要是因为当基数不太大时，可能存在一些空桶，这些空桶的\(\rho_{max}\)为0。由于LLC的估计值依赖于各桶\(\rho_{max}\)的几何平均数，而几何平均数对于特殊值（这里就是指0）非常敏感，因此当存在一些空桶时，LLC的估计效果就变得较差。&lt;/p&gt;
&lt;p&gt;这一篇文章中将要介绍的HyperLogLog Counting及Adaptive Counting算法均是对LLC算法的改进，可以有效克服LLC对于较小基数估计效果差的缺点。&lt;/p&gt;
&lt;!--more--&gt;
&lt;h1&gt;评价基数估计算法的精度&lt;/h1&gt;
&lt;p&gt;首先我们来分析一下LLC的问题。一般来说LLC最大问题在于当基数不太大时，估计效果比较差。上文说过，LLC的渐近标准误差为\(1.30/\sqrt{m}\)，看起来貌似只和分桶数m有关，那么为什么基数的大小也会导致效果变差呢？这就需要重点研究一下如何评价基数估计算法的精度，以及“渐近标准误差”的意义是什么。&lt;/p&gt;
&lt;h2&gt;标准误差&lt;/h2&gt;
&lt;p&gt;首先需要明确标准误差的意义。例如标准误差为0.02，到底表示什么意义。&lt;/p&gt;
&lt;p&gt;标准误差是针对一个统计量（或估计量）而言。在分析基数估计算法的精度时，我们关心的统计量是\(\hat{n}/n\)。注意这个量分子分母均为一组抽样的统计量。下面正式描述一下这个问题。&lt;/p&gt;
&lt;p&gt;设S是我们要估计基数的可重复有限集合。S中每个元素都是来自值服从均匀分布的样本空间的一个独立随机抽样样本。这个集合共有C个元素，但其基数不一定是C，因为其中可能存在重复元素。设\(f_n\)为定义在S上的函数：&lt;/p&gt;
&lt;p&gt;\(f_n(S) = Cardinality\;of\;S\)&lt;/p&gt;
&lt;p&gt;同时定义\(f_\hat{n}\)也是定义在S上的函数：&lt;/p&gt;
&lt;p&gt;\(f_\hat{n}(S)=LogLog\;estimate\;value\;of\;S\)&lt;/p&gt;
&lt;p&gt;我们想得到的第一个函数值，但是由于第一个函数值不好计算，所以我们计算同样集合的第二个函数值来作为第一个函数值得估计。因此最理想的情况是对于任意一个集合两个函数值是相等的，如果这样估计就是100%准确了。不过显然没有这么好的事，因此我们退而求其次，只希望\(f_\hat{n}(S)\)是一个无偏估计，即：&lt;/p&gt;
&lt;p&gt;\(E(\frac{f_\hat{n}(S)}{f_n(S)})=1\)&lt;/p&gt;
&lt;p&gt;这个在上一篇文章中已经说明了。同时也可以看到，\(\frac{f_\hat{n}(S)}{f_n(S)}\)实际上是一个随机变量，并且服从正态分布。对于正态分布随机变量，一般可以通过标准差\(\sigma\)度量其稳定性，直观来看，标准差越小，则整体分布越趋近于均值，所以估计效果就越好。这是定性的，那么定量来看标准误差\(\sigma\)到底表达了什么意思呢。它的意义是这样的：&lt;/p&gt;
&lt;p&gt;对于无偏正态分布而言，随机变量的一次随机取值落在均值一个标准差范围内的概率是68.2%，而落在两个和三个标准差范围内的概率分别为95.4%和99.6%，如下图所示（图片来自&lt;a href=&quot;http://zh.wikipedia.org/wiki/%E6%A0%87%E5%87%86%E8%AF%AF&quot; target=&quot;_blank&quot;&gt;维基百科&lt;/a&gt;）：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/algorithms-for-cardinality-estimation-part-iv/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;因此，假设标准误差是0.02（2%），它实际的意义是：假设真实基数为n，n与估计值之比落入(0.98, 1.02)的概率是68.2%，落入(0.96, 1.04)的概率是95.4%，落入(0.94, 1.06)的概率是99.6%。显然这个比值越大则估计值越不准，因此对于0.02的标准误差，这个比值大于1.06或小于0.94的概率不到0.004。&lt;/p&gt;
&lt;p&gt;再直观一点，假设真实基数为10000，则一次估计值有99.6%的可能不大于10600且不小于9400。&lt;/p&gt;
&lt;h2&gt;组合计数与渐近分析&lt;/h2&gt;
&lt;p&gt;如果LLC能够做到绝对服从\(1.30/\sqrt{m}\)，那么也算很好了，因为我们只要通过控制分桶数m就可以得到一个一致的标准误差。这里的一致是指标准误差与基数无关。不幸的是并不是这样，上面已经说过，这是一个“渐近”标注误差。下面解释一下什么叫渐近。&lt;/p&gt;
&lt;p&gt;在计算数学中，有一个非常有用的分支就是组合计数。组合计数简单来说就是分析自然数的组合函数随着自然数的增长而增长的量级。可能很多人已经意识到这个听起来很像算法复杂度分析。没错，算法复杂度分析就是组合计数在算法领域的应用。&lt;/p&gt;
&lt;p&gt;举个例子，设A是一个有n个元素的集合（这里A是严格的集合，不存在重复元素），则A的幂集（即由A的所有子集组成的集合）有\(2^n\)个元素。&lt;/p&gt;
&lt;p&gt;上述关于幂集的组合计数是一个非常整齐一致的组合计数，也就是不管n多大，A的幂集总有\(2^n\)个元素。&lt;/p&gt;
&lt;p&gt;可惜的是现实中一般的组合计数都不存在如此干净一致的解。LLC的偏差和标准差其实都是组合函数，但是论文中已经分析出，LLC的偏差和标准差都是渐近组合计数，也就是说，随着n趋向于无穷大，标准差趋向于\(1.30/\sqrt{m}\)，而不是说n多大时其值都一致为\(1.30/\sqrt{m}\)。另外，其无偏性也是渐近的，只有当n远远大于m时，其估计值才近似无偏。因此当n不太大时，LLC的效果并不好。&lt;/p&gt;
&lt;p&gt;庆幸的是，同样通过统计分析方法，我们可以得到n具体小到什么程度我们就不可忍受了，另外就是当n太小时可不可以用别的估计方法替代LLC来弥补LLC这个缺陷。HyperLogLog Counting及Adaptive Counting都是基于这个思想实现的。&lt;/p&gt;
&lt;h1&gt;Adaptive Counting&lt;/h1&gt;
&lt;p&gt;Adaptive Counting（简称AC）在“Fast and accurate traffic matrix measurement using adaptive cardinality counting”一文中被提出。其思想也非常简单直观：实际上AC只是简单将LC和LLC组合使用，根据基数量级决定是使用LC还是LLC。具体是通过分析两者的标准差，给出一个阈值，根据阈值选择使用哪种估计。&lt;/p&gt;
&lt;h2&gt;基本算法&lt;/h2&gt;
&lt;p&gt;如果分析一下LC和LLC的存储结构，可以发现两者是兼容的，区别仅仅在于LLC关心每个桶的\(\rho_{max}\)，而LC仅关心此桶是否为空。因此只要简单认为\(\rho_{max}\)值不为0的桶为非空，0为空就可以使用LLC的数据结构做LC估计了。&lt;/p&gt;
&lt;p&gt;而我们已经知道，LC在基数不太大时效果好，基数太大时会失效；LLC恰好相反，因此两者有很好的互补性。&lt;/p&gt;
&lt;p&gt;回顾一下，LC的标准误差为：&lt;/p&gt;
&lt;p&gt;\(SE_{lc}(\hat{n}/n)=\sqrt{e^t-t-1}/(t\sqrt{m})\)&lt;/p&gt;
&lt;p&gt;LLC的标准误差为：&lt;/p&gt;
&lt;p&gt;\(SE_{llc}(\hat{n}/n)=1.30/\sqrt{m}\)&lt;/p&gt;
&lt;p&gt;将两个公式联立：&lt;/p&gt;
&lt;p&gt;\(\sqrt{e^t-t-1}/(t\sqrt{m})=1.30/\sqrt{m}\)&lt;/p&gt;
&lt;p&gt;解得\(t \approx 2.89\)。注意m被消掉了，说明这个阈值与m无关。其中\(t=n/m\)。&lt;/p&gt;
&lt;p&gt;设\(\beta\)为空桶率，根据LC的估算公式，带入上式可得：&lt;/p&gt;
&lt;p&gt;\(\beta = e^{-t} \approx 0.051\)&lt;/p&gt;
&lt;p&gt;因此可以知道，当空桶率大于0.051时，LC的标准误差较小，而当小于0.051时，LLC的标准误差较小。&lt;/p&gt;
&lt;p&gt;完整的AC算法如下：&lt;/p&gt;
&lt;p&gt;\(\hat{n}=\left\{ \begin{eqnarray} \alpha_m m2^{\frac{1}{m}\sum{M}} &amp; if &amp; 0 \leq \beta &lt; 0.051 \\ -mlog(\beta) &amp; if &amp; 0.051 \leq \beta \leq 1 \end{eqnarray} \right.\)&lt;/p&gt;
&lt;h2&gt;误差分析&lt;/h2&gt;
&lt;p&gt;因为AC只是LC和LLC的简单组合，所以误差分析可以依照LC和LLC进行。值得注意的是，当\(\beta &lt; 0.051\)时，LLC最大的偏差不超过0.17%，因此可以近似认为是无偏的。&lt;/p&gt;
&lt;h1&gt;HyperLogLog Counting&lt;/h1&gt;
&lt;p&gt;HyperLogLog Counting（以下简称HLLC）的基本思想也是在LLC的基础上做改进，不过相对于AC来说改进的比较多，所以相对也要复杂一些。本文不做具体细节分析，具体细节请参考“HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm”这篇论文。&lt;/p&gt;
&lt;h2&gt;基本算法&lt;/h2&gt;
&lt;p&gt;HLLC的第一个改进是使用调和平均数替代几何平均数。注意LLC是对各个桶取算数平均数，而算数平均数最终被应用到2的指数上，所以总体来看LLC取得是几何平均数。由于几何平均数对于离群值（例如这里的0）特别敏感，因此当存在离群值时，LLC的偏差就会很大，这也从另一个角度解释了为什么n不太大时LLC的效果不太好。这是因为n较小时，可能存在较多空桶，而这些特殊的离群值强烈干扰了几何平均数的稳定性。&lt;/p&gt;
&lt;p&gt;因此，HLLC使用调和平均数来代替几何平均数，调和平均数的定义如下：&lt;/p&gt;
&lt;p&gt;\(H = \frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + ... + \frac{1}{x_n}} = \frac{n}{\sum_{i=1}^n \frac{1}{x_i}}\)&lt;/p&gt;
&lt;p&gt;调和平均数可以有效抵抗离群值的扰动。使用调和平均数代替几何平均数后，估计公式变为如下：&lt;/p&gt;
&lt;p&gt;\(\hat{n}=\frac{\alpha_m m^2}{\sum{2^{-M}}}\)&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\(\alpha_m=(m\int _0^\infty (log_2(\frac{2+u}{1+u}))^m du)^{-1}\)&lt;/p&gt;
&lt;h2&gt;偏差分析&lt;/h2&gt;
&lt;p&gt;根据论文中的分析结论，与LLC一样HLLC是渐近无偏估计，且其渐近标准差为：&lt;/p&gt;
&lt;p&gt;\(SE_{hllc}(\hat{n}/n)=1.04/\sqrt{m}\)&lt;/p&gt;
&lt;p&gt;因此在存储空间相同的情况下，HLLC比LLC具有更高的精度。例如，对于分桶数m为2^13（8k字节）时，LLC的标准误差为1.4%，而HLLC为1.1%。&lt;/p&gt;
&lt;h2&gt;分段偏差修正&lt;/h2&gt;
&lt;p&gt;在HLLC的论文中，作者在实现建议部分还给出了在n相对于m较小或较大时的偏差修正方案。具体来说，设E为估计值：&lt;/p&gt;
&lt;p&gt;当\(E \leq \frac{5}{2}m\)时，使用LC进行估计。&lt;/p&gt;
&lt;p&gt;当\(\frac{5}{2}m &lt; E \leq \frac{1}{30}2^{32}\)是，使用上面给出的HLLC公式进行估计。&lt;/p&gt;
&lt;p&gt;当\(E &gt; \frac{1}{30}2^{32}\)时，估计公式如为\(\hat{n}=-2^{32}log(1-E/2^{32})\)。&lt;/p&gt;
&lt;p&gt;关于分段偏差修正效果分析也可以在原论文中找到。&lt;/p&gt;
&lt;h1&gt;小结&lt;/h1&gt;
&lt;p&gt;本文首先介绍了基数估计算法标准误差的意义，并据此说明了为什么LLC在基数较小时效果不好。然后，以此介绍了两种对LLC的改进算法：HyperLogLog Counting及Adaptive Counting。到此为止，常见的四种基数估计算法就介绍完了。在本系列最后一篇文章中，我会介绍一淘数据部的基数估计实现&lt;a href=&quot;https://github.com/chaoslawful/ccard-lib&quot; target=&quot;_blank&quot;&gt;ccard-lib&lt;/a&gt;的一些实现细节和使用方式。并做一些实验。&lt;/p&gt;
</description>
</item>
<item>
<title>解读Cardinality Estimation算法（第三部分：LogLog Counting）</title>
<link>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iii.html</link>
<guid>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iii.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Thu, 03 Jan 2013 00:00:00 GMT</pubDate>
<description>&lt;p&gt;&lt;a href=&quot;/articles/algorithms-for-cardinality-estimation-part-ii.html&quot; target=&quot;_blank&quot;&gt;上一篇文章&lt;/a&gt;介绍的Linear Counting算法相较于直接映射bitmap的方法能大大节省内存（大约只需后者1/10的内存），但毕竟只是一个常系数级的降低，空间复杂度仍然为\(O(N_{max})\)。而本文要介绍的LogLog Counting却只有\(O(log_2(log_2(N_{max})))\)。例如，假设基数的上限为1亿，原始bitmap方法需要12.5M内存，而LogLog Counting只需不到1K内存（640字节）就可以在标准误差不超过4%的精度下对基数进行估计，效果可谓十分惊人。&lt;/p&gt;
&lt;p&gt;本文将介绍LogLog Counting。
&lt;!--more--&gt;
&lt;/p&gt;
&lt;h1&gt;简介&lt;/h1&gt;
&lt;p&gt;LogLog Counting（以下简称LLC）出自论文“Loglog Counting of Large Cardinalities”。LLC的空间复杂度仅有\(O(log_2(log_2(N_{max})))\)，使得通过KB级内存估计数亿级别的基数成为可能，因此目前在处理大数据的基数计算问题时，所采用算法基本为LLC或其几个变种。下面来具体看一下这个算法。&lt;/p&gt;
&lt;h1&gt;基本算法&lt;/h1&gt;
&lt;h2&gt;均匀随机化&lt;/h2&gt;
&lt;p&gt;与LC一样，在使用LLC之前需要选取一个哈希函数H应用于所有元素，然后对哈希值进行基数估计。H必须满足如下条件（定性的）：&lt;/p&gt;
&lt;p&gt;1、H的结果具有很好的均匀性，也就是说无论原始集合元素的值分布如何，其哈希结果的值几乎服从均匀分布（完全服从均匀分布是不可能的，D. Knuth已经证明不可能通过一个哈希函数将一组不服从均匀分布的数据映射为绝对均匀分布，但是很多哈希函数可以生成几乎服从均匀分布的结果，这里我们忽略这种理论上的差异，认为哈希结果就是服从均匀分布）。&lt;/p&gt;
&lt;p&gt;2、H的碰撞几乎可以忽略不计。也就是说我们认为对于不同的原始值，其哈希结果相同的概率非常小以至于可以忽略不计。&lt;/p&gt;
&lt;p&gt;3、H的哈希结果是固定长度的。&lt;/p&gt;
&lt;p&gt;以上对哈希函数的要求是随机化和后续概率分析的基础。后面的分析均认为是针对哈希后的均匀分布数据进行。&lt;/p&gt;
&lt;h2&gt;思想来源&lt;/h2&gt;
&lt;p&gt;下面非正式的从直观角度描述LLC算法的思想来源。&lt;/p&gt;
&lt;p&gt;设a为待估集合（哈希后）中的一个元素，由上面对H的定义可知，a可以看做一个长度固定的比特串（也就是a的二进制表示），设H哈希后的结果长度为L比特，我们将这L个比特位从左到右分别编号为1、2、…、L：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/algorithms-for-cardinality-estimation-part-iii/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;又因为a是从服从均与分布的样本空间中随机抽取的一个样本，因此a每个比特位服从如下分布且相互独立。&lt;/p&gt;
&lt;p&gt;\(P(x=k)=\left\{\begin{matrix} 0.5 (k=0)\\ 0.5 (k=1)\end{matrix}\right.\)&lt;/p&gt;
&lt;p&gt;通俗说就是a的每个比特位为0和1的概率各为0.5，且相互之间是独立的。&lt;/p&gt;
&lt;p&gt;设\(\rho(a)\)为a的比特串中第一个“1”出现的位置，显然\(1 \leq \rho(a) \leq L\)，这里我们忽略比特串全为0的情况（概率为\(1/2^L\)）。如果我们遍历集合中所有元素的比特串，取\(\rho_{max}\)为所有\(\rho(a)\)的最大值。&lt;/p&gt;
&lt;p&gt;此时我们可以将\(2^{\rho_{max}}\)作为基数的一个粗糙估计，即：&lt;/p&gt;
&lt;p&gt;\(\hat{n} = 2^{\rho_{max}}\)&lt;/p&gt;
&lt;p&gt;下面解释为什么可以这样估计。注意如下事实：&lt;/p&gt;
&lt;p&gt;由于比特串每个比特都独立且服从0-1分布，因此从左到右扫描上述某个比特串寻找第一个“1”的过程从统计学角度看是一个伯努利过程，例如，可以等价看作不断投掷一个硬币（每次投掷正反面概率皆为0.5），直到得到一个正面的过程。在一次这样的过程中，投掷一次就得到正面的概率为\(1/2\)，投掷两次得到正面的概率是\(1/2^2\)，…，投掷k次才得到第一个正面的概率为\(1/2^k\)。&lt;/p&gt;
&lt;p&gt;现在考虑如下两个问题：&lt;/p&gt;
&lt;p&gt;1、进行n次伯努利过程，所有投掷次数都不大于k的概率是多少？&lt;/p&gt;
&lt;p&gt;2、进行n次伯努利过程，至少有一次投掷次数等于k的概率是多少？&lt;/p&gt;
&lt;p&gt;首先看第一个问题，在一次伯努利过程中，投掷次数大于k的概率为\(1/2^k\)，即连续掷出k个反面的概率。因此，在一次过程中投掷次数不大于k的概率为\(1-1/2^k\)。因此，n次伯努利过程投掷次数均不大于k的概率为：&lt;/p&gt;
&lt;p&gt;\(P_n(X \leq k)=(1-1/2^k)^n\)&lt;/p&gt;
&lt;p&gt;显然第二个问题的答案是：&lt;/p&gt;
&lt;p&gt;\(P_n(X \geq k)=1-(1-1/2^{k-1})^n\)&lt;/p&gt;
&lt;p&gt;从以上分析可以看出，当\(n \ll 2^k\)时，\(P_n(X \geq k)\)的概率几乎为0，同时，当\(n \gg 2^k\)时，\(P_n(X \leq k)\)的概率也几乎为0。用自然语言概括上述结论就是：当伯努利过程次数远远小于\(2^k\)时，至少有一次过程投掷次数等于k的概率几乎为0；当伯努利过程次数远远大于\(2^k\)时，没有一次过程投掷次数大于k的概率也几乎为0。&lt;/p&gt;
&lt;p&gt;如果将上面描述做一个对应：一次伯努利过程对应一个元素的比特串，反面对应0，正面对应1，投掷次数k对应第一个“1”出现的位置，我们就得到了下面结论：&lt;/p&gt;
&lt;p&gt;设一个集合的基数为n，\(\rho_{max}\)为所有元素中首个“1”的位置最大的那个元素的“1”的位置，如果n远远小于\(2^{\rho_{max}}\)，则我们得到\(\rho_{max}\)为当前值的概率几乎为0（它应该更小），同样的，如果n远远大于\(2^{\rho_{max}}\)，则我们得到\(\rho_{max}\)为当前值的概率也几乎为0（它应该更大），因此\(2^{\rho_{max}}\)可以作为基数n的一个粗糙估计。&lt;/p&gt;
&lt;h2&gt;分桶平均&lt;/h2&gt;
&lt;p&gt;上述分析给出了LLC的基本思想，不过如果直接使用上面的单一估计量进行基数估计会由于偶然性而存在较大误差。因此，LLC采用了分桶平均的思想来消减误差。具体来说，就是将哈希空间平均分成m份，每份称之为一个桶（bucket）。对于每一个元素，其哈希值的前k比特作为桶编号，其中\(2^k=m\)，而后L-k个比特作为真正用于基数估计的比特串。桶编号相同的元素被分配到同一个桶，在进行基数估计时，首先计算每个桶内元素最大的第一个“1”的位置，设为M[i]，然后对这m个值取平均后再进行估计，即：&lt;/p&gt;
&lt;p&gt;\(\hat{n}=2^{\frac{1}{m}\sum{M[i]}}\)&lt;/p&gt;
&lt;p&gt;这相当于物理试验中经常使用的多次试验取平均的做法，可以有效消减因偶然性带来的误差。&lt;/p&gt;
&lt;p&gt;下面举一个例子说明分桶平均怎么做。&lt;/p&gt;
&lt;p&gt;假设H的哈希长度为16bit，分桶数m定为32。设一个元素哈希值的比特串为“0001001010001010”，由于m为32，因此前5个bit为桶编号，所以这个元素应该归入“00010”即2号桶（桶编号从0开始，最大编号为m-1），而剩下部分是“01010001010”且显然\(\rho(01010001010)=2\)，所以桶编号为“00010”的元素最大的\(\rho\)即为M[2]的值。&lt;/p&gt;
&lt;h2&gt;偏差修正&lt;/h2&gt;
&lt;p&gt;上述经过分桶平均后的估计量看似已经很不错了，不过通过数学分析可以知道这并不是基数n的无偏估计。因此需要修正成无偏估计。这部分的具体数学分析在“Loglog Counting of Large Cardinalities”中，过程过于艰涩这里不再具体详述，有兴趣的朋友可以参考原论文。这里只简要提一下分析框架：&lt;/p&gt;
&lt;p&gt;首先上文已经得出：&lt;/p&gt;
&lt;p&gt;\(P_n(X \leq k)=(1-1/2^k)^n\)&lt;/p&gt;
&lt;p&gt;因此：&lt;/p&gt;
&lt;p&gt;\(P_n(X = k)=(1-1/2^k)^n - (1-1/2^{k-1})^n\)&lt;/p&gt;
&lt;p&gt;这是一个未知通项公式的递推数列，研究这种问题的常用方法是使用生成函数（generating function）。通过运用指数生成函数和poissonization得到上述估计量的Poisson期望和方差为：&lt;/p&gt;
&lt;p&gt;\(\varepsilon _n\sim [(\Gamma (-1/m)\frac{1-2^{1/m}}{log2})^m+\epsilon _n]n\)&lt;/p&gt;
&lt;p&gt;\(\nu _n\sim [(\Gamma (-2/m)\frac{1-2^{2/m}}{log2})^m - (\Gamma (-1/m)\frac{1-2^{1/m}}{log2})^{2m}+\eta _n]n^2\)&lt;/p&gt;
&lt;p&gt;其中\(|\epsilon _n|\)和\(|\eta _n|\)不超过\(10^{-6}\)。&lt;/p&gt;
&lt;p&gt;最后通过depoissonization得到一个渐进无偏估计量：&lt;/p&gt;
&lt;p&gt;\(\hat{n}=\alpha _m 2^{\frac{1}{m}\sum{M[i]}}\)&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\(\alpha _m = (\Gamma (-1/m)\frac{1-2^{1/m}}{log2})^{-m}\)&lt;/p&gt;
&lt;p&gt;\(\Gamma (s)=\frac{1}{s}\int_{0}^{\infty }e^{-t}t^sdt\)&lt;/p&gt;
&lt;p&gt;其中m是分桶数。这就是LLC最终使用的估计量。&lt;/p&gt;
&lt;h2&gt;误差分析&lt;/h2&gt;
&lt;p&gt;不加证明给出如下结论：&lt;/p&gt;
&lt;p&gt;\(E_n(\hat{n})/n = 1 + \theta_{1,n} + o(1)\)&lt;/p&gt;
&lt;p&gt;\(\sqrt{Var_n(E)}/n = \beta_m / \sqrt{m} + \theta_{2,n} + o(1)\)&lt;/p&gt;
&lt;p&gt;其中\(|\theta_{1,n}|\)和\(|\theta_{2,n}|\)不超过\(10^{-6}\)。&lt;/p&gt;
&lt;p&gt;当m不太小（不小于64）时，\(\beta\)大约为1.30。因此：&lt;/p&gt;
&lt;p&gt;\(StdError(\hat{n}/n) \approx \frac{1.30}{\sqrt{m}}\)&lt;/p&gt;
&lt;h1&gt;算法应用&lt;/h1&gt;
&lt;h2&gt;误差控制&lt;/h2&gt;
&lt;p&gt;在应用LLC时，主要需要考虑的是分桶数m，而这个m主要取决于误差。根据上面的误差分析，如果要将误差控制在\(\epsilon\)之内，则：&lt;/p&gt;
&lt;p&gt;\(m &gt; (\frac{1.30}{\epsilon})^2\)&lt;/p&gt;
&lt;h2&gt;内存使用分析&lt;/h2&gt;
&lt;p&gt;内存使用与m的大小及哈希值得长度（或说基数上限）有关。假设H的值为32bit，由于\(\rho_{max} \leq 32\)，因此每个桶需要5bit空间存储这个桶的\(\rho_{max}\)，m个桶就是\(5 \times m/8\)字节。例如基数上限为一亿（约\(2^{27}\)），当分桶数m为1024时，每个桶的基数上限约为\(2^{27} / 2^{10} = 2^{17}\)，而\(log_2(log_2(2^{17}))=4.09\)，因此每个桶需要5bit，需要字节数就是\(5 \times 1024 / 8 = 640\)，误差为\(1.30 / \sqrt{1024} = 0.040625\)，也就是约为4%。&lt;/p&gt;
&lt;h2&gt;合并&lt;/h2&gt;
&lt;p&gt;与LC不同，LLC的合并是以桶为单位而不是bit为单位，由于LLC只需记录桶的\(\rho_{max}\)，因此合并时取相同桶编号数值最大者为合并后此桶的数值即可。&lt;/p&gt;
&lt;h1&gt;小结&lt;/h1&gt;
&lt;p&gt;本文主要介绍了LogLog Counting算法，相比LC其最大的优势就是内存使用极少。不过LLC也有自己的问题，就是当n不是特别大时，其估计误差过大，因此目前实际使用的基数估计算法都是基于LLC改进的算法，这些改进算法通过一定手段抑制原始LLC在n较小时偏差过大的问题。后面要介绍的HyperLogLog Counting和Adaptive Counting就是这类改进算法。&lt;/p&gt;
</description>
</item>
<item>
<title>解读Cardinality Estimation算法（第二部分：Linear Counting）</title>
<link>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-ii.html</link>
<guid>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-ii.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Mon, 31 Dec 2012 00:00:00 GMT</pubDate>
<description>&lt;p&gt;在&lt;a href=&quot;/articles/algorithms-for-cardinality-estimation-part-i.html&quot; target=&quot;_blank&quot;&gt;上一篇文章&lt;/a&gt;中，我们知道传统的精确基数计数算法在数据量大时会存在一定瓶颈，瓶颈主要来自于数据结构合并和内存使用两个方面。因此出现了很多基数估计的概率算法，这些算法虽然计算出的结果不是精确的，但误差可控，重要的是这些算法所使用的数据结构易于合并，同时比传统方法大大节省内存。&lt;/p&gt;
&lt;p&gt;在这一篇文章中，我们讨论Linear Counting算法。
&lt;!--more--&gt;
&lt;/p&gt;
&lt;h1&gt;简介&lt;/h1&gt;
&lt;p&gt;Linear Counting（以下简称LC）在1990年的一篇论文“A linear-time probabilistic counting algorithm for database applications”中被提出。作为一个早期的基数估计算法，LC在空间复杂度方面并不算优秀，实际上LC的空间复杂度与上文中简单bitmap方法是一样的（但是有个常数项级别的降低），都是\(O(N_{max})\)，因此目前很少单独使用LC。不过作为Adaptive Counting等算法的基础，研究一下LC还是比较有价值的。&lt;/p&gt;
&lt;h1&gt;基本算法&lt;/h1&gt;
&lt;h2&gt;思路&lt;/h2&gt;
&lt;p&gt;LC的基本思路是：设有一哈希函数H，其哈希结果空间有m个值（最小值0，最大值m-1），并且哈希结果服从均匀分布。使用一个长度为m的bitmap，每个bit为一个桶，均初始化为0，设一个集合的基数为n，此集合所有元素通过H哈希到bitmap中，如果某一个元素被哈希到第k个比特并且第k个比特为0，则将其置为1。当集合所有元素哈希完成后，设bitmap中还有u个bit为0。则：&lt;/p&gt;
&lt;p&gt;\(\hat{n}=-mlog\frac{u}{m}\)&lt;/p&gt;
&lt;p&gt;为n的一个估计，且为最大似然估计（MLE）。&lt;/p&gt;
&lt;p&gt;示意图如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/algorithms-for-cardinality-estimation-part-ii/1.png&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;推导及证明&lt;/h2&gt;
&lt;p&gt;（对数学推导不感兴趣的读者可以跳过本节）&lt;/p&gt;
&lt;p&gt;由上文对H的定义已知n个不同元素的哈希值服从独立均匀分布。设\(A_j\)为事件“经过n个不同元素哈希后，第j个桶值为0”，则：&lt;/p&gt;
&lt;p&gt;\(P(A_j)=(1-\frac{1}{m})^n\)&lt;/p&gt;
&lt;p&gt;又每个桶是独立的，则u的期望为：&lt;/p&gt;
&lt;p&gt;\(E(u)=\sum_{j=1}^mP(A_j)=m(1-\frac{1}{m})^n=m((1+\frac{1}{-m})^{-m})^{-n/m}\)&lt;/p&gt;
&lt;p&gt;当n和m趋于无穷大时，其值约为\(me^{-n/m}\)&lt;/p&gt;
&lt;p&gt;令：&lt;/p&gt;
&lt;p&gt;\(E(u)=me^{-n/m}\)&lt;/p&gt;
&lt;p&gt;得：&lt;/p&gt;
&lt;p&gt;\(n=-mlog\frac{E(u)}{m}\)&lt;/p&gt;
&lt;p&gt;显然每个桶的值服从参数相同0-1分布，因此u服从二项分布。由概率论知识可知，当n很大时，可以用正态分布逼近二项分布，因此可以认为当n和m趋于无穷大时u渐进服从正态分布。&lt;/p&gt;
&lt;p&gt;因此u的概率密度函数为：&lt;/p&gt;
&lt;p&gt;\(f(x) = {1 \over \sigma\sqrt{2\pi} }\,e^{- {{(x-\mu )^2 \over 2\sigma^2}}}\)&lt;/p&gt;
&lt;p&gt;由于我们观察到的空桶数u是从正态分布中随机抽取的一个样本，因此它就是\(\mu\)的最大似然估计（正态分布的期望的最大似然估计是样本均值）。&lt;/p&gt;
&lt;p&gt;又由如下定理：&lt;/p&gt;
&lt;p&gt;设\(f(x)\)是可逆函数\(\hat{x}\)是\(x\)的最大似然估计，则\(f(\hat{x})\)是\(f(x)\)的最大似然估计。&lt;/p&gt;
&lt;p&gt;且\(-mlog\frac{x}{m}\)是可逆函数，则\(\hat{n}=-mlog\frac{u}{m}\)是\(-mlog\frac{E(u)}{m}=n\)的最大似然估计。&lt;/p&gt;
&lt;h2&gt;偏差分析&lt;/h2&gt;
&lt;p&gt;下面不加证明给出如下结论：&lt;/p&gt;
&lt;p&gt;\(Bias(\frac{\hat{n}}{n})=E(\frac{\hat{n}}{n})-1=\frac{e^t-t-1}{2n}\)&lt;/p&gt;
&lt;p&gt;\(StdError(\frac{\hat{n}}{n})=\frac{\sqrt{m}(e^t-t-1)^{1/2}}{n}\)&lt;/p&gt;
&lt;p&gt;其中\(t=n/m\)&lt;/p&gt;
&lt;p&gt;以上结论的推导在“A linear-time probabilistic counting algorithm for database applications”可以找到。&lt;/p&gt;
&lt;h1&gt;算法应用&lt;/h1&gt;
&lt;p&gt;在应用LC算法时，主要需要考虑的是bitmap长度m的选择。这个选择主要受两个因素的影响：基数n的量级以及容许的误差。这里假设估计基数n的量级大约为N，允许的误差为\(\epsilon\)，则m的选择需要遵循如下约束。&lt;/p&gt;
&lt;h2&gt;误差控制&lt;/h2&gt;
&lt;p&gt;这里以标准差作为误差。由上面标准差公式可以推出，当基数的量级为N，容许误差为\(\epsilon\)时，有如下限制：&lt;/p&gt;
&lt;p&gt;\(m &gt; \frac{e^t-t-1}{(\epsilon t)^2}\)&lt;/p&gt;
&lt;p&gt;将量级和容许误差带入上式，就可以得出m的最小值。&lt;/p&gt;
&lt;h2&gt;满桶控制&lt;/h2&gt;
&lt;p&gt;由LC的描述可以看到，如果m比n小太多，则很有可能所有桶都被哈希到了，此时u的值为0，LC的估计公式就不起作用了（变成无穷大）。因此m的选择除了要满足上面误差控制的需求外，还要保证满桶的概率非常小。&lt;/p&gt;
&lt;p&gt;上面已经说过，u满足二项分布，而当n非常大，p非常小时，可以用泊松分布近似逼近二项分布。因此这里我们可以认为u服从泊松分布（注意，上面我们说u也可以近似服从正态分布，这并不矛盾，实际上泊松分布和正态分布分别是二项分布的离散型和连续型概率逼近，且泊松分布以正态分布为极限）：&lt;/p&gt;
&lt;p&gt;当n、m趋于无穷大时：&lt;/p&gt;
&lt;p&gt;\(Pr(u=k)=(\frac{\lambda^k}{k!})e^{-\lambda}\)&lt;/p&gt;
&lt;p&gt;因此：&lt;/p&gt;
&lt;p&gt;\(Pr(u=0)&lt;e^{-5}=0.007\)&lt;/p&gt;
&lt;p&gt;由于泊松分布的方差为\(\lambda\)，因此只要保证u的期望偏离0点\(\sqrt{5}\)的标准差就可以保证满桶的概率不大约0.7%。因此可得：&lt;/p&gt;
&lt;p&gt;\(m &gt; 5(e^t-t-1)\)&lt;/p&gt;
&lt;p&gt;综上所述，当基数量级为N，可接受误差为\(\epsilon\)，则m的选取应该遵从&lt;/p&gt;
&lt;p&gt;\(m &gt; \beta (e^t-t-1)\)&lt;/p&gt;
&lt;p&gt;其中\(\beta = max(5, 1/(\epsilon t)^2)\)&lt;/p&gt;
&lt;p&gt;下图是论文作者预先计算出的关于不同基数量级和误差情况下，m的选择表：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/algorithms-for-cardinality-estimation-part-ii/2.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;可以看出精度要求越高，则bitmap的长度越大。随着m和n的增大，m大约为n的十分之一。因此LC所需要的空间只有传统的bitmap直接映射方法的1/10，但是从渐进复杂性的角度看，空间复杂度仍为\(O(N_{max})\)。&lt;/p&gt;
&lt;h2&gt;合并&lt;/h2&gt;
&lt;p&gt;LC非常方便于合并，合并方案与传统bitmap映射方法无异，都是通过按位或的方式。&lt;/p&gt;
&lt;h1&gt;小结&lt;/h1&gt;
&lt;p&gt;这篇文章主要介绍了Linear Counting。LC算法虽然由于空间复杂度不够理想已经很少被单独使用，但是由于其在元素数量较少时表现非常优秀，因此常被用于弥补LogLog Counting在元素较少时误差较大的缺陷，实际上LC及其思想是组成HyperLogLog Counting和Adaptive Counting的一部分。&lt;/p&gt;
&lt;p&gt;在下一篇文章中，我会介绍空间复杂度仅有\(O(log_2(log_2(N_{max})))\)的基数估计算法LogLog Counting。&lt;/p&gt;
</description>
</item>
<item>
<title>解读Cardinality Estimation算法（第一部分：基本概念）</title>
<link>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-i.html</link>
<guid>http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-i.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Sun, 30 Dec 2012 00:00:00 GMT</pubDate>
<description>&lt;p&gt;基数计数（cardinality counting）是实际应用中一种常见的计算场景，在数据分析、网络监控及数据库优化等领域都有相关需求。精确的基数计数算法由于种种原因，在面对大数据场景时往往力不从心，因此如何在误差可控的情况下对基数进行估计就显得十分重要。目前常见的基数估计算法有Linear Counting、LogLog Counting、HyperLogLog Counting及Adaptive Counting等。这几种算法都是基于概率统计理论所设计的概率算法，它们克服了精确基数计数算法的诸多弊端（如内存需求过大或难以合并等），同时可以通过一定手段将误差控制在所要求的范围内。&lt;/p&gt;
&lt;p&gt;作为“解读Cardinality Estimation算法”系列文章的第一部分，本文将首先介绍基数的概念，然后通过一个电商数据分析的例子说明基数如何在具体业务场景中发挥作用以及为什么在大数据面前基数的计算是困难的，在这一部分也同时会详述传统基数计数的解决方案及遇到的难题。&lt;/p&gt;
&lt;p&gt;后面在第二部分-第四部分会分别详细介绍Linear Counting、LogLog Counting、HyperLogLog Counting及Adaptive Counting四个算法，会涉及算法的基本思路、概率分析及论文关键部分的解读。&lt;/p&gt;
&lt;p&gt;最后在第五部分会介绍一淘数据部的开源基数估计算法库&lt;a href=&quot;https://github.com/chaoslawful/ccard-lib&quot; target=&quot;_blank&quot;&gt;ccard-lib&lt;/a&gt;，这个算法库由一淘数据部工程师&lt;a href=&quot;http://weibo.com/u/1919389283&quot; target=&quot;_blank&quot;&gt;清无&lt;/a&gt;（王晓哲）、&lt;a href=&quot;http://weibo.com/u/1447857772&quot; target=&quot;_blank&quot;&gt;民瞻&lt;/a&gt;（张维）及我开发，并已用于一淘数据部多个业务线，ccard-lib实现了上述四种算法，整个库使用C写成，并附带PHP扩展模块，我会在这一部分介绍ccard-lib的实现重点及使用方法。&lt;/p&gt;
&lt;!--more--&gt;
文章索引：
&lt;p&gt;&lt;a href=&quot;/articles/algorithms-for-cardinality-estimation-part-i.html&quot; target=&quot;_blank&quot;&gt;第一部分：基本概念&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;/articles/algorithms-for-cardinality-estimation-part-ii.html&quot; target=&quot;_blank&quot;&gt;第二部分：Linear Counting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;/articles/algorithms-for-cardinality-estimation-part-iii.html&quot;&gt;第三部分：LogLog Counting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;/articles/algorithms-for-cardinality-estimation-part-iv.html&quot; target=&quot;_blank&quot;&gt;第四部分：HyperLogLog Counting及Adaptive Counting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第五部分：ccard-lib介绍&lt;/p&gt;
&lt;h1&gt;基数的定义&lt;/h1&gt;
&lt;p&gt;简单来说，基数（cardinality，也译作势），是指一个集合（这里的集合允许存在重复元素，与集合论对集合严格的定义略有不同，如不做特殊说明，本文中提到的集合均允许存在重复元素）中不同元素的个数。例如看下面的集合：&lt;/p&gt;
&lt;p&gt;\(\{1, 2, 3, 4, 5, 2, 3, 9, 7\}\)&lt;/p&gt;
&lt;p&gt;这个集合有9个元素，但是2和3各出现了两次，因此不重复的元素为1,2,3,4,5,9,7，所以这个集合的基数是7。&lt;/p&gt;
&lt;p&gt;如果两个集合具有相同的基数，我们说这两个集合等势。基数和等势的概念在有限集范畴内比较直观，但是如果扩展到无限集则会比较复杂，一个无限集可能会与其真子集等势（例如整数集和偶数集是等势的）。不过在这个系列文章中，我们仅讨论有限集的情况，关于无限集合基数的讨论，有兴趣的同学可以参考实变分析相关内容。&lt;/p&gt;
&lt;p&gt;容易证明，如果一个集合是有限集，则其基数是一个自然数。&lt;/p&gt;
&lt;h1&gt;基数的应用实例&lt;/h1&gt;
&lt;p&gt;下面通过一个实例说明基数在电商数据分析中的应用。&lt;/p&gt;
&lt;p&gt;假设一个淘宝网店在其店铺首页放置了10个宝贝链接，分别从Item01到Item10为这十个链接编号。店主希望可以在一天中随时查看从今天零点开始到目前这十个宝贝链接分别被多少个独立访客点击过。所谓独立访客（Unique Visitor，简称UV）是指有多少个自然人，例如，即使我今天点了五次Item01，我对Item01的UV贡献也是1，而不是5。&lt;/p&gt;
&lt;p&gt;用术语说这实际是一个实时数据流统计分析问题。&lt;/p&gt;
&lt;p&gt;要实现这个统计需求。需要做到如下三点：&lt;/p&gt;
&lt;p&gt;1、对独立访客做标识&lt;/p&gt;
&lt;p&gt;2、在访客点击链接时记录下链接编号及访客标记&lt;/p&gt;
&lt;p&gt;3、对每一个要统计的链接维护一个数据结构和一个当前UV值，当某个链接发生一次点击时，能迅速定位此用户在今天是否已经点过此链接，如果没有则此链接的UV增加1&lt;/p&gt;
&lt;p&gt;下面分别介绍三个步骤的实现方案&lt;/p&gt;
&lt;h2&gt;对独立访客做标识&lt;/h2&gt;
&lt;p&gt;客观来说，目前还没有能在互联网上准确对一个自然人进行标识的方法，通常采用的是近似方案。例如通过登录用户+cookie跟踪的方式：当某个用户已经登录，则采用会员ID标识；对于未登录用户，则采用跟踪cookie的方式进行标识。为了简单起见，我们假设完全采用跟踪cookie的方式对独立访客进行标识。&lt;/p&gt;
&lt;h2&gt;记录链接编号及访客标记&lt;/h2&gt;
&lt;p&gt;这一步可以通过javascript埋点及记录accesslog完成，具体原理和实现方案可以参考我之前的一篇文章：&lt;a href=&quot;http://www.codinglabs.org/html/how-web-analytics-data-collection-system-work.html&quot; target=&quot;_blank&quot;&gt;网站统计中的数据收集原理及实现&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;实时UV计算&lt;/h2&gt;
&lt;p&gt;可以看到，如果将每个链接被点击的日志中访客标识字段看成一个集合，那么此链接当前的UV也就是这个集合的基数，因此UV计算本质上就是一个基数计数问题。&lt;/p&gt;
&lt;p&gt;在实时计算流中，我们可以认为任何一次链接点击均触发如下逻辑（伪代码描述）：&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;cand_counting(item_no, user_id) {
    if (user_id is not in the item_no visitor set) {
        add user_id to item_no visitor set;
        cand[item_no]++;
    }
}&lt;/pre&gt;
&lt;p&gt;逻辑非常简单，每当有一个点击事件发生，就去相应的链接被访集合中寻找此访客是否已经在里面，如果没有则将此用户标识加入集合，并将此链接的UV加1。&lt;/p&gt;
&lt;p&gt;虽然逻辑非常简单，但是在实际实现中尤其面临大数据场景时还是会遇到诸多困难，下面一节我会介绍两种目前被业界普遍使用的精确算法实现方案，并通过分析说明当数据量增大时它们面临的问题。&lt;/p&gt;
&lt;h1&gt;传统的基数计数实现&lt;/h1&gt;
&lt;p&gt;接着上面的例子，我们看一下目前常用的基数计数的实现方法。&lt;/p&gt;
&lt;h2&gt;基于B树的基数计数&lt;/h2&gt;
&lt;p&gt;对上面的伪代码做一个简单分析，会发现关键操作有两个：查找-迅速定位当前访客是否已经在集合中，插入-将新的访客标识插入到访客集合中。因此，需要为每一个需要统计UV的点（此处就是十个宝贝链接）维护一个查找效率较高的数据结构，又因为实时数据流的关系，这个数据结构需要尽量在内存中维护，因此这个数据结构在空间复杂度上也要比较适中。综合考虑一种传统的做法是在实时计算引擎采用了B树来组织这个集合。下图是一个示意图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/algorithms-for-cardinality-estimation-part-i/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;之所以选用B树是因为B树的查找和插入相关高效，同时空间复杂度也可以接受（关于B树具体的性能分析请参考&lt;a href=&quot;http://en.wikipedia.org/wiki/B-tree&quot; target=&quot;_blank&quot;&gt;这里&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;这种实现方案为一个基数计数器维护一棵B树，由于B树在查找效率、插入效率和内存使用之间非常平衡，所以算是一种可以接受的解决方案。但是当数据量特别巨大时，例如要同时统计几万个链接的UV，如果要将几万个链接一天的访问记录全部维护在内存中，这个内存使用量也是相当可观的（假设每个B树占用1M内存，10万个B树就是100G！）。一种方案是在某个时间点将内存数据结构写入磁盘（双十一和双十二大促时一淘数据部的效果平台是每分钟将数据写入HBase）然后将内存中的计数器和数据结构清零，但是B树并不能高效的进行合并，这就使得内存数据落地成了非常大的难题。&lt;/p&gt;
&lt;p&gt;另一个需要数据结构合并的场景是查看并集的基数，例如在上面的例子中，如果我想查看Item1和Item2的总UV，是没有办法通过这种B树的结构快速得到的。当然可以为每一种可能的组合维护一棵B树。不过通过简单的分析就可以知道这个方案基本不可行。N个元素集合的非空幂集数量为\(2^N-1\)，因此要为10个链接维护1023棵B树，而随着链接的增加这个数量会以幂指级别增长。&lt;/p&gt;
&lt;h2&gt;基于bitmap的基数计数&lt;/h2&gt;
&lt;p&gt;为了克服B树不能高效合并的问题，一种替代方案是使用bitmap表示集合。也就是使用一个很长的bit数组表示集合，将bit位顺序编号，bit为1表示此编号在集合中，为0表示不在集合中。例如“00100110”表示集合 {2，5，6}。bitmap中1的数量就是这个集合的基数。&lt;/p&gt;
&lt;p&gt;显然，与B树不同bitmap可以高效的进行合并，只需进行按位或（or）运算就可以，而位运算在计算机中的运算效率是很高的。但是bitmap方式也有自己的问题，就是内存使用问题。&lt;/p&gt;
&lt;p&gt;很容易发现，bitmap的长度与集合中元素个数无关，而是与基数的上限有关。例如在上面的例子中，假如要计算上限为1亿的基数，则需要12.5M字节的bitmap，十个链接就需要125M。关键在于，这个内存使用与集合元素数量无关，即使一个链接仅仅有一个1UV，也要为其分配12.5M字节。&lt;/p&gt;
&lt;p&gt;由此可见，虽然bitmap方式易于合并，却由于内存使用问题而无法广泛用于大数据场景。&lt;/p&gt;
&lt;h1&gt;小结&lt;/h1&gt;
&lt;p&gt;本文重点在于通过电商数据分析中UV计算的例子，说明基数的应用、传统的基数计数算法及这些算法在大数据面前遇到的问题。实际上目前还没有发现更好的在大数据场景中准确计算基数的高效算法，因此在不追求绝对准确的情况下，使用概率算法算是一个不错的解决方案。在后续文章中，我将逐一解读常用的基数估计概率算法。&lt;/p&gt;
</description>
</item>
<item>
<title>基数估计算法概览</title>
<link>http://blog.codinglabs.org/articles/cardinality-estimation.html</link>
<guid>http://blog.codinglabs.org/articles/cardinality-estimation.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Fri, 23 Nov 2012 00:00:00 GMT</pubDate>
<description>&lt;p&gt;翻译自《&lt;a href=&quot;http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation&quot; target=&quot;_blank&quot;&gt;Damn Cool Algorithms: Cardinality Estimation&lt;/a&gt;》，原文链接：&lt;a title=&quot;http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation&quot; href=&quot;http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation&quot; target=&quot;_blank&quot;&gt;http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;假如你有一个巨大的含有重复数据项数据集，这个数据集过于庞大以至于无法全部放到内存中处理。现在你想知道这个数据集里有多少不同的元素，但是数据集没有排好序，而且对如此大的一个数据集进行排序和计数几乎是不可行的。你要如何估计数据集中有多少不同的数据项？很多应用场景都涉及这个问题，例如设计数据库的查询策略：一个良好的数据库查询策略不但和总的数据量有关，同时也依赖于数据中不同数据项的数量。&lt;/p&gt;
&lt;p&gt;我建议在继续阅读本文前你可以稍微是思考一下这个问题，因为接下来我们要谈的算法相当有创意，而且实在是不怎么直观。&lt;/p&gt;
&lt;h1&gt;一个简单直观的基数估计方法&lt;/h1&gt;
&lt;p&gt;让我们从一个简单直观的例子开始吧。假设你通过如下步骤生成了一个数据集：&lt;/p&gt;
&lt;p&gt;1、随机生成n个服从均匀分布的数字&lt;/p&gt;
&lt;p&gt;2、随便重复其中一些数字，重复的数字和重复次数都不确定&lt;/p&gt;
&lt;p&gt;3、打乱这些数字的顺序，得到一个数据集&lt;/p&gt;
&lt;p&gt;我们要如何估计这个数据集中有多少不同的数字呢？因为知道这些数字是服从均匀分布的随机数字，一个比较简单的可行方案是：找出数据集中最小的数字。假如m是数值上限，x是找到的最小的数，则\(m/x\)是基数的一个估计。例如，我们扫描一个包含0到1之间数字组成的数据集，其中最小的数是0.01，则一个比较合理的推断是数据集中大约有100个不同的元素，否则我们应该预期能找到一个更小的数。注意这个估计值和重复次数无关：就如最小值重复多少次都不改变最小值的数值。&lt;/p&gt;
&lt;p&gt;这个估计方法的优点是十分直观，但是准确度一般。例如，一个只有很少不同数值的数据集却拥有很小的最小值；类似的一个有很多不同值的数据集可能最小值并不小。最后一点，其实只有很少的数据集符合随机均匀分布这一前提。尽管如此，这个原型算法仍然是了解基数估计思想的一个途径；后面我们会了解一些更加精巧的算法。&lt;/p&gt;
&lt;h1&gt;基数估计的概率算法&lt;/h1&gt;
&lt;p&gt;最早研究高精度基数估计的论文是Flajolet和Martin的&lt;a href=&quot;http://www.cse.unsw.edu.au/~cs9314/07s1/lectures/Lin_CS9314_References/fm85.pdf&quot;&gt;Probabilistic Counting Algorithms for Data Base Applications&lt;/a&gt;，后来Flajolet又发表了&lt;a href=&quot;http://www.ic.unicamp.br/~celio/peer2peer/math/bitmap-algorithms/durand03loglog.pdf&quot;&gt;LogLog counting of large cardinalities&lt;/a&gt;和&lt;a href=&quot;http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf&quot;&gt;HyperLogLog: The analysis of a near-optimal cardinality estimation algorithm&lt;/a&gt;两篇论文对算法进行了进一步改进。通过逐篇阅读这些论文来了解算法的发展和细节固然有趣，不过在这篇文章中我会忽略一些算法的理论细节，把精力主要放在如何通过论文中的算法解决问题。有兴趣的读者可以读一下这三篇论文；本文不会介绍其中的数学细节。&lt;/p&gt;
&lt;p&gt;Flajolet和Martin最早发现通过一个良好的哈希函数，可以将任意数据集映射成服从均匀分布的（伪）随机值。根据这一事实，可以将任意数据集变换为均匀分布的随机数集合，然后就可以使用上面的方法进行估计了，不过只是这样是远远不够的。&lt;/p&gt;
&lt;p&gt;接下来，他们陆续发现一些其它的基数估计方法，而其中一些方法的效果优于之前提到的方法。Flajolet和Martin计算了哈希值的二进制表示的0前缀，结果发现在随机数集合中，通过计算每一个元素的二进制表示的0前缀，设k为最长的0前缀的长度，则平均来说集合中大约有\(2^k\)个不同的元素；我们可以用这个方法估计基数。但是，这仍然不是很理想的估计方法，因为和基于最小值的估计一样，这个方法的方差很大。不过另一方面，这个估计方法比较节省资源：对于32位的哈希值来说，只需要5比特去存储0前缀的长度。&lt;/p&gt;
&lt;p&gt;值得一提的是，Flajolet-Martin在最初的论文里通过一种基于bitmap的过程去提高估计算法的准确度。关于这点我就不再详述了，因为这种方法已经被后续论文中更好的方法所取代；对这个细节有兴趣的读者可以去阅读原始论文。&lt;/p&gt;
&lt;p&gt;到目前为止，我们这种基于位模式的估计算法给出的结果仍然不够理想。如何进行改进呢？一个直观的改进方法就是使用多个相互独立的哈希函数：通过计算每个哈希函数所产生的最长0前缀，然后取其平均值可以提高算法的精度。&lt;/p&gt;
&lt;p&gt;实践表明从统计意义来说这种方法确实可以提高估计的准确度，但是计算哈希值的消耗比较大。另一个更高效的方法就是随机平均（stochastic averaging）。这种方法不是使用多个哈希函数，而是使用一个哈希函数，但是将哈希值的区间按位切分成多个桶（bucket）。例如我们希望取1024个数进行平均，那么我们可以取哈希值的前10比特作为桶编号，然后计算剩下部分的0前缀长度。这种方法的准确度和多哈希函数方法相当，但是比计算多个哈希效率高很多。&lt;/p&gt;
&lt;p&gt;根据上述分析，我们可以给出一个简单的算法实现。这个实现等价于Durand-Flajolet的论文中提出的LogLog算法；不过为了方便，这个实现中统计的是0尾缀而不是0前缀；其效果是等价的。&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;def trailing_zeroes(num):
    &quot;&quot;&quot;Counts the number of trailing 0 bits in num.&quot;&quot;&quot;
    if num == 0:
        return 32 # Assumes 32 bit integer inputs!
    p = 0
    while (num &gt;&gt; p) &amp; 1 == 0:
        p += 1
    return p

def estimate_cardinality(values, k):
    &quot;&quot;&quot;Estimates the number of unique elements in the input set values.

    Arguments:
        values: An iterator of hashable elements to estimate the cardinality of.
        k: The number of bits of hash to use as a bucket number; there will be 2**k buckets.
    &quot;&quot;&quot;
    num_buckets = 2 ** k
    max_zeroes = [0] * num_buckets
    for value in values:
        h = hash(value)
        bucket = h &amp; (num_buckets - 1) # Mask out the k least significant bits as bucket ID
        bucket_hash = h &gt;&gt; k
        max_zeroes[bucket] = max(max_zeroes[bucket], trailing_zeroes(bucket_hash))
    return 2 ** (float(sum(max_zeroes)) / num_buckets) * num_buckets * 0.79402&lt;/pre&gt;
    &lt;p&gt;这段代码实现了我们上面讨论的估计算法：我们计算每个桶的0前缀（或尾缀）的最长长度；然后计算这些长度的平均数；假设平均数是x，桶数量是m，则最终的估计值是\(2^x \times m\)。其中一个没提过的地方是魔法数字0.79402。统计分析显示这种预测方法存在一个可预测的偏差；这个魔法数字是对这个偏差的修正。实际经验表明计算值随着桶数量的不同而变化，不过当桶数量不太小时（大于64），计算值会收敛于估计值。原论文中描述了这个结论的推导过程。&lt;/p&gt;
    &lt;p&gt;这个方法给出的估计值比较精确 —— 在分桶数为m的情况下，平均误差为\(1.3/\sqrt{m}\)。因此对于分桶数为1024的情况（所需内存1024*5 = 5120位，或640字节），大约会有4%的平均误差；每桶5比特的存储已经足以估计\(2^{27}\)的数据集，而我们只用的不到1k的内存！&lt;/p&gt;
    &lt;p&gt;让我们看一下试验结果：&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;&gt;&gt;&gt; [100000/estimate_cardinality([random.random() for i in range(100000)], 10) for j in range(10)]
[0.9825616152548807, 0.9905752876839672, 0.979241749110407, 1.050662616357679, 0.937090578752079, 0.9878968276629505, 0.9812323203117748, 1.0456960262467019, 0.9415413413873975, 0.9608567203911741]&lt;/pre&gt;
    &lt;p&gt;不错！虽然有些估计误差大于4%的平均误差，但总体来说效果良好。如果你准备自己做一下这个试验，有一点需要注意：Python内置的 hash() 方法将整数哈希为它自己。因此诸如 estimate_cardinality(range(10000), 10) 这种方式得到的结果不会很理想，因为内置 hash() 对于这种情况并不能生成很好的散列。但是像上面例子中使用随机数会好很多。&lt;/p&gt;
    &lt;h1&gt;提升准确度：SuperLogLog和HyperLogLog&lt;/h1&gt;
    &lt;p&gt;虽然我们已经有了一个不错的估计算法，但是我们还能进一步提升算法的准确度。Durand和Flajolet发现离群点会大大降低估计准确度；如果在计算平均值前丢弃一些特别大的离群值，则可以提高精确度。特别的，通过丢弃最大的30%的桶的值，只使用较小的70%的桶的值来进行平均值计算，则平均误差可以从\(1.3/\sqrt{m}\)降低到\(1.05/\sqrt{m}\)！这意味着在我们上面的例子中，使用640个字节可情况下可以将平均误差从4%降低到3.2%，而所需内存并没有增加。&lt;/p&gt;
    &lt;p&gt;最后，Flajolet等人在HyperLogLog论文中给出一种不同的平均值，使用调和平均数取代几何平均数（译注：原文有误，此处应该是算数平均数）。这一改进可以将平均误差降到\(1.04/\sqrt{m}\)，而且并没不需要额外资源。但是这个算法比前面的算法复杂很多，因为对于不同基数的数据集要做不同的修正。有兴趣的读者可以阅读原论文。&lt;/p&gt;
    &lt;h1&gt;并行化&lt;/h1&gt;
    &lt;p&gt;这些基数估计算法的一个好处就是非常容易并行化。对于相同分桶数和相同哈希函数的情况，多台机器节点可以独立并行的执行这个算法；最后只要将各个节点计算的同一个桶的最大值做一个简单的合并就可以得到这个桶最终的值。而且这种并行计算的结果和单机计算结果是完全一致的，所需的额外消耗仅仅是小于1k的字节在不同节点间的传输。&lt;/p&gt;
    &lt;h1&gt;结论&lt;/h1&gt;
    &lt;p&gt;基数估计算法使用很少的资源给出数据集基数的一个良好估计，一般只要使用少于1k的空间存储状态。这个方法和数据本身的特征无关，而且可以高效的进行分布式并行计算。估计结果可以用于很多方面，例如流量监控（多少不同IP访问过一个服务器）以及数据库查询优化（例如我们是否需要排序和合并，或者是否需要构建哈希表）。&lt;/p&gt;
</description>
</item>
<item>
<title>从抛硬币试验看概率论的基本内容及统计方法</title>
<link>http://blog.codinglabs.org/articles/basis-of-probability-and-statistics.html</link>
<guid>http://blog.codinglabs.org/articles/basis-of-probability-and-statistics.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Tue, 20 Nov 2012 00:00:00 GMT</pubDate>
<description>&lt;p&gt;一般说到概率，就喜欢拿抛硬币做例子。大多数时候，会简单认为硬币正背面的概率各为二分之一，其实事情远没有这么简单。这篇文章会以抛硬币试验为例子并贯穿全文，引出一系列概率论和数理统计的基本内容。这篇文章会涉及的有古典概型、公理化概率、二项分布、正态分布、最大似然估计和假设检验等一系列内容。主要目的是以抛硬币试验为例说明现代数学观点下的概率是什么样子以及以概率论为基础的一些基本数理统计方法。&lt;/p&gt;
&lt;!--more--&gt;
&lt;h1&gt;概率的存在性&lt;/h1&gt;
&lt;p&gt;好吧，首先我们要回答一个基本问题就是概率为什么是存在的。其实这不是个数学问题，而是哲学问题（貌似一般存在不存在啥的都是哲学问题）。之所以要先讨论这个问题，是因为任何数学活动都是在一定哲学观点前提下进行的，如果不明确哲学前提，数学活动就无法进行了（例如如果在你的哲学观点下概率根本不存在，那还讨论啥概率论啊）。&lt;/p&gt;
&lt;p&gt;概率的存在是在一定哲学观点前提下的，我不想用哲学术语拽文，简单来说，就是你首先得承认事物是客观存在的，并可以通过大量的观察和实践被抽象总结。举个例子，我们经常会讨论“身高”，为什么我们都认为身高是存在的？因为我们经过长期的观察实践发现一个人身体的高度在短期内不会出现大幅度的变动，因此我们可以用一个有单位的数字来描述一个人的身体在一段不算长的时间内相对稳定的高度。这就是“身高”作为被普遍承认存在的哲学前提。&lt;/p&gt;
&lt;p&gt;与此相似，人们在长期的生活中，发现世界上有一些事情的结果是无法预料的，例如抛硬币得到正面还是背面，但是，后来有些人发现，虽然单次的结果不可预料，但是如果我不断抛，抛很多次，正面结果占全部抛硬币次数的比率是趋于稳定的，而且次数越多越接近某个固定的数值。换句话说，抛硬币这件事，单次结果不可预料，但是多次试验的结果却在总体上是有规律可循的（术语叫统计规律）。&lt;/p&gt;
&lt;p&gt;下面是历史上一些著名的抛硬币试验的数据记录：&lt;/p&gt;
&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;2&quot; width=&quot;400&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;&lt;strong&gt;试验者&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;&lt;strong&gt;试验次数&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;&lt;strong&gt;正面次数&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;&lt;strong&gt;正面占比&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;德摩根&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;4092&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;2048&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;50.05%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;蒲丰&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;4040&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;2048&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;50.69%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;费勒&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;10000&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;4979&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;49.79%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;皮尔逊&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;24000&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;12012&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;50.05%&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;罗曼洛夫斯基&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;80640&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;39699&lt;/td&gt;
&lt;td valign=&quot;top&quot; width=&quot;100&quot;&gt;49.23%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;可以看到，虽然这些试验在不同时间、不同地点由不同的人完成，但是冥冥中似乎有一股力量将正面的占比固定在50%附近。&lt;/p&gt;
&lt;p&gt;后来，人们发现还有很多其它不可预测的事情都与抛硬币类似，例如掷骰子、买六合彩等等，甚至渐渐发现不只这些简单的事情，人类社会方方面面从简单到复杂的很多不可预测的事情宏观上看都具有统计规律。于是人们推测，在某些条件下的一些不可预测事件，都是有统计规律的，或者直观说很多不可预测结果的试验在多次进行后总体上看结果会趋近于一些常数（这个现象后来被严格定义为大数定律，成为概率论最基础的定理之一，下文会提到）。这种可观测现象，成为概率存在的哲学基础，而这些常数就是概率在朴素观点下的定义。&lt;/p&gt;
&lt;h1&gt;概率模型&lt;/h1&gt;
&lt;p&gt;在认识到上述事实后，人们希望将这种规律加以利用（人类文明的发展不就是发现和利用规律么，呵呵），但是想要利用就首先要对概率进行严格的形式化定义，也就是要建立数学模型。比较知名的数学模型有古典概型、几何概率模型和公理化概率，本文将会讨论古典概型和公理化概率。&lt;/p&gt;
&lt;h2&gt;古典概型&lt;/h2&gt;
&lt;p&gt;古典概型是人类对概率和统计规律最早的建模尝试，表达了朴素的数学原则下人们对概率的认识。在表述古典概型之前，需要先定义一些概念。&lt;/p&gt;
&lt;p&gt;首先是随机试验。&lt;/p&gt;
&lt;p&gt;如果一个同时试验满足下面三条原则，则这个试验称为随机试验：&lt;/p&gt;
&lt;p&gt;1、可在相同条件下（相对来说）重复进行。&lt;/p&gt;
&lt;p&gt;2、可能出现的结果不止一个，但事先明确知道所有可能的结果（可以是无限个，例如所有自然数，但必须事先明确知道结果的取值范围）。&lt;/p&gt;
&lt;p&gt;3、事先无法预测在一次试验中哪一个结果会出现。&lt;/p&gt;
&lt;p&gt;显然上面的抛硬币试验是一个随机试验。&lt;/p&gt;
&lt;p&gt;然后需要定义样本空间和样本点。一个随机试验的样本空间是这个试验所有可能结果组成的集合，而其中每个元素是一个样本点。例如，抛硬币试验中，样本空间为\(\{F, B\}\)，其中F表示正面，B表示背面，而F、B就是两个样本点。&lt;/p&gt;
&lt;p&gt;另一个非常重要的概念就是随机事件（简称事件）：样本空间的一个子集称为一个事件。例如，抛硬币试验有四个不同的事件：\(\emptyset\)，\(\{F\}\)，\(\{B\}\)，\(\{F, B\}\)，分别表示“既不出现正面也不出现反面”，“出现正面”，“出现反面”和“出现正面或反面”。在不考虑硬币立起来等特殊情况时，第一个事件不可能出现，但它确实是一个合乎定义的事件，叫不可能事件；而最后一个事件必然出现，叫必然事件。&lt;/p&gt;
&lt;p&gt;有了上面概念，就可以定义古典概型了：&lt;/p&gt;
&lt;p&gt;如果一个概率模型满足 1）样本空间是一个有限集合，2）每一个基本事件（只包含一个样本点的事件）出现的概率相同，则这是一个古典概型。例如，在上面的抛硬币试验中，再定义\(\{F\}\)，\(\{B\}\)的概率均为0.5，则就构成了一个古典概型。&lt;/p&gt;
&lt;p&gt;古典概型简单、直观，在早期的概率研究中广泛被使用。但是这个模型太朴素太不严格了，在这种不完善的定义下，根本没有办法做严格的数学推理，而且有限样本空间和等可能性在很多现实随机试验中并不满足，甚至对等可能不同定义会导致不同结论。因此必须使用一个更严格的定义，以符合现代数学公理化推导的要求，这就是公理化概率。&lt;/p&gt;
&lt;h2&gt;公理化概率&lt;/h2&gt;
&lt;p&gt;公理化概率对概率做如下定义：&lt;/p&gt;
&lt;p&gt;概率是事件集合到实数域的一个函数，设事件集合为E，则如若\(A\in E\overset{p}{\rightarrow}P(A)\in \mathbb{R}\)满足：&lt;/p&gt;
&lt;p&gt;对于任意事件A，\(P(A)=0\)。&lt;/p&gt;
&lt;p&gt;对于必然事件S，\(P(S)=1\)。&lt;/p&gt;
&lt;p&gt;对于两两互斥的事件，有\(P(A_1\cup A_2\cup\cdots \cup A_n) = P(A_1)+P(A_2)+\cdots +P(A_n)\)。&lt;/p&gt;
&lt;p&gt;公理化概率对概率做了严格的数学定义，可以较好的基于公理系统进行推导和证明。但是，概率模型只是给出了概率“是什么”（定性），没有回答“是多少”（定量）这个问题。也就是说，仅有概率模型，是不能定量回答抛硬币问题的。下面介绍对概率进行定量分析的方法。&lt;/p&gt;
&lt;h1&gt;度量与估计概率&lt;/h1&gt;
&lt;p&gt;从公理化概率的角度，我们可以这样定义抛硬币试验的概率：设\(N\)是全部抛硬币的次数，而\(C_F\)是正面向上的次数，则如下函数定义了这个概率：&lt;/p&gt;
&lt;p&gt;\(P(A)=\left\{\begin{align} 0 &amp; A=\emptyset\\ \frac{C_F}{N} &amp; A=\{F\}\\ 1-\frac{C_F}{N} &amp; A=\{B\}\\ 1 &amp; A=\{F,B\} \end{align}\right.\)&lt;/p&gt;
&lt;p&gt;容易验证，这个定义完全符合公理化概率的所有条件。下面就是确定\(N\)和\(C_F\)。不幸的是，显然N是无法穷尽的，因为理论上你不可能抛无数次硬币。由于不能精确度量这个概率，因此你必须通过某个可以精确度量的值去估计这个概率，而且还要从数学上证明这个估计方法是靠谱的，最好能定量给出这个估计量的可信程度。而对不可直接观测概率的一个估计度量值就是频率。&lt;/p&gt;
&lt;h2&gt;频率估计&lt;/h2&gt;
&lt;p&gt;频率是这样定义的：事件A的频率是在相同条件下重复一个实验n次，事件A发生的次数在n次实验中的占比。一种简单的估计概率的方法就是用频率当做概率的估计。&lt;/p&gt;
&lt;p&gt;例如，我刚刚抛完十次硬币，其中六次正面，四次背面，因此根据此次实验，我估计我这枚硬币出现正面的概率为0.6。这就是频率估计。&lt;/p&gt;
&lt;p&gt;不过你一定有疑惑，为什么可以使用频率估计概率？有上面理论依据？如何对估计的准确性做出定理的分析？下面解答这些问题。&lt;/p&gt;
&lt;h2&gt;大数定律&lt;/h2&gt;
&lt;p&gt;频率估计的理论基础是大数定律。毫不夸张的说，大数定律是整个现代概率论和统计学的最重要基石，几乎一切统计方法的正确性都依赖于大数定律的正确，因此大数定律被有些人称为概率论的首要定律。&lt;/p&gt;
&lt;p&gt;大数定律直观来看表述了这样一种事实：在相同条件下，随着随机试验次数的增多，频率越来越接近于概率。注意大数定律陈述的是一个随着n趋向于无穷大时频率对真实概率的一种无限接近的趋势。&lt;/p&gt;
&lt;p&gt;下面给出大数定律的数理表述，大数定律有多重数学表述，这里取伯努利大数定律：&lt;/p&gt;
&lt;p&gt;\(\lim_{n \to \infty}{P{\left\{ \left|\frac{n_x}{n} - p \right| &lt; \varepsilon \right\}}} = 1\)&lt;/p&gt;
&lt;p&gt;其中\(n_x\)表述在n次试验中事件x出现的次数。伯努利大数定律代表的意义是，当试验次数越来越多，频率与概率相差较大的可能性变得很小。大数定律从数学上严格证明了频率对概率的收敛性以及稳定性。这就是频率估计的理论基础。在后面关于中心极限定理的部分，还将定量给出估计的置信度（表示这个估计有多可靠）。&lt;/p&gt;
&lt;h2&gt;最大似然估计&lt;/h2&gt;
&lt;p&gt;下面给出另一种估计概率的方法，就是最大似然估计。最大似然估计是参数估计的一种方法，用于在已知概率分布的情况下对分布函数的参数进行估计。而这里分布函数的参数刚好是要估计的概率。&lt;/p&gt;
&lt;p&gt;最大似然估计基于这样一个朴素的思想：如果已经得到一组试验数据，在概率分布已知的情况下，可以将出现这组试验数据的概率表述为分布函数参数的函数。&lt;/p&gt;
&lt;p&gt;看到上面的话很多人肯定又晕了，我还是举个具体的例子吧（非数学严格的例子，但思想一致）。我来到一所陌生的大学门口，想知道这所大学男生多还是女生多，我蹲在校门口数了走出校门的100名同学，发现80个男生20个女生，如果我认为这所学校每个学生这段时间内出校门的概率都是差不多的，那么我会推断男生多。因为男生多的学校更大可能性产生我观察的结果。所以，最大似然估计的核心思想就是：知道了结果，但不知道结果所在总体的情况，然后计算在总体在每种可能下产生这个结果的概率，哪种情况下产生已知结果的概率最大，就认为这种情况是总体的情况。&lt;/p&gt;
&lt;p&gt;下面正式使用这个方法估计硬币正面出现的概率。&lt;/p&gt;
&lt;p&gt;还是上面的实验，我已经得到“抛了十次，六次正面”这个结果，下面我想知道正面向上的概率。由于这个概率是一定存在的（第一节已经说明了哈，在既定哲学观点下），而且这个概率的取值范围应该是0到1的开区间（正面背面都出现过，所以不可能是0或1）：&lt;/p&gt;
&lt;p&gt;\(p\in (0,1)\)&lt;/p&gt;
&lt;p&gt;由一些背景知识知道，每抛十次硬币，正面出现的次数服从二项分布：&lt;/p&gt;
&lt;p&gt;\(C_n^kp^k(1-p)^{n-k}\)&lt;/p&gt;
&lt;p&gt;由于已知n=10，k=6，将其带入，得到一个函数：&lt;/p&gt;
&lt;p&gt;\(L(p)=C_{10}^6p^6(1-p)^{10-6}\)&lt;/p&gt;
&lt;p&gt;其中p的定义域为\(p\in (0,1)\)。这个函数表示的是，当出现正面的真实概率为p时，“抛十次六次正面”这个事件出现的概率。我们希望估计的p让这个函数取值最大，以下是求解过程：&lt;/p&gt;
&lt;p&gt;因为在(0,1)区间，ln(x)是x的单调递增函数，所以最大化lnL(p)就等于最大化L(p)。这样做主要是取对数可以让连乘变成连加，方便后面求导。&lt;/p&gt;
&lt;p&gt;由微积分知识可知：&lt;/p&gt;
&lt;p&gt;\(\frac{dlnF(p)}{dp}=C_{10}^6(\frac{6}{p}+\frac{4}{p-1})=C_{10}^6(\frac{10p-6}{p^2-p})\)&lt;/p&gt;
&lt;p&gt;让这个导数为0，解得p为0.6，这就是我们对概率的最大似然估计，与概率估计的结果一致。&lt;/p&gt;
&lt;h1&gt;显著性及假设检验&lt;/h1&gt;
&lt;p&gt;到此为止，我们已经说明了概率是存在的、建立了概率的数学模型，并能对不可直接观测的概率进行估计。但似乎还缺点什么。&lt;/p&gt;
&lt;p&gt;大数定律只说明了理论上我们的估计是靠谱的，但是到底有多靠谱，却无法通过大数定律定量计算。这一节，我们就来解决这个问题：定量计算出估计的可靠性（术语叫显著性）。&lt;/p&gt;
&lt;h2&gt;评估显著性&lt;/h2&gt;
&lt;p&gt;还是上面我抛那十次硬币的试验。根据最优的频率估计和最大似然估计，均估计p（出现正面的概率）为0.6。但是如果有人提出异议，说我的估计可能是错的，p实际是0.5，我那个出现六次正面是因为只是偶然性的结果。这时我需要找证据反驳他，由于不能做无数次试验，我只能给出一个较高可信度的证据，例如，我想证明至少95%的可能性出现六次正面是因为p不等于0.5，也就是说，证明如果p为0.5，则偶然出现我这个结果的可能性不超过5%（5%称作显著水平）。&lt;/p&gt;
&lt;h2&gt;中心极限定理&lt;/h2&gt;
&lt;p&gt;要评估显著性，首先要借助于中心极限定理。中心极限定理也是统计学的基石定理之一，它的一种表述是：&lt;/p&gt;
&lt;p&gt;设随机变量\(X_1, X_2, \cdots ,X_n\)独立同分布，且数学期望为\(\mu\)，方差为\(\sigma^2 \neq 0\)。则其均值\(\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}\)近似服从期望为\(\mu\)，方差为\(\sigma/n\)的正态分布。等价的，\(\zeta_n=\frac{\bar{X} -\mu}{\sigma/\sqrt{n}}\)近似服从标准正态分布。&lt;/p&gt;
&lt;p&gt;中心极限定理的直观意义是，随便一个服从什么的总体中，你独立随机的抽取一组样本，那么样本的均值服从正态分布，并且可以根据总体的期望和方差推导出这个均值服从的正态分布的期望和方差，然后简单变换一下就可以得到一个服从标准正态分布的随机量。由于标准正态分布的概率密度函数是已知的，那么就可以得到这个量出现的概率。&lt;/p&gt;
&lt;p&gt;这样说貌似太抽象了，我们下面还是看这个定理的应用实例吧。&lt;/p&gt;
&lt;h2&gt;假设检验&lt;/h2&gt;
&lt;p&gt;上面说过，我要反驳的是抛硬币得到正面的实际概率是0.5，那么我就要证明如果p是0.5，则得到这组结果的概率是很小的（上面要求小于5%）。&lt;/p&gt;
&lt;p&gt;设正面取值为1，背面取值为0。如果p是0.5，则每一次抛硬币的取值服从一个p为0.5的0-1分布。由期望及方差的定义可知，这个分布的期望和方差分别为：&lt;/p&gt;
&lt;p&gt;\(\mu = p\times 1 + (1-p)\times 0 = 0.5\times 1 + (1-0.5)\times 0=0.5\)&lt;/p&gt;
&lt;p&gt;\(\sigma^2= (1-\mu)^2\times 0.5 + (0-\mu)^2\times 0.5 = 0.25\times 0.5 + 0.25\times 0.5 = 0.25\)&lt;/p&gt;
&lt;p&gt;由中心极限定理\(\frac{\bar{X}-0.5}{\sqrt{0.25/10}}\)近似服从标准正态分布。&lt;/p&gt;
&lt;p&gt;而我抛的十次硬币可以看做十个独立随机抽样，它们的均值是0.6，变换后的值为\(\frac{0.6-0.5}{\sqrt{0.25/10}}\approx 0.632\)。&lt;/p&gt;
&lt;p&gt;标准正态分布的概率密度公式为：&lt;/p&gt;
&lt;p&gt;\(f(x) = {1 \over \sigma\sqrt{2\pi} }\,e^{- {{(x-\mu )^2 \over 2\sigma^2}}}\)&lt;/p&gt;
&lt;p&gt;上面说过，我们希望显著水平是5%，所以，我需要找到x=z，使得此概率密度函数从-z到z的定积分为0.95，然后看0.632在不在[-z, z]内，如果在的话，我会认为我确实错了，至少我没有95%以上的把握说p不等于0.5，而如果0.632不再这个范围内，则我可以拍着胸脯说，我已经从理论上证明我有95%以上的把握，p不是0.5（换句话说，如果p是0.5，抛十次六次正面的可能性不足5%）。&lt;/p&gt;
&lt;p&gt;坦白说这个z不是很好算，不过还好由于这东西特别常用，任何一本概率课本后面都可以找到标准正态分布表（或者很多工具如R语言可以直接计算分位点），下面就是我在网上找到的一个（来源&lt;a title=&quot;http://www.mathsisfun.com/data/standard-normal-distribution-table.html&quot; href=&quot;http://www.mathsisfun.com/data/standard-normal-distribution-table.html&quot;&gt;http://www.mathsisfun.com/data/standard-normal-distribution-table.html&lt;/a&gt;）：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/basis-of-probability-and-statistics/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;这是一个单侧表，要保证显著水平为5%，则单侧积分上限不能低于0.475，通过查上表，可知0.475对应的z是1.96，远大于我们算出的0.632。很不幸，我在5%的显著水平下无法拒绝p=0.5的假设。同时通过上表可以看到，0.63对应的单侧概率是0.2357，也就是说，通过抛十次得到六次正面，我们只有约50%的把握说出现正面的概率不是0.5。换句话说，抛十次硬币来做频率估计是不太合适的，于是，我们需要增加试验次数。&lt;/p&gt;
&lt;p&gt;假如，我又做了100次实验，抛出了60次正面，40次背面。那么这个试验结果可以显著的认为p不是0.5吗？用同样的方法算出\(\frac{0.6-0.5}{\sqrt{0.25/100}} = 2.0\)。很显然，2.0大于1.96，所以这个试验结果可以充分（超过95%的可能）说明这枚硬币正面朝上的概率确实不是0.5。通过查表可以看到，2.0的显著水平约为0.046，换句话说，这次试验结果95.4%以上表明硬币正面出现的概率不是0.5。当然，也有可能结论是错误的，因为毕竟还有4.6%的可能这是在p=0.5的情况下偶然出现的。&lt;/p&gt;
&lt;p&gt;通过假设检验理论，可以通过增加试验次数，将犯错的概率缩小到任意小的值。&lt;/p&gt;
&lt;h1&gt;总结&lt;/h1&gt;
&lt;p&gt;这篇文章以抛硬币试验为引子引出了一系列现代数学中概率的基本模型、定理及基本的估计及显著性检验方法。写这篇文章是我无聊抛硬币时一时兴起，其中对很多东西只是给出一个轮廓，没有处处给出严格的定义和证明，不过大约说明了常用的一些统计方法及其理论基础，限于篇幅不能面面俱到，例如一个假设检验如果展开写可以单独写一篇文章。目前随着大数据概念的热炒，基于互联网的数据挖掘和机器学习也变得火热，其实很多数据挖掘和机器学习都是基于概率和统计理论的，很多方法甚至只是传统统计方法的应用。因此如果准备在这方面深入学习，不妨考虑先在概率论和数理统计方面打好基础。&lt;/p&gt;
</description>
</item>
<item>
<title>x86-64体系下一个奇怪问题的定位</title>
<link>http://blog.codinglabs.org/articles/trouble-of-x86-64-platform.html</link>
<guid>http://blog.codinglabs.org/articles/trouble-of-x86-64-platform.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Tue, 13 Nov 2012 00:00:00 GMT</pubDate>
<description>&lt;p&gt;问题来源于一个朋友在百度的笔试题。上周六我一个朋友参加了百度举行的专场招聘会，其中第一道笔试题是这样的：&lt;/p&gt;
&lt;p&gt;
&lt;hr&gt;
&lt;p&gt;给出下面一段代码&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;#include &lt;stdio.h&gt;
main() {
    double a = 10; 
    printf(&quot;a = %d\n&quot;, a); 
}&lt;/pre&gt;
&lt;p&gt;请问代码的运行结果以及原因。&lt;/p&gt;
&lt;p&gt;
&lt;hr&gt;
&lt;p&gt;当朋友参加完笔试和我聊起这道题时，我第一反应是这道题考察的是浮点数的内存表示，当然，在不同的CPU体系下，运行结果可能会有所不同，主要是受CPU位数和字节序的影响。&lt;/p&gt;
&lt;!--more--&gt;
&lt;h1&gt;最初分析&lt;/h1&gt;
&lt;p&gt;不妨以目前最普遍的x86-64体系（64位，小端序）考虑此问题。在64位机器上，double是符合&lt;a href=&quot;http://zh.wikipedia.org/wiki/IEEE_754&quot; target=&quot;_blank&quot;&gt;IEEE754标准&lt;/a&gt;的双精度浮点数。根据IEEE标准，双精度浮点数由8个字节共64位组成，其中最高位为符号位，次高的11位为指数位，余下的52位为尾数位。示意见下图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/trouble-of-x86-64-platform/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;各位段意义如下：&lt;/p&gt;
&lt;p&gt;S = 0表示正数，S = 1表示负数。&lt;/p&gt; 
&lt;p&gt;E可以看成一个无符号整数，当其二进制位为全0或全1时，表示非规约浮点数或特殊值，此处不讨论，仅讨论其不全为0或全为1的情况。当E不全为零或全为1时，浮点数是规约的，此时E表示以2为底的指数加上一个固定的偏移量。偏移量被定义为\(2^{(E) - 1} - 1\)，其中(E)表示E所占的比特数，此处为11，所以偏移量为\(2^{(11) - 1} - 1=1023\)。因此实际的指数值要在E的基础上减1023，例如E的位表示是10000000000（十进制1024），则表示实际指数值为\(1024-1023=1\)。&lt;/p&gt;
&lt;p&gt;M在规约形式下，表示一个二进制小数，实际值是这个小数加1。例如，M=101000…0表示\(2^{-1} + 2^{-3} + 1 = 1.625\)。&lt;/p&gt; 
&lt;p&gt;一个规约的IEEE双精度浮点数的实际值为：&lt;/p&gt;
&lt;p&gt;\(V = (-1)^S \times 2^{E - 1023} \times (1 + M)\)&lt;/p&gt;
&lt;p&gt;根据以上分析，10可以表示为\(1.25 \times 8\)，因此取S = 0，E = 10000000010，M = 0100…0，则整个浮点数的二进制表示为：&lt;/p&gt;
&lt;p&gt;01000000, 00100100, 00000000, 00000000, 00000000, 00000000, 00000000, 00000000&lt;/p&gt;
&lt;p&gt;为了便于观察我在每8bit之间插入了分隔符。当printf使用“%d”输出时，由于int类型是4字节，所以只能取其中四个字节。当a被当做参数传递给printf时，有两种可能保存a的地方：寄存器或栈帧中。&lt;/p&gt;
&lt;p&gt;如果是寄存器，则printf会取低四字节。&lt;/p&gt;
&lt;p&gt;如果是栈，在小端序中，高字节存放在高地址，低字节放在低地址，而栈是从高地址向低地址增长的，所以入栈后每个字节的位置如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/trouble-of-x86-64-platform/2.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;printf会从低地址到高地址读取4个字节当做int型数据去解释并输出，所以，经过分析这段代码的输出应该为“a = 0”。&lt;/p&gt;
&lt;h1&gt;奇怪的结果&lt;/h1&gt;
&lt;p&gt;分析完了，下一步当然是通过实践验证，我在我的VPS上（CentOS 64位）用gcc编译。结果非常出乎意料，不但不是0，而且每次运行的结果都不一样！（见下图）&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/trouble-of-x86-64-platform/3.png&quot;/&gt;&lt;/p&gt;
&lt;h1&gt;定位问题&lt;/h1&gt;
&lt;p&gt;在试图解释这个奇怪现象时，我最初从C的层面上进行了诸多分析，结果都无法分析出问题所在，所以我怀疑出现这个问题的原因在机器代码层面。于是我将其汇编代码打出来：&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;.file   &quot;double_as_int.c&quot;
.section    .rodata.str1.1,&quot;aMS&quot;,@progbits,1
.LC1:
.string &quot;a = %d\n&quot;
.text
.globl main
.type   main, @function
main:
.LFB11:
.cfi_startproc
subq    $8, %rsp
.cfi_def_cfa_offset 16
movsd   .LC0(%rip), %xmm0
movl    $.LC1, %edi
movl    $1, %eax
call    printf
addq    $8, %rsp
.cfi_def_cfa_offset 8
ret 
.cfi_endproc
.LFE11:
.size   main, .-main
.section    .rodata.cst8,&quot;aM&quot;,@progbits,8
.align 8
.LC0:
.long   0   
.long   1076101120
.ident  &quot;GCC: (GNU) 4.4.6 20120305 (Red Hat 4.4.6-4)&quot;
.section    .note.GNU-stack,&quot;&quot;,@progbits&lt;/pre&gt;
&lt;p&gt;为了方便对比，我重新写了下面的C代码：&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;#include &lt;stdio.h&gt;

main() {
    int a = 10; 
    printf(&quot;a = %d\n&quot;, a); 
}&lt;/pre&gt;
&lt;p&gt;其汇编为：&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;.file   &quot;int_as_int.c&quot;
.section    .rodata.str1.1,&quot;aMS&quot;,@progbits,1
.LC0:
.string &quot;a = %d\n&quot;
.text
.globl main
.type   main, @function
main:
.LFB11:
.cfi_startproc
subq    $8, %rsp
.cfi_def_cfa_offset 16
movl    $10, %esi
movl    $.LC0, %edi
movl    $0, %eax
call    printf
addq    $8, %rsp
.cfi_def_cfa_offset 8
ret 
.cfi_endproc
.LFE11:
.size   main, .-main
.ident  &quot;GCC: (GNU) 4.4.6 20120305 (Red Hat 4.4.6-4)&quot;
.section    .note.GNU-stack,&quot;&quot;,@progbits&lt;/pre&gt;
&lt;p&gt;将注意力集中在main函数中调用printf之前的行为，可以看到，在第一段代码中，LC0有个常数1076101120，将其转换为二进制刚好是我们上面分析的双精度10的二进制表示，而汇编代码将这个数送入了一个叫xmm0寄存器。通过查阅x86-64处理器的相关资料，知道这个寄存器和&lt;a href=&quot;http://en.wikipedia.org/wiki/SIMD&quot; target=&quot;_blank&quot;&gt;SIMD&lt;/a&gt;（单指令多数据流）扩展指令集有关。简单来说，在64位操作系统下，x86-64通过SIMD机制提高浮点运算能力，所以double类型的a被送入了xmm0（SIMD会用到8个128bit寄存器，xmm0 - xmm7）。&lt;/p&gt;
&lt;p&gt;对比一下第二段代码，当a被声明是int类型时，立即数10被送入了esi（一个通用寄存器，在64位CPU中表示rsi的低32位）。其它部分似乎没有区别。&lt;/p&gt;
&lt;p&gt;通过对比，我猜测64位操作系统下由于启用了SIMD，浮点数会被送入mmx寄存器，而整形会被送入通用寄存器。为了证实我的想法，我查阅了&lt;a href=&quot;http://www.x86-64.org/documentation/abi.pdf&quot; target=&quot;_blank&quot;&gt;x86-64的ABI文档&lt;/a&gt;，在“3.2.3 Parameter Passing”一小节找到了如下的文字：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;INTEGER&lt;/strong&gt; This class consists of integral types that ﬁt into one of the general purpose registers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SSE&lt;/strong&gt; The class consists of types that ﬁt into a vector register.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;这段话和相关汇编代码基本印证了我的猜测。为了进一步验证，我考虑手工改一下汇编代码，将movsd .LC0(%rip), %xmm0改为将数据送入rsi（其低32位就是esi），修改后代码如下，注意第13行代码是我修改过的：&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;.file   &quot;double_as_int.c&quot;
.section    .rodata.str1.1,&quot;aMS&quot;,@progbits,1
.LC1:
.string &quot;a = %d\n&quot;
.text
.globl main
.type   main, @function
main:
.LFB11:
.cfi_startproc
subq    $8, %rsp
.cfi_def_cfa_offset 16
movq    .LC0(%rip), %rsi
movl    $.LC1, %edi
movl    $1, %eax
call    printf
addq    $8, %rsp
.cfi_def_cfa_offset 8
ret 
.cfi_endproc
.LFE11:
.size   main, .-main
.section    .rodata.cst8,&quot;aM&quot;,@progbits,8
.align 8
.LC0:
.long   0   
.long   1076101120
.ident  &quot;GCC: (GNU) 4.4.6 20120305 (Red Hat 4.4.6-4)&quot;
.section    .note.GNU-stack,&quot;&quot;,@progbits&lt;/pre&gt;
&lt;p&gt;编译这段汇编代码执行，果然结果固定为0：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/trouble-of-x86-64-platform/4.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;最后，我用-m32指令编译成32位代码，结果也固定为0，并且汇编代码中没有看到mmx相关寄存器的使用。然后，我手工用movl将12345送入esi，结果为输出总为12345，证明printf默认认为第一个int参数放在esi中。至此问题原因基本确定。&lt;/p&gt;
&lt;h1&gt;总结&lt;/h1&gt;
&lt;p&gt;从上述过程知道，最初的笔试代码，在64位环境下，浮点数参数被送入mmx寄存器，而%d告诉printf第一个参数为int类型，所以printf仍然去默认的esi中寻找第一个int参数，所以从esi中读取了一个未确定的32bit数据并按int解释，最终造成结果的不确定。&lt;/p&gt;
&lt;p&gt;所以这道题的正确答案（小端序）是，在32位下，输出为“a = 0”；在64位启用SIMD情况下，输出结果不确定。&lt;/p&gt;
&lt;p&gt;特别需要说明的是，由于汇编代码在不同CPU、不同操作系统、不同gcc选项下可能会有差异，所以你得到的汇编代码未必和我的相同，但原因是一致的：64位环境下int和double放置的位置不同，double告诉a放到一个地方，而%d告诉printf到另一个地方取数据，结果自然无法取到变量a。&lt;/p&gt;
&lt;p&gt;由此也可以看出，printf最好不要将占位符和实际参数设为不同的类型，因为这样会造成不可预料的结果。&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;http://www.x86-64.org/documentation/abi.pdf&quot; target=&quot;_blank&quot;&gt;System V Application Binary Interface AMD64 Architecture Processor Supplement&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://grouper.ieee.org/groups/754/&quot; target=&quot;_blank&quot;&gt;IEEE 754: Standard for Binary Floating-Point Arithmetic&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.cs.virginia.edu/~evans/cs216/guides/x86.html&quot; target=&quot;_blank&quot;&gt;x86 Assembly Guide&lt;/a&gt;&lt;/p&gt;
</description>
</item>
<item>
<title>网站统计中的数据收集原理及实现</title>
<link>http://blog.codinglabs.org/articles/how-web-analytics-data-collection-system-work.html</link>
<guid>http://blog.codinglabs.org/articles/how-web-analytics-data-collection-system-work.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Wed, 24 Oct 2012 00:00:00 GMT</pubDate>
<description>&lt;p&gt;网站数据统计分析工具是网站站长和运营人员经常使用的一种工具，比较常用的有&lt;a href=&quot;http://www.google.com/analytics/&quot; target=&quot;_blank&quot;&gt;谷歌分析&lt;/a&gt;、&lt;a href=&quot;http://tongji.baidu.com&quot; target=&quot;_blank&quot;&gt;百度统计&lt;/a&gt;和&lt;a href=&quot;http://ta.qq.com&quot; target=&quot;_blank&quot;&gt;腾讯分析&lt;/a&gt;等等。所有这些统计分析工具的第一步都是网站访问数据的收集。目前主流的数据收集方式基本都是基于javascript的。本文将简要分析这种数据收集的原理，并一步一步实际搭建一个实际的数据收集系统。
&lt;!--more--&gt;
&lt;/p&gt;
&lt;h1&gt;数据收集原理分析&lt;/h1&gt;
&lt;p&gt;简单来说，网站统计分析工具需要收集到用户浏览目标网站的行为（如打开某网页、点击某按钮、将商品加入购物车等）及行为附加数据（如某下单行为产生的订单金额等）。早期的网站统计往往只收集一种用户行为：页面的打开。而后用户在页面中的行为均无法收集。这种收集策略能满足基本的流量分析、来源分析、内容分析及访客属性等常用分析视角，但是，随着ajax技术的广泛使用及电子商务网站对于电子商务目标的统计分析的需求越来越强烈，这种传统的收集策略已经显得力不能及。&lt;/p&gt;
&lt;p&gt;后来，Google在其产品谷歌分析中创新性的引入了可定制的数据收集脚本，用户通过谷歌分析定义好的可扩展接口，只需编写少量的javascript代码就可以实现自定义事件和自定义指标的跟踪和分析。目前百度统计、搜狗分析等产品均照搬了谷歌分析的模式。&lt;/p&gt;
&lt;p&gt;其实说起来两种数据收集模式的基本原理和流程是一致的，只是后一种通过javascript收集到了更多的信息。下面看一下现在各种网站统计工具的数据收集基本原理。&lt;/p&gt;
&lt;h2&gt;流程概览&lt;/h2&gt;
&lt;p&gt;首先通过一幅图总体看一下数据收集的基本流程。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/1.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图1. 网站统计数据收集基本流程&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;首先，用户的行为会触发浏览器对被统计页面的一个http请求，这里姑且先认为行为就是打开网页。当网页被打开，页面中的埋点javascript片段会被执行，用过相关工具的朋友应该知道，一般网站统计工具都会要求用户在网页中加入一小段javascript代码，这个代码片段一般会动态创建一个script标签，并将src指向一个单独的js文件，此时这个单独的js文件（图1中绿色节点）会被浏览器请求到并执行，这个js往往就是真正的数据收集脚本。数据收集完成后，js会请求一个后端的数据收集脚本（图1中的backend），这个脚本一般是一个伪装成图片的动态脚本程序，可能由php、python或其它服务端语言编写，js会将收集到的数据通过http参数的方式传递给后端脚本，后端脚本解析参数并按固定格式记录到访问日志，同时可能会在http响应中给客户端种植一些用于追踪的cookie。&lt;/p&gt;
&lt;p&gt;上面是一个数据收集的大概流程，下面以谷歌分析为例，对每一个阶段进行一个相对详细的分析。&lt;/p&gt;
&lt;h2&gt;埋点脚本执行阶段&lt;/h2&gt;
&lt;p&gt;若要使用谷歌分析（以下简称GA），需要在页面中插入一段它提供的javascript片段，这个片段往往被称为埋点代码。下面是我的博客中所放置的谷歌分析埋点代码截图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/2.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图2. 谷歌分析埋点代码&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;其中_gaq是GA的的全局数组，用于放置各种配置，其中每一条配置的格式为：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;_gaq.push(['Action', 'param1', 'param2', ...]);&lt;/pre&gt;
&lt;p&gt;Action指定配置动作，后面是相关的参数列表。GA给的默认埋点代码会给出两条预置配置，_setAccount用于设置网站标识ID，这个标识ID是在注册GA时分配的。_trackPageview告诉GA跟踪一次页面访问。更多配置请参考：&lt;a title=&quot;https://developers.google.com/analytics/devguides/collection/gajs/&quot; href=&quot;https://developers.google.com/analytics/devguides/collection/gajs/&quot; target=&quot;_blank&quot;&gt;https://developers.google.com/analytics/devguides/collection/gajs/&lt;/a&gt;。实际上，这个_gaq是被当做一个FIFO队列来用的，配置代码不必出现在埋点代码之前，具体请参考上述链接的说明。&lt;/p&gt;
&lt;p&gt;就本文来说，_gaq的机制不是重点，重点是后面匿名函数的代码，这才是埋点代码真正要做的。这段代码的主要目的就是引入一个外部的js文件（ga.js），方式是通过document.createElement方法创建一个script并根据协议（http或https）将src指向对应的ga.js，最后将这个element插入页面的dom树上。&lt;/p&gt;
&lt;p&gt;注意ga.async = true的意思是异步调用外部js文件，即不阻塞浏览器的解析，待外部js下载完成后异步执行。这个属性是HTML5新引入的。&lt;/p&gt;
&lt;h2&gt;数据收集脚本执行阶段&lt;/h2&gt;
&lt;p&gt;数据收集脚本（ga.js）被请求后会被执行，这个脚本一般要做如下几件事：&lt;/p&gt;
&lt;p&gt;1、通过浏览器内置javascript对象收集信息，如页面title（通过document.title）、referrer（上一跳url，通过document.referrer）、用户显示器分辨率（通过windows.screen）、cookie信息（通过document.cookie）等等一些信息。&lt;/p&gt;
&lt;p&gt;2、解析_gaq收集配置信息。这里面可能会包括用户自定义的事件跟踪、业务数据（如电子商务网站的商品编号等）等。&lt;/p&gt;
&lt;p&gt;3、将上面两步收集的数据按预定义格式解析并拼接。&lt;/p&gt;
&lt;p&gt;4、请求一个后端脚本，将信息放在http request参数中携带给后端脚本。&lt;/p&gt;
&lt;p&gt;这里唯一的问题是步骤4，javascript请求后端脚本常用的方法是ajax，但是ajax是不能跨域请求的。这里ga.js在被统计网站的域内执行，而后端脚本在另外的域（GA的后端统计脚本是&lt;a href=&quot;http://www.google-analytics.com/__utm.gif&quot;&gt;http://www.google-analytics.com/__utm.gif&lt;/a&gt;），ajax行不通。一种通用的方法是js脚本创建一个Image对象，将Image对象的src属性指向后端脚本并携带参数，此时即实现了跨域请求后端。这也是后端脚本为什么通常伪装成gif文件的原因。通过http抓包可以看到ga.js对__utm.gif的请求：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/3.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图3. 后端脚本请求的http包&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;可以看到ga.js在请求__utm.gif时带了很多信息，例如utmsr=1280×1024是屏幕分辨率，utmac=UA-35712773-1是_gaq中解析出的我的GA标识ID等等。&lt;/p&gt;
&lt;p&gt;值得注意的是，__utm.gif未必只会在埋点代码执行时被请求，如果用_trackEvent配置了事件跟踪，则在事件发生时也会请求这个脚本。&lt;/p&gt;
&lt;p&gt;由于ga.js经过了压缩和混淆，可读性很差，我们就不分析了，具体后面实现阶段我会实现一个功能类似的脚本。&lt;/p&gt;
&lt;h2&gt;后端脚本执行阶段&lt;/h2&gt;
&lt;p&gt;GA的__utm.gif是一个伪装成gif的脚本。这种后端脚本一般要完成以下几件事情：&lt;/p&gt;
&lt;p&gt;1、解析http请求参数的到信息。&lt;/p&gt;
&lt;p&gt;2、从服务器（WebServer）中获取一些客户端无法获取的信息，如访客ip等。&lt;/p&gt;
&lt;p&gt;3、将信息按格式写入log。&lt;/p&gt;
&lt;p&gt;5、生成一副1×1的空gif图片作为响应内容并将响应头的Content-type设为image/gif。&lt;/p&gt;
&lt;p&gt;5、在响应头中通过Set-cookie设置一些需要的cookie信息。&lt;/p&gt;
&lt;p&gt;之所以要设置cookie是因为如果要跟踪唯一访客，通常做法是如果在请求时发现客户端没有指定的跟踪cookie，则根据规则生成一个全局唯一的cookie并种植给用户，否则Set-cookie中放置获取到的跟踪cookie以保持同一用户cookie不变（见图4）。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/4.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图4. 通过cookie跟踪唯一用户的原理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这种做法虽然不是完美的（例如用户清掉cookie或更换浏览器会被认为是两个用户），但是是目前被广泛使用的手段。注意，如果没有跨站跟踪同一用户的需求，可以通过js将cookie种植在被统计站点的域下（GA是这么做的），如果要全网统一定位，则通过后端脚本种植在服务端域下（我们待会的实现会这么做）。&lt;/p&gt;
&lt;h1&gt;系统的设计实现&lt;/h1&gt;
&lt;p&gt;根据上述原理，我自己搭建了一个访问日志收集系统。总体来说，搭建这个系统要做如下的事：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/5.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图5. 访问数据收集系统工作分解&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面详述每一步的实现。我将这个系统叫做MyAnalytics。&lt;/p&gt;
&lt;h2&gt;确定收集的信息&lt;/h2&gt;
&lt;p&gt;为了简单起见，我不打算实现GA的完整数据收集模型，而是收集以下信息。&lt;/p&gt;
&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;2&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;名称&lt;/td&gt;
&lt;td&gt;途径&lt;/td&gt;
&lt;td&gt;备注&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;访问时间&lt;/td&gt;
&lt;td&gt;web server&lt;/td&gt;
&lt;td&gt;Nginx $msec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IP&lt;/td&gt;
&lt;td&gt;web server&lt;/td&gt;
&lt;td&gt;Nginx $remote_addr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;域名&lt;/td&gt;
&lt;td&gt;javascript&lt;/td&gt;
&lt;td&gt;document.domain&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;URL&lt;/td&gt;
&lt;td&gt;javascript&lt;/td&gt;
&lt;td&gt;document.URL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;页面标题&lt;/td&gt;
&lt;td&gt;javascript&lt;/td&gt;
&lt;td&gt;document.title&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;分辨率&lt;/td&gt;
&lt;td&gt;javascript&lt;/td&gt;
&lt;td&gt;window.screen.height &amp; width&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;颜色深度&lt;/td&gt;
&lt;td&gt;javascript&lt;/td&gt;
&lt;td&gt;window.screen.colorDepth&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Referrer&lt;/td&gt;
&lt;td&gt;javascript&lt;/td&gt;
&lt;td&gt;document.referrer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;浏览客户端&lt;/td&gt;
&lt;td&gt;web server&lt;/td&gt;
&lt;td&gt;Nginx $http_user_agent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;客户端语言&lt;/td&gt;
&lt;td&gt;javascript&lt;/td&gt;
&lt;td&gt;navigator.language&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;访客标识&lt;/td&gt;
&lt;td&gt;cookie&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;网站标识&lt;/td&gt;
&lt;td&gt;javascript&lt;/td&gt;
&lt;td&gt;自定义对象&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;埋点代码&lt;/h2&gt;
&lt;p&gt;埋点代码我将借鉴GA的模式，但是目前不会将配置对象作为一个FIFO队列用。一个埋点代码的模板如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;&lt;script type=&quot;text/javascript&quot;&gt;
var _maq = _maq || [];
_maq.push(['_setAccount', '网站标识']);

(function() {
    var ma = document.createElement('script'); ma.type = 'text/javascript'; ma.async = true;
    ma.src = ('https:' == document.location.protocol ? 'https://analytics' : 'http://analytics') + '.codinglabs.org/ma.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ma, s);
})();
&lt;/script&gt;&lt;/pre&gt;
&lt;p&gt;这里我启用了二级域名analytics.codinglabs.org，统计脚本的名称为ma.js。当然这里有一点小问题，因为我并没有https的服务器，所以如果一个https站点部署了代码会有问题，不过这里我们先忽略吧。&lt;/p&gt;
&lt;h2&gt;前端统计脚本&lt;/h2&gt;
&lt;p&gt;我写了一个不是很完善但能完成基本工作的统计脚本ma.js：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;(function () {
    var params = {};
    //Document对象数据
    if(document) {
        params.domain = document.domain || ''; 
        params.url = document.URL || ''; 
        params.title = document.title || ''; 
        params.referrer = document.referrer || ''; 
    }   
    //Window对象数据
    if(window &amp;&amp; window.screen) {
        params.sh = window.screen.height || 0;
        params.sw = window.screen.width || 0;
        params.cd = window.screen.colorDepth || 0;
    }   
    //navigator对象数据
    if(navigator) {
        params.lang = navigator.language || ''; 
    }   
    //解析_maq配置
    if(_maq) {
        for(var i in _maq) {
            switch(_maq[i][0]) {
                case '_setAccount':
                    params.account = _maq[i][1];
                    break;
                default:
                    break;
            }   
        }   
    }   
    //拼接参数串
    var args = ''; 
    for(var i in params) {
        if(args != '') {
            args += '&amp;';
        }   
        args += i + '=' + encodeURIComponent(params[i]);
    }   

    //通过Image对象请求后端脚本
    var img = new Image(1, 1); 
    img.src = 'http://analytics.codinglabs.org/1.gif?' + args;
})();&lt;/pre&gt;
&lt;p&gt;整个脚本放在匿名函数里，确保不会污染全局环境。功能在原理一节已经说明，不再赘述。其中1.gif是后端脚本。&lt;/p&gt;
&lt;h2&gt;日志格式&lt;/h2&gt;
&lt;p&gt;日志采用每行一条记录的方式，采用不可见字符^A（ascii码0x01，Linux下可通过ctrl + v ctrl + a输入，下文均用“^A”表示不可见字符0x01），具体格式如下：&lt;/p&gt;
&lt;p&gt;时间&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;IP&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;域名&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;URL&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;页面标题&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;&lt;span style=&quot;color: #000000;&quot;&gt;Referrer&lt;/span&gt;&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;分辨率高&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;分辨率宽&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;颜色深度&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;语言&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;客户端信息&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;用户标识&lt;span style=&quot;color: #4bacc6;&quot;&gt;^A&lt;/span&gt;网站标识&lt;/p&gt;
&lt;h2&gt;后端脚本&lt;/h2&gt;
&lt;p&gt;为了简单和效率考虑，我打算直接使用nginx的access_log做日志收集，不过有个问题就是nginx配置本身的逻辑表达能力有限，所以我选用了&lt;a href=&quot;http://openresty.org/&quot; target=&quot;_blank&quot;&gt;OpenResty&lt;/a&gt;做这个事情。OpenResty是一个基于Nginx扩展出的高性能应用开发平台，内部集成了诸多有用的模块，其中的核心是通过ngx_lua模块集成了Lua，从而在nginx配置文件中可以通过Lua来表述业务。关于这个平台我这里不做过多介绍，感兴趣的同学可以参考其官方网站&lt;a title=&quot;http://openresty.org/&quot; href=&quot;http://openresty.org/&quot; target=&quot;_blank&quot;&gt;http://openresty.org/&lt;/a&gt;，或者这里有其作者章亦春（agentzh）做的一个非常有爱的介绍OpenResty的slide：&lt;a href=&quot;http://agentzh.org/misc/slides/ngx-openresty-ecosystem/&quot; target=&quot;_blank&quot;&gt;http://agentzh.org/misc/slides/ngx-openresty-ecosystem/&lt;/a&gt;，关于ngx_lua可以参考：&lt;a title=&quot;https://github.com/chaoslawful/lua-nginx-module&quot; href=&quot;https://github.com/chaoslawful/lua-nginx-module&quot; target=&quot;_blank&quot;&gt;https://github.com/chaoslawful/lua-nginx-module&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;首先，需要在nginx的配置文件中定义日志格式：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;log_format tick &quot;$msec^A$remote_addr^A$u_domain^A$u_url^A$u_title^A$u_referrer^A$u_sh^A$u_sw^A$u_cd^A$u_lang^A$http_user_agent^A$u_utrace^A$u_account&quot;;&lt;/pre&gt;
&lt;p&gt;注意这里以u_开头的是我们待会会自己定义的变量，其它的是nginx内置变量。&lt;/p&gt;
&lt;p&gt;然后是核心的两个location：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;location /1.gif {
#伪装成gif文件
    default_type image/gif;    
#本身关闭access_log，通过subrequest记录log
    access_log off;

    access_by_lua &quot;
        -- 用户跟踪cookie名为__utrace
        local uid = ngx.var.cookie___utrace        
        if not uid then
            -- 如果没有则生成一个跟踪cookie，算法为md5(时间戳+IP+客户端信息)
            uid = ngx.md5(ngx.now() .. ngx.var.remote_addr .. ngx.var.http_user_agent)
        end 
        ngx.header['Set-Cookie'] = {'__utrace=' .. uid .. '; path=/'}
        if ngx.var.arg_domain then
        -- 通过subrequest到/i-log记录日志，将参数和用户跟踪cookie带过去
            ngx.location.capture('/i-log?' .. ngx.var.args .. '&amp;utrace=' .. uid)
        end 
    &quot;;  

    #此请求不缓存
    add_header Expires &quot;Fri, 01 Jan 1980 00:00:00 GMT&quot;;
    add_header Pragma &quot;no-cache&quot;;
    add_header Cache-Control &quot;no-cache, max-age=0, must-revalidate&quot;;

    #返回一个1×1的空gif图片
    empty_gif;
}   

location /i-log {
    #内部location，不允许外部直接访问
    internal;

    #设置变量，注意需要unescape
    set_unescape_uri $u_domain $arg_domain;
    set_unescape_uri $u_url $arg_url;
    set_unescape_uri $u_title $arg_title;
    set_unescape_uri $u_referrer $arg_referrer;
    set_unescape_uri $u_sh $arg_sh;
    set_unescape_uri $u_sw $arg_sw;
    set_unescape_uri $u_cd $arg_cd;
    set_unescape_uri $u_lang $arg_lang;
    set_unescape_uri $u_utrace $arg_utrace;
    set_unescape_uri $u_account $arg_account;

    #打开日志
    log_subrequest on;
    #记录日志到ma.log，实际应用中最好加buffer，格式为tick
    access_log /path/to/logs/directory/ma.log tick;

    #输出空字符串
    echo '';
}&lt;/pre&gt;
&lt;p&gt;要完全解释这段脚本的每一个细节有点超出本文的范围，而且用到了诸多第三方ngxin模块（全都包含在OpenResty中了），重点的地方我都用注释标出来了，可以不用完全理解每一行的意义，只要大约知道这个配置完成了我们在原理一节提到的后端逻辑就可以了。&lt;/p&gt;
&lt;h2&gt;日志轮转&lt;/h2&gt;
&lt;p&gt;真正的日志收集系统访问日志会非常多，时间一长文件变得很大，而且日志放在一个文件不便于管理。所以通常要按时间段将日志切分，例如每天或每小时切分一个日志。我这里为了效果明显，每一小时切分一个日志。我是通过crontab定时调用一个shell脚本实现的，shell脚本如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;_prefix=&quot;/path/to/nginx&quot;
time=`date +%Y%m%d%H`

mv ${_prefix}/logs/ma.log ${_prefix}/logs/ma/ma-${time}.log
kill -USR1 `cat ${_prefix}/logs/nginx.pid`&lt;/pre&gt;
&lt;p&gt;这个脚本将ma.log移动到指定文件夹并重命名为ma-{yyyymmddhh}.log，然后向nginx发送USR1信号令其重新打开日志文件。&lt;/p&gt;
&lt;p&gt;然后再/etc/crontab里加入一行：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;59  *  *  *  * root /path/to/directory/rotatelog.sh&lt;/pre&gt;
&lt;p&gt;在每个小时的59分启动这个脚本进行日志轮转操作。&lt;/p&gt;
&lt;h2&gt;测试&lt;/h2&gt;
&lt;p&gt;下面可以测试这个系统是否能正常运行了。我昨天就在我的博客中埋了相关的点，通过http抓包可以看到ma.js和1.gif已经被正确请求：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/6.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图6. http包分析ma.js和1.gif的请求&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;同时可以看一下1.gif的请求参数：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/7.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图7. 1.gif的请求参数&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;相关信息确实也放在了请求参数中。&lt;/p&gt;
&lt;p&gt;然后我tail打开日志文件，然后刷新一下页面，因为没有设access log buffer， 我立即得到了一条新日志：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;1351060731.360^A0.0.0.0^Awww.codinglabs.org^Ahttp://www.codinglabs.org/^ACodingLabs^A^A1024^A1280^A24^Azh-CN^AMozilla/5.0 (Macintosh; Intel Mac OS X 10_8_2) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4^A4d612be64366768d32e623d594e82678^AU-1-1&lt;/pre&gt;
&lt;p&gt;注意实际上原日志中的^A是不可见的，这里我用可见的^A替换为方便阅读，另外IP由于涉及隐私我替换为了0.0.0.0。&lt;/p&gt;
&lt;p&gt;看一眼日志轮转目录，由于我之前已经埋了点，所以已经生成了很多轮转文件：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/how-web-analytics-data-collection-system-work/8.png&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;图8. 轮转日志&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;关于分析&lt;/h1&gt;
&lt;p&gt;通过上面的分析和开发可以大致理解一个网站统计的日志收集系统是如何工作的。有了这些日志，就可以进行后续的分析了。本文只注重日志收集，所以不会写太多关于分析的东西。&lt;/p&gt;
&lt;p&gt;注意，原始日志最好尽量多的保留信息而不要做过多过滤和处理。例如上面的MyAnalytics保留了毫秒级时间戳而不是格式化后的时间，时间的格式化是后面的系统做的事而不是日志收集系统的责任。后面的系统根据原始日志可以分析出很多东西，例如通过IP库可以定位访问者的地域、user agent中可以得到访问者的操作系统、浏览器等信息，再结合复杂的分析模型，就可以做流量、来源、访客、地域、路径等分析了。当然，一般不会直接对原始日志分析，而是会将其清洗格式化后转存到其它地方，如MySQL或HBase中再做分析。&lt;/p&gt;
&lt;p&gt;分析部分的工作有很多开源的基础设施可以使用，例如实时分析可以使用&lt;a href=&quot;https://github.com/nathanmarz/storm&quot; target=&quot;_blank&quot;&gt;Storm&lt;/a&gt;，而离线分析可以使用&lt;a href=&quot;http://hadoop.apache.org/&quot; target=&quot;_blank&quot;&gt;Hadoop&lt;/a&gt;。当然，在日志比较小的情况下，也可以通过shell命令做一些简单的分析，例如，下面三条命令可以分别得出我的博客在今天上午8点到9点的访问量（PV），访客数（UV）和独立IP数（IP）：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;awk -F^A '{print $1}' ma-2012102409.log | wc -l
awk -F^A '{print $12}' ma-2012102409.log | uniq | wc -l
awk -F^A '{print $2}' ma-2012102409.log | uniq | wc -l&lt;/pre&gt;
&lt;p&gt;其它好玩的东西朋友们可以慢慢挖掘。&lt;/p&gt;
&lt;h1&gt;参考&lt;/h1&gt;
&lt;p&gt;GA的开发者文档：&lt;a title=&quot;https://developers.google.com/analytics/devguides/collection/gajs/&quot; href=&quot;https://developers.google.com/analytics/devguides/collection/gajs/&quot; target=&quot;_blank&quot;&gt;https://developers.google.com/analytics/devguides/collection/gajs/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一篇关于实现nginx收日志的文章：&lt;a title=&quot;http://blog.linezing.com/2011/11/%E4%BD%BF%E7%94%A8nginx%E8%AE%B0%E6%97%A5%E5%BF%97&quot; href=&quot;http://blog.linezing.com/2011/11/%E4%BD%BF%E7%94%A8nginx%E8%AE%B0%E6%97%A5%E5%BF%97&quot; target=&quot;_blank&quot;&gt;http://blog.linezing.com/2011/11/%E4%BD%BF%E7%94%A8nginx%E8%AE%B0%E6%97%A5%E5%BF%97&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;关于Nginx可以参考：&lt;a title=&quot;http://wiki.nginx.org/Main&quot; href=&quot;http://wiki.nginx.org/Main&quot; target=&quot;_blank&quot;&gt;http://wiki.nginx.org/Main&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OpenResty的官方网站为：&lt;a href=&quot;http://openresty.org&quot; target=&quot;_blank&quot;&gt;http://openresty.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ngx_lua模块可参考：&lt;a title=&quot;https://github.com/chaoslawful/lua-nginx-module&quot; href=&quot;https://github.com/chaoslawful/lua-nginx-module&quot; target=&quot;_blank&quot;&gt;https://github.com/chaoslawful/lua-nginx-module&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文http抓包使用&lt;a href=&quot;http://www.google.com/chrome&quot; target=&quot;_blank&quot;&gt;Chrome&lt;/a&gt;浏览器开发者工具，绘制思维导图使用&lt;a href=&quot;http://www.xmind.net/&quot; target=&quot;_blank&quot;&gt;Xmind&lt;/a&gt;，流程和结构图使用&lt;a href=&quot;http://www.texample.net/tikz/&quot; target=&quot;_blank&quot;&gt;Tikz PGF&lt;/a&gt;&lt;/p&gt;
</description>
</item>
<item>
<title>聊聊如何检测素数</title>
<link>http://blog.codinglabs.org/articles/prime-test.html</link>
<guid>http://blog.codinglabs.org/articles/prime-test.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Tue, 28 Aug 2012 00:00:00 GMT</pubDate>
<description>&lt;p&gt;最近看到一则&lt;a href=&quot;http://edu.people.com.cn/n/2012/0822/c1053-18799177.html&quot; target=&quot;_blank&quot;&gt;颇为有趣的新闻&lt;/a&gt;，说北大一名大一新生，以素数为标准选手机号，受到广大网友膜拜。其实素数的检测算法是很有趣的，并且会涉及到数论、概率算法等诸多内容，一直觉得素数探测算法是了解概率算法很好的入口。本文和大家简单聊聊如何确定一个数是素数。&lt;/p&gt;
&lt;!--more--&gt;
&lt;h1&gt;素数&lt;/h1&gt;
&lt;h2&gt;素数的定义&lt;/h2&gt;
&lt;p&gt;素数是这样被定义的：&lt;/p&gt;
&lt;p&gt;一个大于1的整数，如果不能被除1和它本身外的其它正整数整除，则是素数（又称质数）。&lt;/p&gt;
&lt;p&gt;与素数相关的定义还有合数：&lt;/p&gt;
&lt;p&gt;一个大于1的整数，如果不是素数则是合数。其中能整除这个数的正整数叫做约数，不等于1也不等于合数本身的约数叫做非平凡约数。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;注意1既不是素数又不是合数。&lt;/p&gt;
&lt;p&gt;举几个例子：&lt;/p&gt;
&lt;p&gt;2是素数，因为除1和2外没有其它正整数可以整除2。&lt;/p&gt;
&lt;p&gt;3也是素数。&lt;/p&gt;
&lt;p&gt;4不是素数，因为2可以整除4。&lt;/p&gt;
&lt;p&gt;11是素数，除1和11外没有正整数可以整除它。&lt;/p&gt;
&lt;p&gt;15不是素数，3和5可以整除15。&lt;/p&gt;
&lt;h2&gt;素数的性质&lt;/h2&gt;
&lt;p&gt;素数有一些有趣的性质，下面不加证明的列几条。&lt;/p&gt;
&lt;p&gt;素数有无穷多个。&lt;/p&gt;
&lt;p&gt;设f(n)为定义在大于1的整数集合上的函数，令f(n)的值为不大于n的素数的个数，则：&lt;/p&gt;
&lt;p&gt;\(\lim_{n \to \infty }\frac{f(n)}{n/\ln{n}}=1\)&lt;/p&gt;
&lt;p&gt;这个函数叫做素数分布函数，反映了素数的分布律。换言之，可以认为大于1的前n个正整数中，素数的个数大约是\(n/\ln{n}\)。&lt;/p&gt;
&lt;h1&gt;检测素数&lt;/h1&gt;
&lt;p&gt;所谓素数检测，就是给定任意一个大于1的整数，判断这个数是否素数。&lt;/p&gt;
&lt;h2&gt;因子检测法&lt;/h2&gt;
&lt;p&gt;最直观的素数检测算法就是因子检测法。说白了，就是从2到n-1一个个拿来试，看能否整除n，如果有能整除的（找到一个因子），则输出不是质数，否则则认为n为质数。当然，实际上不需要试探到n-1，只要到\(\sqrt{n}\)就好了，原因如下：&lt;/p&gt;
&lt;p&gt;设\(n=a\times b\)，且a、b均为n的非平凡约数，显然\(a&gt;\sqrt{n}\)和\(b&gt;\sqrt{n}\)不可能同时成立，因为同时成立时a*b就会大于n，所以，如果n存在非平凡约数，则至少有一个小于等于\(\sqrt{n}\)，因此只要遍历到\(\sqrt{n}\)就可以了。&lt;/p&gt;
&lt;p&gt;因子检测法的实现代码如下（python）：&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;def prime_test_factor(n):
    if n == 1:
        return False
    for i in range(2, 1 + int(floor(sqrt(n)))):
        if n % i == 0:
            return False
    return True&lt;/pre&gt;
&lt;p&gt;做几个测试：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;print prime_test_factor(2) #True
print prime_test_factor(11) #True
print prime_test_factor(15) #False
print prime_test_factor(2147483647) #True&lt;/pre&gt;
&lt;p&gt;很明显，因子检测法的时间复杂度为\(O(\sqrt{n})\)，一般来看，这个时间复杂度已经很不错了，不过对于超级大的数（例如&lt;a href=&quot;http://en.wikipedia.org/wiki/RSA_(algorithm)&quot; target=&quot;_blank&quot;&gt;RSA&lt;/a&gt;加密中找几百位的素数是很正常的），这个复杂度还是太大了。&lt;/p&gt;
&lt;p&gt;例如对于下面的整数：&lt;/p&gt;
&lt;p&gt;6864797660130609714981900799081393217269435300143305409394463459185543183397656052122559640661454554 977296311391480858037121987999716643812574028291115057151&lt;/p&gt;
&lt;p&gt;哪位壮士可以试试用因子检测法检测这个数是质数还是合数，估计这辈子结果是出不来了，下辈子也悬。所以需要更高效的素数检测算法。&lt;/p&gt;
&lt;h2&gt;费马检测&lt;/h2&gt;
&lt;p&gt;坦白说，对于大素数的探测，目前并没有非常有效的确定性算法。不过借助费马定理，可以构造一种有效的概率算法来进行素数探测。&lt;/p&gt;
&lt;h3&gt;费马定理&lt;/h3&gt;
&lt;p&gt;首先看一下什么是费马定理。这条定理是&lt;a href=&quot;http://en.wikipedia.org/wiki/Fermat&quot; target=&quot;_blank&quot;&gt;史上最杰出的业余数学家费马&lt;/a&gt;发现的一条数论中的重要定理， 这条定理可以表述为：&lt;/p&gt;
&lt;p&gt;如果p为素数，则对任何小于p的正整数a有&lt;/p&gt;
&lt;p&gt;\(a^{p-1}\equiv 1(mod\ p)\)&lt;/p&gt;
&lt;p&gt;根据基本数理逻辑，一个命题正确，当且仅当其逆否命题正确。所以费马定理蕴含了这样一个事实：如果某个小于p的正整数不符合上述公式，则p一定不是素数；令人惊讶的是，费马定理的逆命题也“几乎正确”，也就是说如果所有小于p的正整数都符合上述公式，则p“几乎就是一个素数”。当然，“几乎正确”就意味着有出错的可能，这个话题我们后续再来讨论。至少从目前来看，费马定理给我们提供了一条检测素数的方法。&lt;/p&gt;
&lt;p&gt;下面再通过例子说明一下费马定理表达的意义，例如我们知道7是一个素数，则：&lt;/p&gt;
&lt;p&gt;\(\begin{align} 
1^6 &amp;= 1     &amp;\equiv 1(mod\ 7) \\ 
2^6 &amp;= 64    &amp;\equiv 1(mod\ 7) \\ 
3^6 &amp;= 729   &amp;\equiv 1(mod\ 7) \\ 
4^6 &amp;= 4096  &amp;\equiv 1(mod\ 7) \\ 
5^6 &amp;= 15625 &amp;\equiv 1(mod\ 7) \\ 
6^6 &amp;= 46656 &amp;\equiv 1(mod\ 7)
\end{align}\)&lt;/p&gt;
&lt;p&gt;其它素数可以可以用类似方法验证，关于这个定理的严格证明本文不再给出。&lt;/p&gt;
&lt;p&gt;所以可以使用如下方法进行大素数探测：选择一个底数（例如2），对于大整数p，如果2^(p-1)与1不是模p同余数，则p&lt;strong&gt;一定&lt;/strong&gt;不是素数；否则，则p&lt;strong&gt;很可能&lt;/strong&gt;是一个素数。&lt;/p&gt;
&lt;p&gt;至于出现假阳性（即合数被判定为素数）的概率，已有研究表明，随着整数趋向于无穷，这个概率趋向于零，在以2为底的情况下，512位整数碰到假阳性的概率为1/10^20，而在1024位整数中，碰到假阳性的概率为1/10^41。因此如果使用此法检测充分大的数，碰到错误的可能性微乎其微。&lt;/p&gt;
&lt;h3&gt;模幂的快速算法&lt;/h3&gt;
&lt;p&gt;仅有费马定理还不能写检测算法，因为对于大整数p来说，a^(p - 1) (mod p)不是一个容易计算的数字，例如上上面那个超大整数来说，直接计算2的那么多次幂真是要死人了，其效果一点不比因子分解法好。所以寻找一种更有效的取模幂算法。通常来说，重复平方法是一个不错的选择。下面通过例子介绍一下这个方法。&lt;/p&gt;
&lt;p&gt;假设现在要求2的10次方，一种方法当然是将10个2连乘，不过还有这样一种计算方法：&lt;/p&gt;
&lt;p&gt;10的二进制表示是1010，因此：&lt;/p&gt;
&lt;p&gt;\(2^{10}=2^{[1010]_2}\)&lt;/p&gt;
&lt;p&gt;现初始化结果d=2^0=1，我们希望通过乘上某些数变换到2^10，变换序列如下：&lt;/p&gt;
&lt;p&gt;\(\begin{align} 
2^{[0]_2}   &amp;                   &amp;         &amp; \\ 
2^{[0]_2}   &amp;\times 2^{[0]_2}   &amp;\times 2 &amp;= 2^{[1]_2} \\ 
2^{[1]_2}   &amp;\times 2^{[1]_2}   &amp;         &amp;= 2^{[10]_2} \\ 
2^{[10]_2}  &amp;\times 2^{[10]_2}  &amp;\times 2 &amp;= 2^{[101]_2} \\ 
2^{[101]_2} &amp;\times 2^{[101]_2} &amp;         &amp;= 2^{[1010]_2}
\end{align}\)&lt;/p&gt;
&lt;p&gt;可以看到这样一个规律：对中间结果d自身进行平方，等于在二进制指数的尾部“生出”一个0；对中间结果d自身进行平方再乘以底数，等于在二进制指数尾部“生出”一个1。靠这样不断让指数“生长”，就可以构造出幂。如果在每次运算时取模，就可以得到模幂了，下面是这个算法的python实现：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;def compute_power(a, p, m):
    result = 1
    p_bin = bin(p)[2:]
    length = len(p_bin)
    for i in range(0, length):
        result = result**2 % m
        if p_bin[i] == '1':
            result = result * a % m

    return result&lt;/pre&gt;
&lt;p&gt;这个算法的复杂度正比于a、p和m中位数最多的数的二进制位数，要远远低于朴素的模幂求解法。&lt;/p&gt;
&lt;p&gt;例如，下面的代码在我的机器上瞬间可以完成：&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;compute_power(2, 6864797660130609714981900799081393217269435300143305409394463459185543183397656052122559640661454554977296311391480858037121987999716643812574028291115057150, 6864797660130609714981900799081393217269435300143305409394463459185543183397656052122559640661454554977296311391480858037121987999716643812574028291115057151)&lt;/pre&gt;
&lt;p&gt;而用直观方法计算如此大指数的幂基本是不可能的。&lt;/p&gt;
&lt;h3&gt;费马检测的实现&lt;/h3&gt;
&lt;p&gt;有了上的铺垫，下面可以实现费马检测了：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;def prime_test_fermat(p):
    if p == 1:
        return False
    if p == 2:
        return True   
    d = compute_power(2, p - 1, p)
    if d == 1:
        return True
    return False&lt;/pre&gt;
&lt;p&gt;以下是一些测试：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;print prime_test_fermat(7) #True
print prime_test_fermat(11) #True
print prime_test_fermat(15) #False
print prime_test_fermat(121) #False
print prime_test_fermat(561) #True
print prime_test_fermat(6864797660130609714981900799081393217269435300143305409394463459185543183397656052122559640661454554977296311391480858037121987999716643812574028291115057151) #True&lt;/pre&gt;
&lt;p&gt;需要注意的是，倒数第二个结果实际是错的，因为561可以分解为3和187。&lt;/p&gt;
&lt;p&gt;相对来说，因子分解法适合比较小的数的探测，可以给出准确的结论，但是对于大整数效率不可接受，例如上面最后一个超大整数，因子分解法基本不可行；费马测试当给出否定结论时，是准确的，但是肯定结论有可能是错误的，对于大整数的效率很高，并且误判率随着整数的增大而降低。&lt;/p&gt;
&lt;h2&gt;Miller-Rabin检测&lt;/h2&gt;
&lt;p&gt;上文说，费马检测失误的概率随着整数不断增大而趋向于0，看似是对大素数检测很好的算法。那么我们考虑另外一个问题：如果一个数p是合数，a是小于p的正整数且a不满足费马定理公式，那么a叫做p是合数的一个证据，问题是，对于任意一个合数p，是否总存在证据？&lt;/p&gt;
&lt;p&gt;答案是否定的。例如561这个数，可以分解为3乘以187，但是如果你试过会发现所有小于561的正整数均符合费马定理公式。这就意味着，费马检测对于561是完全失效的。类似561这样是合数但是可以完全欺骗费马检测的数叫做Carmichael数。Carmichael数虽然密度不大（前10亿个正整数中约600个），但是已经被证明有无穷多个。Carmichael数的存在迫使需要一种更强的检测条件配合单纯费马检测使用，其中Miller-Rabin检测是目前应用比较广泛的一种。&lt;/p&gt;
&lt;p&gt;Miller-Rabin检测依赖以下定理：&lt;/p&gt;
&lt;p&gt;如果p是素数，x是小于p的正整数，且x^2 = 1 mod p，则x要么为1，要么为p-1。&lt;/p&gt;
&lt;p&gt;简单证明：如果x^2 = 1 mod p，则p整除x^2 - 1，即整除(x+1)(x-1)，由于p是素数，所以p要么整除x+1，要么整除x-1，前者则x为p-1，后者则x为1。&lt;/p&gt;
&lt;p&gt;以上定理说明，如果对于任意一个小于p的正整数x，发现1（模p）的非平凡平方根存在，则说明p是合数。&lt;/p&gt;
&lt;p&gt;对于p-1，我们总可以将其表示为u2^t，其中u是奇数，t是正整数。此时：&lt;/p&gt;
&lt;p&gt;\(a^{p-1}=a^{u2^t}=(a^u)^{2^t}\)&lt;/p&gt;
&lt;p&gt;也就是可以通过先算出a^u，然后经过连续t次平方计算出a^(p-1)，并且，在任意一次平方时发现了非平凡平方根，则断定p是合数。&lt;/p&gt;
&lt;p&gt;例如，560 = 35 * 2^4，所以可设u=35，t=4：&lt;/p&gt;
&lt;p&gt;\(\begin{align} 
2^{35} &amp;\mod 561 = 263 \\ 
263^2  &amp;\mod 561 = 166 \\ 
166^2  &amp;\mod 561 = 67 \\ 
67^2   &amp;\mod 561 = 1
\end{align}\)&lt;/p&gt;
&lt;p&gt;由于找到了一个非平凡平方根67，所以可以断言561是合数。因此2就成为了561是合数的一个证据。&lt;/p&gt;
&lt;p&gt;一般的，Miller-Rabin算法的python实现如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;def miller_rabin_witness(a, p):
    if p == 1:
        return False
    if p == 2:
        return True

    n = p - 1
    t = int(floor(log(n, 2)))
    u = 1
    while t &gt; 0:
        u = n / 2**t
        if n % 2**t == 0 and u % 2 == 1:
            break
        t = t - 1

    b1 = b2 = compute_power(a, u, p)
    for i in range(1, t + 1):
        b2 = b1**2 % p
        if b2 == 1 and b1 != 1 and b1 != (p - 1):
            return False
        b1 = b2
    if b1 != 1:
        return False

    return True

def prime_test_miller_rabin(p, k):
    while k &gt; 0:
        a = randint(1, p - 1)
        if not miller_rabin_witness(a, p):
            return False
        k = k - 1
    return True&lt;/pre&gt;
&lt;p&gt;其中miller_rabin_witness用于确认a是否为p为合数的证据，prime_test_miller_rabin共探测k次，每次随机产生一个1至p-1间的整数。只要有一次发现p为合数的证据就认为p为合数，否则认为p为素数。一些测试：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;print prime_test_miller_rabin(7, 5) #True
print prime_test_miller_rabin(21, 5) #False
print prime_test_miller_rabin(561, 50) #False
print prime_test_miller_rabin(6864797660130609714981900799081393217269435300143305409394463459185543183397656052122559640661454554977296311391480858037121987999716643812574028291115057151, 50) #True&lt;/pre&gt;
&lt;p&gt;Miller-Rabin检测也同样存在假阳性的问题，但是与费马检测不同，MR检测的正确概率不依赖被检测数p（排除了Carmichael数失效问题），而仅依赖于检测次数。已经证明，如果一个数p为合数，那么Miller-Rabin检测的证据数量不少于比其小的正整数的3/4，换言之，k次检测后得到错误结果的概率为(1/4)^k，例如上面最后一个大整数，Miller-Rabin检测认为其实素数，我设k为50，也就是说它被误认为素数的概率为(1/4)^50。这个概率有多小呢，小到你不可想象。直观来说，大约等于一个人连续中得5次双色球头奖的概率。&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&quot;http://www.amazon.com/Introduction-To-Algorithms-Thomas-Cormen/dp/0070131430/&quot; target=&quot;_blank&quot;&gt;算法导论&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a title=&quot;http://www.wikipedia.org/&quot; href=&quot;http://www.wikipedia.org/&quot; target=&quot;_blank&quot;&gt;http://www.wikipedia.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a title=&quot;http://www.matrix67.com/blog/archives/234&quot; href=&quot;http://www.matrix67.com/blog/archives/234&quot; target=&quot;_blank&quot;&gt;http://www.matrix67.com/blog/archives/234&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a title=&quot;http://www.bigprimes.net/&quot; href=&quot;http://www.bigprimes.net/&quot; target=&quot;_blank&quot;&gt;http://www.bigprimes.net/&lt;/a&gt;&lt;/p&gt;
</description>
</item>
<item>
<title>浅析PageRank算法</title>
<link>http://blog.codinglabs.org/articles/intro-to-pagerank.html</link>
<guid>http://blog.codinglabs.org/articles/intro-to-pagerank.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Mon, 02 Jul 2012 00:00:00 GMT</pubDate>
<description>&lt;p&gt;很早就对Google的PageRank算法很感兴趣，但一直没有深究，只有个轮廓性的概念。前几天趁团队outing的机会，在动车上看了一些相关的资料（PS：在动车上看看书真是一种享受），趁热打铁，将所看的东西整理成此文。&lt;/p&gt;
&lt;p&gt;本文首先会讨论搜索引擎的核心难题，同时讨论早期搜索引擎关于结果页面重要性评价算法的困境，借此引出PageRank产生的背景。第二部分会详细讨论PageRank的思想来源、基础框架，并结合互联网页面拓扑结构讨论PageRank处理Dead Ends及平滑化的方法。第三部分讨论Topic-Sensitive PageRank算法。最后将讨论对PageRank的Spam攻击方法：Spam Farm以及搜索引擎对Spam Farm的防御。
&lt;!--more--&gt;
&lt;/p&gt;
&lt;h1&gt;搜索引擎的难题&lt;/h1&gt;
&lt;p&gt;Google早已成为全球最成功的互联网搜索引擎，但这个当前的搜索引擎巨无霸却不是最早的互联网搜索引擎，在Google出现之前，曾出现过许多通用或专业领域搜索引擎。Google最终能击败所有竞争对手，很大程度上是因为它解决了困扰前辈们的最大难题：对搜索结果按重要性排序。而解决这个问题的算法就是PageRank。毫不夸张的说，是PageRank算法成就了Google今天的低位。要理解为什么解决这个难题如此重要，我们先来看一下搜索引擎的核心框架。&lt;/p&gt;
&lt;h2&gt;搜索引擎的核心框架&lt;/h2&gt;
&lt;p&gt;虽然搜索引擎已经发展了很多年，但是其核心却没有太大变化。从本质上说，搜索引擎是一个资料检索系统，搜索引擎拥有一个资料库（具体到这里就是互联网页面），用户提交一个检索条件（例如关键词），搜索引擎返回符合查询条件的资料列表。理论上检索条件可以非常复杂，为了简单起见，我们不妨设检索条件是一至多个以空格分隔的词，而其表达的语义是同时含有这些词的资料（等价于布尔代数的逻辑与）。例如，提交“张洋 博客”，意思就是“给我既含有‘张洋’又含有‘博客’词语的页面”，以下是Google对这条关键词的搜索结果：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-to-pagerank/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;可以看到我的博客出现在第五条，而第四条是我之前在博客园的博客。&lt;/p&gt;
&lt;p&gt;当然，实际上现在的搜索引擎都是有分词机制的，例如如果以“张洋的博客”为关键词，搜索引擎会自动将其分解为“张洋 的 博客”三个词，而“的”作为&lt;a href=&quot;http://en.wikipedia.org/wiki/Stop_word&quot; target=&quot;_blank&quot;&gt;停止词&lt;/a&gt;（Stop Word）会被过滤掉。关于分词及词权评价算法（如&lt;a href=&quot;http://en.wikipedia.org/wiki/Tf*idf&quot; target=&quot;_blank&quot;&gt;TF-IDF算法&lt;/a&gt;）是一个很大的话题，这里就不展开讨论了，为了简单此处可以将搜索引擎想象为一个只会机械匹配词语的检索系统。&lt;/p&gt;
&lt;p&gt;这样看来，建立一个搜索引擎的核心问题就是两个：1、建立资料库；2、建立一种数据结构，可以根据关键词找到含有这个词的页面。&lt;/p&gt;
&lt;p&gt;第一个问题一般是通过一种叫&lt;a href=&quot;http://en.wikipedia.org/wiki/Web_crawler&quot; target=&quot;_blank&quot;&gt;爬虫&lt;/a&gt;（Spider）的特殊程序实现的（当然，专业领域搜索引擎例如某个学术会议的论文检索系统可能直接从数据库建立资料库），简单来说，爬虫就是从一个页面出发（例如新浪首页），通过HTTP协议通信获取这个页面的所有内容，把这个页面url和内容记录下来（记录到资料库），然后分析页面中的链接，再去分别获取这些链接链向页面的内容，记录到资料库后再分析这个页面的链接……重复这个过程，就可以将整个互联网的页面全部获取下来（当然这是理想情况，要求整个Web是一个强连通（&lt;a href=&quot;http://en.wikipedia.org/wiki/Strongly_connected&quot; target=&quot;_blank&quot;&gt;Strongly Connected&lt;/a&gt;），并且所有页面的&lt;a href=&quot;http://www.robotstxt.org/&quot; target=&quot;_blank&quot;&gt;robots协议&lt;/a&gt;允许爬虫抓取页面，为了简单，我们仍然假设Web是一个强连通图，且不考虑robots协议）。抽象来看，可以将资料库看做一个巨大的key-value结构，key是页面url，value是页面内容。&lt;/p&gt;
&lt;p&gt;第二个问题是通过一种叫倒排索引（&lt;a href=&quot;http://en.wikipedia.org/wiki/Inverted_index&quot; target=&quot;_blank&quot;&gt;inverted index&lt;/a&gt;）的数据结构实现的，抽象来说倒排索引也是一组key-value结构，key是关键词，value是一个页面编号集合（假设资料库中每个页面有唯一编号），表示这些页面含有这个关键词。本文不详细讨论倒排索引的建立方法。&lt;/p&gt;
&lt;p&gt;有了上面的分析，就可以简要说明搜索引擎的核心动作了：搜索引擎获取“张洋 博客”查询条件，将其分为“张洋”和“博客”两个词。然后分别从倒排索引中找到“张洋”所对应的集合，假设是{1， 3， 6， 8， 11， 15}；“博客”对应的集合是{1， 6， 10， 11， 12， 17， 20， 22}，将两个集合做交运算（intersection），结果是{1， 6， 11}。最后，从资料库中找出1、6、11对应的页面返回给用户就可以了。&lt;/p&gt;
&lt;h2&gt;搜索引擎的核心难题&lt;/h2&gt;
&lt;p&gt;上面阐述了一个非常简单的搜索引擎工作框架，虽然现代搜索引擎的具体细节原理要复杂的多，但其本质却与这个简单的模型并无二异。实际Google在上述两点上相比其前辈并无高明之处。其最大的成功是解决了第三个、也是最为困难的问题：如何对查询结果排序。&lt;/p&gt;
&lt;p&gt;我们知道Web页面数量非常巨大，所以一个检索的结果条目数量也非常多，例如上面“张洋 博客”的检索返回了超过260万条结果。用户不可能从如此众多的结果中一一查找对自己有用的信息，所以，一个好的搜索引擎必须想办法将“质量”较高的页面排在前面。其实直观上也可以感觉出，在使用搜索引擎时，我们并不太关心页面是否够全（上百万的结果，全不全有什么区别？而且实际上搜索引擎都是取top，并不会真的返回全部结果。），而很关心前一两页是否都是质量较高的页面，是否能满足我们的实际需求。&lt;/p&gt;
&lt;p&gt;因此，对搜索结果按重要性合理的排序就成为搜索引擎的最大核心，也是Google最终成功的突破点。&lt;/p&gt;
&lt;h2&gt;早期搜索引擎的做法&lt;/h2&gt;
&lt;h3&gt;不评价&lt;/h3&gt;
&lt;p&gt;这个看起来可能有点搞笑，但实际上早期很多搜索引擎（甚至包括现在的很多专业领域搜索引擎）根本不评价结果重要性，而是直接按照某自然顺序（例如时间顺序或编号顺序）返回结果。这在结果集比较少的情况下还说得过去，但是一旦结果集变大，用户叫苦不迭，试想让你从几万条质量参差不齐的页面中寻找需要的内容，简直就是一场灾难，这也注定这种方法不可能用于现代的通用搜索引擎。&lt;/p&gt;
&lt;h3&gt;基于检索词的评价&lt;/h3&gt;
&lt;p&gt;后来，一些搜索引擎引入了基于检索关键词去评价搜索结构重要性的方法，实际上，这类方法如TF-IDF算法在现代搜索引擎中仍在使用，但其已经不是评价质量的唯一指标。完整描述TF-IDF比较繁琐，本文这里用一种更简单的抽象模型描述这种方法。&lt;/p&gt;
&lt;p&gt;基于检索词评价的思想非常朴素：和检索词匹配度越高的页面重要性越高。“匹配度”就是要定义的具体度量。一个最直接的想法是关键词出现次数越多的页面匹配度越高。还是搜索“张洋 博客”的例子：假设A页面出现“张洋”5次，“博客”10次；B页面出现“张洋”2次，“博客”8次。于是A页面的匹配度为5 + 10 = 15，B页面为2 + 8 = 10，于是认为A页面的重要性高于B页面。很多朋友可能意识到这里的不合理性：内容较长的网页往往更可能比内容较短的网页关键词出现的次数多。因此，我们可以修改一下算法，用关键词出现次数除以页面总词数，也就是通过关键词占比作为匹配度，这样可以克服上面提到的不合理。&lt;/p&gt;
&lt;p&gt;早期一些搜索引擎确实是基于类似的算法评价网页重要性的。这种评价算法看似依据充分、实现直观简单，但却非常容易受到一种叫“Term Spam”的攻击。&lt;/p&gt;
&lt;h3&gt;Term Spam&lt;/h3&gt;
&lt;p&gt;其实从搜索引擎出现的那天起，spammer和搜索引擎反作弊的斗法就没有停止过。Spammer是这样一群人——试图通过搜索引擎算法的漏洞来提高目标页面（通常是一些广告页面或垃圾页面）的重要性，使目标页面在搜索结果中排名靠前。&lt;/p&gt;
&lt;p&gt;现在假设Google单纯使用关键词占比评价页面重要性，而我想让我的博客在搜索结果中排名更靠前（最好排第一）。那么我可以这么做：在页面中加入一个隐藏的html元素（例如一个div），然后其内容是“张洋”重复一万次。这样，搜索引擎在计算“张洋 博客”的搜索结果时，我的博客关键词占比就会非常大，从而做到排名靠前的效果。更进一步，我甚至可以干扰别的关键词搜索结果，例如我知道现在欧洲杯很火热，我就在我博客的隐藏div里加一万个“欧洲杯”，当有用户搜索欧洲杯时，我的博客就能出现在搜索结果较靠前的位置。这种行为就叫做“Term Spam”。&lt;/p&gt;
&lt;p&gt;早期搜索引擎深受这种作弊方法的困扰，加之基于关键词的评价算法本身也不甚合理，因此经常是搜出一堆质量低下的结果，用户体验大大打了折扣。而Google正是在这种背景下，提出了PageRank算法，并申请了专利保护。此举充分保护了当时相对弱小Google，也使得Google一举成为全球首屈一指的搜索引擎。&lt;/p&gt;
&lt;h1&gt;PageRank算法&lt;/h1&gt;
&lt;p&gt;上文已经说到，PageRank的作用是评价网页的重要性，以此作为搜索结果的排序重要依据之一。实际中，为了抵御spam，各个搜索引擎的具体排名算法是保密的，PageRank的具体计算方法也不尽相同，本节介绍一种最简单的基于页面链接属性的PageRank算法。这个算法虽然简单，却能揭示PageRank的本质，实际上目前各大搜索引擎在计算PageRank时链接属性确实是重要度量指标之一。&lt;/p&gt;
&lt;h2&gt;简单PageRank计算&lt;/h2&gt;
&lt;p&gt;首先，我们将Web做如下抽象：1、将每个网页抽象成一个节点；2、如果一个页面A有链接直接链向B，则存在一条有向边从A到B（多个相同链接不重复计算边）。因此，整个Web被抽象为一张有向图。&lt;/p&gt;
&lt;p&gt;现在假设世界上只有四张网页：A、B、C、D，其抽象结构如下图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-to-pagerank/2.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;显然这个图是强连通的（从任一节点出发都可以到达另外任何一个节点）。&lt;/p&gt;
&lt;p&gt;然后需要用一种合适的数据结构表示页面间的连接关系。其实，PageRank算法是基于这样一种背景思想：被用户访问越多的网页更可能质量越高，而用户在浏览网页时主要通过超链接进行页面跳转，因此我们需要通过分析超链接组成的拓扑结构来推算每个网页被访问频率的高低。最简单的，我们可以假设当一个用户停留在某页面时，跳转到页面上每个被链页面的概率是相同的。例如，上图中A页面链向B、C、D，所以一个用户从A跳转到B、C、D的概率各为1/3。设一共有N个网页，则可以组织这样一个N维矩阵：其中i行j列的值表示用户从页面j转到页面i的概率。这样一个矩阵叫做转移矩阵（Transition Matrix）。下面的转移矩阵M对应上图：&lt;/p&gt;
&lt;p&gt;\(\large M=\begin{bmatrix} 0 &amp; 1/2 &amp; 0 &amp; 1/2\\ 1/3 &amp; 0 &amp; 0 &amp; 1/2\\ 1/3 &amp; 1/2 &amp; 0 &amp; 0\\ 1/3 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;然后，设初始时每个页面的rank值为1/N，这里就是1/4。按A-D顺序将页面rank为向量v：&lt;/p&gt;
&lt;p&gt;\(\large v=\begin{bmatrix} 1/4\\ 1/4\\ 1/4\\ 1/4 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;注意，M第一行分别是A、B、C和D转移到页面A的概率，而v的第一列分别是A、B、C和D当前的rank，因此用M的第一行乘以v的第一列，所得结果就是页面A最新rank的合理估计，同理，Mv的结果就分别代表A、B、C、D新rank：&lt;/p&gt;
&lt;p&gt;\(\large Mv=\begin{bmatrix} 1/4\\ 5/24\\ 5/24\\ 1/3 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;然后用M再乘以这个新的rank向量，又会产生一个更新的rank向量。迭代这个过程，可以证明v最终会收敛，即v约等于Mv，此时计算停止。最终的v就是各个页面的pagerank值。例如上面的向量经过几步迭代后，大约收敛在（1/4, 1/4, 1/5, 1/4），这就是A、B、C、D最后的pagerank。&lt;/p&gt;
&lt;h2&gt;处理Dead Ends&lt;/h2&gt;
&lt;p&gt;上面的PageRank计算方法假设Web是强连通的，但实际上，Web并不是强连通（甚至不是联通的）。下面看看PageRank算法如何处理一种叫做Dead Ends的情况。&lt;/p&gt;
&lt;p&gt;所谓Dead Ends，就是这样一类节点：它们不存在外链。看下面的图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-to-pagerank/3.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;注意这里D页面不存在外链，是一个Dead End。上面的算法之所以能成功收敛到非零值，很大程度依赖转移矩阵这样一个性质：每列的加和为1。而在这个图中，M第四列将全为0。在没有Dead Ends的情况下，每次迭代后向量v各项的和始终保持为1，而有了Dead Ends，迭代结果将最终归零（要解释为什么会这样，需要一些矩阵论的知识，比较枯燥，此处略）。&lt;/p&gt;
&lt;p&gt;处理Dead Ends的方法如下：迭代拿掉图中的Dead Ends节点及Dead Ends节点相关的边（之所以迭代拿掉是因为当目前的Dead Ends被拿掉后，可能会出现一批新的Dead Ends），直到图中没有Dead Ends。对剩下部分计算rank，然后以拿掉Dead Ends逆向顺序反推Dead Ends的rank。&lt;/p&gt;
&lt;p&gt;以上图为例，首先拿到D和D相关的边，D被拿到后，C就变成了一个新的Dead Ends，于是拿掉C，最终只剩A、B。此时可很容易算出A、B的PageRank均为1/2。然后我们需要反推Dead Ends的rank，最后被拿掉的是C，可以看到C前置节点有A和B，而A和B的出度分别为3和2，因此C的rank为：1/2 * 1/3 + 1/2 * 1/2 = 5/12；最后，D的rank为：1/2 * 1/3 + 5/12 * 1 = 7/12。所以最终的PageRank为（1/2, 1/2, 5/12, 7/12）。&lt;/p&gt;
&lt;h2&gt;Spider Traps及平滑处理&lt;/h2&gt;
&lt;p&gt;可以预见，如果把真实的Web组织成转移矩阵，那么这将是一个极为稀疏的矩阵，从矩阵论知识可以推断，极度稀疏的转移矩阵迭代相乘可能会使得向量v变得非常不平滑，即一些节点拥有很大的rank，而大多数节点rank值接近0。而一种叫做Spider Traps节点的存在加剧了这种不平滑。例如下图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-to-pagerank/4.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;D有外链所以不是Dead Ends，但是它只链向自己（注意链向自己也算外链，当然同时也是个内链）。这种节点叫做Spider Trap，如果对这个图进行计算，会发现D的rank越来越大趋近于1，而其它节点rank值几乎归零。&lt;/p&gt;
&lt;p&gt;为了克服这种由于矩阵稀疏性和Spider Traps带来的问题，需要对PageRank计算方法进行一个平滑处理，具体做法是加入“心灵转移（teleporting）”。所谓心灵转移，就是我们认为在任何一个页面浏览的用户都有可能以一个极小的概率瞬间转移到另外一个随机页面。当然，这两个页面可能不存在超链接，因此不可能真的直接转移过去，心灵转移只是为了算法需要而强加的一种纯数学意义的概率数字。&lt;/p&gt;
&lt;p&gt;加入心灵转移后，向量迭代公式变为：&lt;/p&gt;
&lt;p&gt;\(\large {v}'=(1-\beta)Mv+e\frac{\beta}{N}\)&lt;/p&gt;
&lt;p&gt;其中β往往被设置为一个比较小的参数（0.2或更小），e为N维单位向量，加入e的原因是这个公式的前半部分是向量，因此必须将β/N转为向量才能相加。这样，整个计算就变得平滑，因为每次迭代的结果除了依赖转移矩阵外，还依赖一个小概率的心灵转移。&lt;/p&gt;
&lt;p&gt;以上图为例，转移矩阵M为：&lt;/p&gt;
&lt;p&gt;\(\large M=\begin{bmatrix} 0 &amp; 1/2 &amp; 0 &amp; 0\\ 1/3 &amp; 0 &amp; 0 &amp; 0\\ 1/3 &amp; 1/2 &amp; 0 &amp; 0\\ 1/3 &amp; 0 &amp; 1 &amp; 1 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;设β为0.2，则加权后的M为：&lt;/p&gt;
&lt;p&gt;\(\large M=\begin{bmatrix} 0 &amp; 2/5 &amp; 0 &amp; 0\\ 4/15 &amp; 0 &amp; 0 &amp; 0\\ 4/15 &amp; 2/5 &amp; 0 &amp; 0\\ 4/15 &amp; 0 &amp; 4/5 &amp; 4/5 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;因此：&lt;/p&gt;
&lt;p&gt;\(\large {v}'=\begin{bmatrix} 0 &amp; 2/5 &amp; 0 &amp; 0\\ 4/15 &amp; 0 &amp; 0 &amp; 0\\ 4/15 &amp; 2/5 &amp; 0 &amp; 0\\ 4/15 &amp; 0 &amp; 4/5 &amp; 4/5 \end{bmatrix}v+\begin{bmatrix} 1/20\\ 1/20\\ 1/20\\ 1/20 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;如果按这个公式迭代算下去，会发现Spider Traps的效应被抑制了，从而每个页面都拥有一个合理的pagerank。&lt;/p&gt;
&lt;h2&gt;Topic-Sensitive PageRank&lt;/h2&gt;
&lt;p&gt;其实上面的讨论我们回避了一个事实，那就是“网页重要性”其实没一个标准答案，对于不同的用户，甚至有很大的差别。例如，当搜索“苹果”时，一个数码爱好者可能是想要看iphone的信息，一个果农可能是想看苹果的价格走势和种植技巧，而一个小朋友可能在找苹果的简笔画。理想情况下，应该为每个用户维护一套专用向量，但面对海量用户这种方法显然不可行。所以搜索引擎一般会选择一种称为Topic-Sensitive的折中方案。Topic-Sensitive PageRank的做法是预定义几个话题类别，例如体育、娱乐、科技等等，为每个话题单独维护一个向量，然后想办法关联用户的话题倾向，根据用户的话题倾向排序结果。&lt;/p&gt;
&lt;p&gt;Topic-Sensitive PageRank分为以下几步：&lt;/p&gt;
&lt;p&gt;1、确定话题分类。&lt;/p&gt;
&lt;p&gt;一般来说，可以参考&lt;a href=&quot;http://www.dmoz.org&quot; target=&quot;_blank&quot;&gt;Open Directory（DMOZ）&lt;/a&gt;的一级话题类别作为topic。目前DMOZ的一级topic有：Arts（艺术）、Business（商务）、Computers（计算机）、Games（游戏）、Health（医疗健康）、Home（居家）、Kids and Teens（儿童）、News（新闻）、Recreation（娱乐修养）、Reference（参考）、Regional（地域）、Science（科技）、Shopping（购物）、Society（人文社会）、Sports（体育）。&lt;/p&gt;
&lt;p&gt;2、网页topic归属。&lt;/p&gt;
&lt;p&gt;这一步需要将每个页面归入最合适的分类，具体归类有很多算法，例如可以使用TF-IDF基于词素归类，也可以聚类后人工归类，具体不再展开。这一步最终的结果是每个网页被归到其中一个topic。&lt;/p&gt;
&lt;p&gt;3、分topic向量计算。&lt;/p&gt;
&lt;p&gt;在Topic-Sensitive PageRank中，向量迭代公式为&lt;/p&gt;
&lt;p&gt;\(\large {v}'=(1-\beta)Mv+s\frac{\beta}{|s|}\)&lt;/p&gt;
&lt;p&gt;首先是单位向量e变为了s。s是这样一个向量：对于某topic的s，如果网页k在此topic中，则s中第k个元素为1，否则为0。注意对于每一个topic都有一个不同的s。而|s|表示s中1的数量。&lt;/p&gt;
&lt;p&gt;还是以上面的四张页面为例，假设页面A归为Arts，B归为Computers，C归为Computers，D归为Sports。那么对于Computers这个topic，s就是：&lt;/p&gt;
&lt;p&gt;\(\large s=\begin{bmatrix} 0\\ 1\\ 1\\ 0 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;而|s|=2。因此，迭代公式为：&lt;/p&gt;
&lt;p&gt;\(\large {v}'=\begin{bmatrix} 0 &amp; 2/5 &amp; 0 &amp; 0\\ 4/15 &amp; 0 &amp; 0 &amp; 0\\ 4/15 &amp; 2/5 &amp; 0 &amp; 0\\ 4/15 &amp; 0 &amp; 4/5 &amp; 4/5 \end{bmatrix}v+\begin{bmatrix} 0\\ 1/10\\ 1/10\\ 0 \end{bmatrix}\)&lt;/p&gt;
&lt;p&gt;最后算出的向量就是Computers这个topic的rank。如果实际计算一下，会发现B、C页在这个topic下的权重相比上面非Topic-Sensitive的rank会升高，这说明如果用户是一个倾向于Computers topic的人（例如程序员），那么在给他呈现的结果中B、C会更重要，因此可能排名更靠前。&lt;/p&gt;
&lt;p&gt;4、确定用户topic倾向。&lt;/p&gt;
&lt;p&gt;最后一步就是在用户提交搜索时，确定用户的topic倾向，以选择合适的rank向量。主要方法有两种，一种是列出所有topic让用户自己选择感兴趣的项目，这种方法在一些社交问答网站注册时经常使用；另外一种方法就是通过某种手段（如cookie跟踪）跟踪用户的行为，进行数据分析判断用户的倾向，这本身也是一个很有意思的话题，按时这个话题超出本文的范畴，不再展开细说。&lt;/p&gt;
&lt;h1&gt;针对PageRank的Spam攻击与反作弊&lt;/h1&gt;
&lt;p&gt;上文说过，Spammer和搜索引擎反作弊工程师的斗法从来就没停止过。实际上，只要是算法，就一定有spam方法，不存在无懈可击的排名算法。下面看一下针对PageRank的spam。&lt;/p&gt;
&lt;h2&gt;Link Spam&lt;/h2&gt;
&lt;p&gt;回到文章开头的例子，如果我想让我的博客在搜索“张洋 博客”时排名靠前，显然在PageRank算法下靠Term Spam是无法实现的。不过既然我明白了PageRank主要靠内链数计算页面权重，那么我是不是可以考虑建立很多空架子网站，让这些网站都链接到我博客首页，这样是不是可以提高我博客首页的PageRank？很不幸，这种方法行不通。再看下PageRank算法，一个页面会将权重均匀散播给被链接网站，所以除了内链数外，上游页面的权重也很重要。而我那些空架子网站本身就没啥权重，所以来自它们的内链并不能起到提高我博客首页PageRank的作用，这样只是自娱自乐而已。&lt;/p&gt;
&lt;p&gt;所以，Spam PageRank的关键就在于想办法增加一些高权重页面的内链。下面具体看一下Link Spam怎么做。&lt;/p&gt;
&lt;p&gt;首先明确将页面分为几个类型：&lt;/p&gt;
&lt;p&gt;1、目标页&lt;/p&gt;
&lt;p&gt;目标页是spammer要提高rank的页面，这里就是我的博客首页。&lt;/p&gt;
&lt;p&gt;2、支持页&lt;/p&gt;
&lt;p&gt;支持页是spammer能完全控制的页面，例如spammer自己建立的站点中页面，这里就是我上文所谓的空架子页面。&lt;/p&gt;
&lt;p&gt;3、可达页&lt;/p&gt;
&lt;p&gt;可达页是spammer无法完全控制，但是可以有接口供spammer发布链接的页面，例如天涯社区、新浪博客等等这种用户可发帖的社区或博客站。&lt;/p&gt;
&lt;p&gt;4、不可达页&lt;/p&gt;
&lt;p&gt;这是那些spammer完全无法发布链接的网站，例如政府网站、百度首页等等。&lt;/p&gt;
&lt;p&gt;作为一个spammer，我能利用的资源就是支持页和可达页。上面说过，单纯通过支持页是没有办法spam的，因此我要做的第一件事情就是尽量找一些rank较高的可达页去加上对我博客首页的链接。例如我可以去天涯、猫扑等地方回个这样的贴：“楼主的帖子很不错！精彩内容：&lt;a href=&quot;http://codinglabs.org&quot; target=&quot;_blank&quot;&gt;http://codinglabs.org&lt;/a&gt;”。我想大家一定在各大社区没少见这种帖子，这就是有人在做spam。&lt;/p&gt;
&lt;p&gt;然后，再通过大量的支持页放大rank，具体做法是让每个支持页和目标页互链，且每个支持页只有一条链接。&lt;/p&gt;
&lt;p&gt;这样一个结构叫做Spam Farm，其拓扑图如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/intro-to-pagerank/5.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;其中T是目标页，A是可达页，S是支持页。下面计算一下link spam的效果。&lt;/p&gt;
&lt;p&gt;设T的总rank为y，则y由三部分组成：&lt;/p&gt;
&lt;p&gt;1、可达页的rank贡献，设为x。&lt;/p&gt;
&lt;p&gt;2、心灵转移的贡献，为β/n。其中n为全部网页的数量，β为转移参数。&lt;/p&gt;
&lt;p&gt;3、支持页的贡献：&lt;/p&gt;
&lt;p&gt;设有m个支持页，因为每个支持页只和T有链接，所以可以算出每个支持页的rank为：&lt;/p&gt;
&lt;p&gt;\(\large \frac{(1-\beta)y}{m}+\frac{\beta}{n}\)&lt;/p&gt;
&lt;p&gt;则支持页贡献的全部rank为：&lt;/p&gt;
&lt;p&gt;\(\large m(1-\beta)(\frac{(1-\beta)y}{m}+\frac{\beta}{n})\)&lt;/p&gt;
&lt;p&gt;因此可以得到：&lt;/p&gt;
&lt;p&gt;\(\large y=m(1-\beta)(\frac{(1-\beta)y}{m}+\frac{\beta}{n})+x+\frac{\beta}{n}\)&lt;/p&gt;
&lt;p&gt;由于相对β，n非常巨大，所以可以认为β/n近似于0。 简化后的方程为：&lt;/p&gt;
&lt;p&gt;\(\large y=m(1-\beta)(\frac{(1-\beta)y}{m})+x\)&lt;/p&gt;
&lt;p&gt;解方程得：&lt;/p&gt;
&lt;p&gt;\(\large y=x\frac{1}{2\beta-\beta^2}\)&lt;/p&gt;
&lt;p&gt;假设β为0.2，则1/(2β-β^2) = 2.77则这个spam farm可以将x约放大2.7倍。因此如果起到不错的spam效果。&lt;/p&gt;
&lt;h2&gt;Link Spam反作弊&lt;/h2&gt;
&lt;p&gt;针对spammer的link spam行为，搜索引擎的反作弊工程师需要想办法检测这种行为，一般来说有两类方法检测link spam。&lt;/p&gt;
&lt;h3&gt;网络拓扑分析&lt;/h3&gt;
&lt;p&gt;一种方法是通过对网页的图拓扑结构分析找出可能存在的spam farm。但是随着Web规模越来越大，这种方法非常困难，因为图的特定结构查找是时间复杂度非常高的一个算法，不可能完全靠这种方法反作弊。&lt;/p&gt;
&lt;h3&gt;TrustRank&lt;/h3&gt;
&lt;p&gt;更可能的一种反作弊方法是叫做一种TrustRank的方法。&lt;/p&gt;
&lt;p&gt;说起来TrustRank其实数学本质上就是Topic-Sensitive Rank，只不过这里定义了一个“可信网页”的虚拟topic。所谓可信网页就是上文说到的不可达页，或者说没法spam的页面。例如政府网站（被黑了的不算）、新浪、网易门户首页等等。一般是通过人力或者其它什么方式选择出一个“可信网页”集合，组成一个topic，然后通过上文的Topic-Sensitive算法对这个topic进行rank计算，结果叫做TrustRank。&lt;/p&gt;
&lt;p&gt;TrustRank的思想很直观：如果一个页面的普通rank远高于可信网页的topic rank，则很可能这个页面被spam了。&lt;/p&gt;
&lt;p&gt;设一个页面普通rank为P，TrustRank为T，则定义网页的Spam Mass为：(P – T)/P。&lt;/p&gt;
&lt;p&gt;Spam Mass越大，说明此页面为spam目标页的可能性越大。&lt;/p&gt;
&lt;h1&gt;总结&lt;/h1&gt;
&lt;p&gt;这篇文章是我对一些资料的归纳汇总，简单介绍了PageRank的背景、作用、计算方法、变种、Spam及反作弊等内容。为了突出重点我简化了搜索引擎的模型，当然在实际中搜索引擎远没有这么简单，真实算法也一定非常复杂。不过目前几乎所有现代搜索引擎页面权重的计算方法都基于PageRank及其变种。因为我没做过搜索引擎相关的开发，因此本文内容主要是基于现有文献的客观总结，稍加一点我的理解。&lt;/p&gt;
&lt;p&gt;文中的图使用PGF/TikZ for Tex绘制：&lt;a title=&quot;http://www.texample.net/tikz/&quot; href=&quot;http://www.texample.net/tikz/&quot; target=&quot;_blank&quot;&gt;http://www.texample.net/tikz/&lt;/a&gt;。&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;p&gt;[1] Anand Rajaraman, Jeffrey D. Ullman, Mining of Massive Datasets. 2010-2011&lt;/p&gt;
&lt;p&gt;[2] S. Brin and L. Page, “Anatomy of a large-scale hypertextual web search engine,” Proc. 7th Intl. World-Wide-Web Conference, pp. 107–117, 1998.&lt;/p&gt;
&lt;p&gt;[3] A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan, R. Stata, A. Tomkins, and J. Weiner, “Graph structure in the web,” Computer Networks 33:1–6, pp. 309–320, 2000.&lt;/p&gt;
&lt;p&gt;[4] T.H. Haveliwala, “Topic-sensitive PageRank,” Proc. 11th Intl. World-Wide-Web Conference, pp. 517–526, 2002&lt;/p&gt;
&lt;p&gt;[5] Z. Gy¨ongi, H. Garcia-Molina, and J. Pedersen, “Combating link spam with trustrank,” Proc. 30th Intl. Conf. on Very Large Databases, pp. 576–587, 2004.&lt;/p&gt;
</description>
</item>
<item>
<title>发布一个查看PHP opcode的扩展模块及Web服务</title>
<link>http://blog.codinglabs.org/articles/opdumper-and-web-opcode-dumper.html</link>
<guid>http://blog.codinglabs.org/articles/opdumper-and-web-opcode-dumper.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Wed, 16 May 2012 00:00:00 GMT</pubDate>
<description>&lt;p&gt;最近花了大约一星期的时间写了一个PHP扩展模块&lt;a href=&quot;https://github.com/ericzhang-cn/opdumper&quot; target=&quot;_blank&quot;&gt;Opdumer&lt;/a&gt;，并封装成了Web服务（&lt;a href=&quot;http://supercompiler.com/app/opcode_dumper&quot; target=&quot;_blank&quot;&gt;点击这里访问&lt;/a&gt;）。这个模块的主要内容是输出PHP代码对应的opcode。其实之前已经有一些用于查看opcode的扩展模块，如比较有名的&lt;a href=&quot;http://pecl.php.net/package/vld&quot; target=&quot;_blank&quot;&gt;vld&lt;/a&gt;。之所以重新实现一个这样的模块，主要是因为vld不支持PHP_FUNCTION API，也就是说vld只能用于CLI形式，而Opdumer同时拥有CLI API和PHP_FUNCTION API，另外，也想借助编写这个模块的机会学习Zend Engine中opcode的编译和执行机制。个人打算后面专门针对opcode的编译执行机制写一篇文章，而本文主要描述Opcode的使用方法及对应Web服务的使用。&lt;/p&gt;
&lt;!--more--&gt;
&lt;h1&gt;Opdumper&lt;/h1&gt;
&lt;h2&gt;安装&lt;/h2&gt;
&lt;p&gt;Opdumper的源码已经托管在github上，其地址为：&lt;a href=&quot;https://github.com/ericzhang-cn/opdumper&quot; target=&quot;_blank&quot;&gt;https://github.com/ericzhang-cn/opdumper&lt;/a&gt;。可以通过以下命令克隆源代码：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;git clone https://github.com/ericzhang-cn/opdumper.git&lt;/pre&gt;
&lt;p&gt;Opdumper是一个标准的PHP Extension，安装方法如下：&lt;/p&gt;
&lt;p&gt;首先将Opdumper源码放到PHP源码包的ext/opdumper目录下，进入此目录执行如下命令：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;phpize
./configure
make
make install&lt;/pre&gt;
&lt;p&gt;然后在php.ini中添加一行配置：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;extension=opdumper.so&lt;/pre&gt;
&lt;p&gt;目前opdumper支持PHP&gt;=5.3，在Linux和MacOS下测试通过，Windows下未做测试。&lt;/p&gt;
&lt;h2&gt;CLI API&lt;/h2&gt;
&lt;p&gt;Opdumper支持类似vld的命令行方式输出opcode，只需在执行php命令时通过-d参数将opdumper.active=1传入。例如我们有一个foo.php：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;&lt;?php
$a = 'hello';
echo $a;
?&gt;&lt;/pre&gt;
&lt;p&gt;执行如下命令：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;php -d opdumper.active=1 foo.php&lt;/pre&gt;
&lt;p&gt;结果如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/opdumper-and-web-opcode-dumper/1.png&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;PHP_FUNCTION API&lt;/h2&gt;
&lt;p&gt;Opdumper还支持vld不支持的PHP_FUNCTION API，Opdumper提供了两个PHP函数：od_dump_opcodes_string和od_dump_opcodes_file。前者接受一个字符串作为产生，字符串是一段PHP代码；后者接受一个PHP文件作为参数，返回值均是一个存有opcode结果的PHP数组。以od_dump_opcodes_file为例，我们在foo.php同一目录下再写一个bar.php：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;&lt;?php
$opcodes = od_dump_opcodes_file('./foo.php');
var_dump($opcodes);
?&gt;&lt;/pre&gt;
&lt;p&gt;执行结果如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;array(3) {
    [0]=&gt;
    array(8) {
        [&quot;lineno&quot;]=&gt;
        int(2)
            [&quot;opcode&quot;]=&gt;
        string(11) &quot;ZEND_ASSIGN&quot;
            [&quot;op1_type&quot;]=&gt;
        string(2) &quot;CV&quot;
            [&quot;op2_type&quot;]=&gt;
        string(5) &quot;CONST&quot;
            [&quot;result_type&quot;]=&gt;
        string(0) &quot;&quot;
            [&quot;op1&quot;]=&gt;
        string(2) &quot;~0&quot;
            [&quot;op2&quot;]=&gt;
        string(5) &quot;hello&quot;
            [&quot;result&quot;]=&gt;
        string(0) &quot;&quot;
    }
    [1]=&gt;
    array(8) {
        [&quot;lineno&quot;]=&gt;
        int(3)
            [&quot;opcode&quot;]=&gt;
        string(9) &quot;ZEND_ECHO&quot;
            [&quot;op1_type&quot;]=&gt;
        string(2) &quot;CV&quot;
            [&quot;op2_type&quot;]=&gt;
        string(6) &quot;UNUSED&quot;
            [&quot;result_type&quot;]=&gt;
        string(6) &quot;UNUSED&quot;
            [&quot;op1&quot;]=&gt;
        string(2) &quot;~0&quot;
            [&quot;op2&quot;]=&gt;
        string(6) &quot;UNUSED&quot;
            [&quot;result&quot;]=&gt;
        string(6) &quot;UNUSED&quot;
    }
    [2]=&gt;
    array(8) {
        [&quot;lineno&quot;]=&gt;
        int(5)
            [&quot;opcode&quot;]=&gt;
        string(11) &quot;ZEND_RETURN&quot;
            [&quot;op1_type&quot;]=&gt;
        string(5) &quot;CONST&quot;
            [&quot;op2_type&quot;]=&gt;
        string(6) &quot;UNUSED&quot;
            [&quot;result_type&quot;]=&gt;
        string(6) &quot;UNUSED&quot;
            [&quot;op1&quot;]=&gt;
        string(1) &quot;1&quot;
            [&quot;op2&quot;]=&gt;
        string(6) &quot;UNUSED&quot;
            [&quot;result&quot;]=&gt;
        string(6) &quot;UNUSED&quot;
    }
}&lt;/pre&gt;
&lt;h1&gt;Opdumper的Web服务：Opcode Dumper&lt;/h1&gt;
&lt;p&gt;坦白说，安装PHP模块还是挺麻烦的。所以为了方便朋友们查看opcode，我为Opdumper搭建了一个在线Web服务：&lt;a href=&quot;http://supercompiler.com/app/opcode_dumper&quot; target=&quot;_blank&quot;&gt;http://supercompiler.com/app/opcode_dumper&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;Web页面访问&lt;/h2&gt;
&lt;p&gt;只要访问这个页面，在编辑框中输入或粘贴进PHP代码，就可以快速看到相应的opcode：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/opdumper-and-web-opcode-dumper/2.png&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;HTTP API方式访问&lt;/h2&gt;
&lt;p&gt;您可以通过访问如下API获取PHP代码的opcode：&lt;/p&gt;
&lt;p&gt;URI: &lt;b&gt;http://supercompiler.com/api/dump_opcodes&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Method: &lt;b&gt;POST&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Params: &lt;b&gt;php_script=[您的PHP代码]&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;返回值为JSON格式，成功时success字段为&quot;true&quot;，data字段存储opcodes；失败时success字段为&quot;false&quot;，msg字段存放失败原因。&lt;/p&gt;
&lt;p&gt;由于跨越的关系，目前只能使用Curl而不能使用Ajax方式调用这个API，后续会为其增加JSONP接口。&lt;/p&gt;
&lt;h1&gt;结语&lt;/h1&gt;
&lt;p&gt;目前这个模块还比较初级，有很多需要完善的地方。也欢迎有兴趣的朋友通过github贡献代码。&lt;/p&gt;
</description>
</item>
<item>
<title>PHP哈希表碰撞攻击原理</title>
<link>http://blog.codinglabs.org/articles/hash-collisions-attack-on-php.html</link>
<guid>http://blog.codinglabs.org/articles/hash-collisions-attack-on-php.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Wed, 04 Jan 2012 00:00:00 GMT</pubDate>
<description>&lt;p&gt;最近哈希表碰撞攻击（Hashtable collisions as DOS attack）的话题不断被提起，各种语言纷纷中招。本文结合PHP内核源码，聊一聊这种攻击的原理及实现。&lt;/p&gt;
&lt;!--more--&gt;
&lt;h1&gt;哈希表碰撞攻击的基本原理&lt;/h1&gt;
&lt;p&gt;哈希表是一种查找效率极高的数据结构，很多语言都在内部实现了哈希表。PHP中的哈希表是一种极为重要的数据结构，不但用于表示Array数据类型，还在Zend虚拟机内部用于存储上下文环境信息（执行上下文的变量及函数均使用哈希表结构存储）。&lt;/p&gt;
&lt;p&gt;理想情况下哈希表插入和查找操作的时间复杂度均为O(1)，任何一个数据项可以在一个与哈希表长度无关的时间内计算出一个哈希值（key），然后在常量时间内定位到一个桶（术语bucket，表示哈希表中的一个位置）。当然这是理想情况下，因为任何哈希表的长度都是有限的，所以一定存在不同的数据项具有相同哈希值的情况，此时不同数据项被定为到同一个桶，称为碰撞（collision）。哈希表的实现需要解决碰撞问题，碰撞解决大体有两种思路，第一种是根据某种原则将被碰撞数据定为到其它桶，例如线性探测——如果数据在插入时发生了碰撞，则顺序查找这个桶后面的桶，将其放入第一个没有被使用的桶；第二种策略是每个桶不是一个只能容纳单个数据项的位置，而是一个可容纳多个数据的数据结构（例如链表或红黑树），所有碰撞的数据以某种数据结构的形式组织起来。&lt;/p&gt;
&lt;p&gt;不论使用了哪种碰撞解决策略，都导致插入和查找操作的时间复杂度不再是O(1)。以查找为例，不能通过key定位到桶就结束，必须还要比较原始key（即未做哈希之前的key）是否相等，如果不相等，则要使用与插入相同的算法继续查找，直到找到匹配的值或确认数据不在哈希表中。&lt;/p&gt;
&lt;p&gt;PHP是使用单链表存储碰撞的数据，因此实际上PHP哈希表的平均查找复杂度为O(L)，其中L为桶链表的平均长度；而最坏复杂度为O(N)，此时所有数据全部碰撞，哈希表退化成单链表。下图PHP中正常哈希表和退化哈希表的示意图。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/hash-collisions-attack-on-php/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;哈希表碰撞攻击就是通过精心构造数据，使得所有数据全部碰撞，人为将哈希表变成一个退化的单链表，此时哈希表各种操作的时间均提升了一个数量级，因此会消耗大量CPU资源，导致系统无法快速响应请求，从而达到拒绝服务攻击（DoS）的目的。&lt;/p&gt;
&lt;p&gt;可以看到，进行哈希碰撞攻击的前提是哈希算法特别容易找出碰撞，如果是MD5或者SHA1那基本就没戏了，幸运的是（也可以说不幸的是）大多数编程语言使用的哈希算法都十分简单（这是为了效率考虑），因此可以不费吹灰之力之力构造出攻击数据。下一节将通过分析Zend相关内核代码，找出攻击哈希表碰撞攻击PHP的方法。&lt;/p&gt;
&lt;h1&gt;Zend哈希表的内部实现&lt;/h1&gt;
&lt;h2&gt;数据结构&lt;/h2&gt;
&lt;p&gt;PHP中使用一个叫Backet的结构体表示桶，同一哈希值的所有桶被组织为一个单链表。哈希表使用HashTable结构体表示。相关源码在zend/Zend_hash.h下：&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;typedef struct bucket {
    ulong h;                        /* Used for numeric indexing */
    uint nKeyLength;
    void *pData;
    void *pDataPtr;
    struct bucket *pListNext;
    struct bucket *pListLast;
    struct bucket *pNext;
    struct bucket *pLast;
    char arKey[1]; /* Must be last element */
} Bucket;

typedef struct _hashtable {
    uint nTableSize;
    uint nTableMask;
    uint nNumOfElements;
    ulong nNextFreeElement;
    Bucket *pInternalPointer;   /* Used for element traversal */
    Bucket *pListHead;
    Bucket *pListTail;
    Bucket **arBuckets;
    dtor_func_t pDestructor;
    zend_bool persistent;
    unsigned char nApplyCount;
    zend_bool bApplyProtection;
#if ZEND_DEBUG
    int inconsistent;
#endif
} HashTable;&lt;/pre&gt;
&lt;p&gt;字段名很清楚的表明其用途，因此不做过多解释。重点明确下面几个字段：Bucket中的“h”用于存储原始key；HashTable中的nTableMask是一个掩码，一般被设为nTableSize - 1，与哈希算法有密切关系，后面讨论哈希算法时会详述；arBuckets指向一个指针数组，其中每个元素是一个指向Bucket链表的头指针。&lt;/p&gt;
&lt;h2&gt;哈希算法&lt;/h2&gt;
&lt;p&gt;PHP哈希表最小容量是8（2^3），最大容量是0x80000000（2^31），并向2的整数次幂圆整（即长度会自动扩展为2的整数次幂，如13个元素的哈希表长度为16；100个元素的哈希表长度为128）。nTableMask被初始化为哈希表长度（圆整后）减1。具体代码在zend/Zend_hash.c的_zend_hash_init函数中，这里截取与本文相关的部分并加上少量注释。&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;ZEND_API int _zend_hash_init(HashTable *ht, uint nSize, hash_func_t pHashFunction, dtor_func_t pDestructor, zend_bool persistent ZEND_FILE_LINE_DC)
{
    uint i = 3;
    Bucket **tmp;

    SET_INCONSISTENT(HT_OK);

    //长度向2的整数次幂圆整
    if (nSize &gt;= 0x80000000) {
        /* prevent overflow */
        ht-&gt;nTableSize = 0x80000000;
    } else {
        while ((1U &lt;&lt; i) &lt; nSize) {
            i++;
        }
        ht-&gt;nTableSize = 1 &lt;&lt; i;
    }

    ht-&gt;nTableMask = ht-&gt;nTableSize - 1;

    /*此处省略若干代码…*/

    return SUCCESS;
}&lt;/pre&gt;
&lt;p&gt;值得一提的是PHP向2的整数次幂取圆整方法非常巧妙，可以背下来在需要的时候使用。&lt;/p&gt;
&lt;p&gt;Zend HashTable的哈希算法异常简单：&lt;/p&gt;
&lt;p&gt;hash(key)=key &amp; nTableMask&lt;/p&gt;
&lt;p&gt;即简单将数据的原始key与HashTable的nTableMask进行按位与即可。&lt;/p&gt;
&lt;p&gt;如果原始key为字符串，则首先使用&lt;a href=&quot;http://blog.csdn.net/chen_alvin/article/details/5846714&quot; target=&quot;_blank&quot;&gt;Times33&lt;/a&gt;算法将字符串转为整形再与nTableMask按位与。&lt;/p&gt;
&lt;p&gt;hash(strkey)=time33(strkey) &amp; nTableMask&lt;/p&gt;
&lt;p&gt;下面是Zend源码中查找哈希表的代码：&lt;/p&gt;&lt;pre  class=&quot;prettyprint&quot;&gt;ZEND_API int zend_hash_index_find(const HashTable *ht, ulong h, void **pData)
{
    uint nIndex;
    Bucket *p;

    IS_CONSISTENT(ht);

    nIndex = h &amp; ht-&gt;nTableMask;

    p = ht-&gt;arBuckets[nIndex];
    while (p != NULL) {
        if ((p-&gt;h == h) &amp;&amp; (p-&gt;nKeyLength == 0)) {
            *pData = p-&gt;pData;
            return SUCCESS;
        }
        p = p-&gt;pNext;
    }
    return FAILURE;
}

ZEND_API int zend_hash_find(const HashTable *ht, const char *arKey, uint nKeyLength, void **pData)
{
    ulong h;
    uint nIndex;
    Bucket *p;

    IS_CONSISTENT(ht);

    h = zend_inline_hash_func(arKey, nKeyLength);
    nIndex = h &amp; ht-&gt;nTableMask;

    p = ht-&gt;arBuckets[nIndex];
    while (p != NULL) {
        if ((p-&gt;h == h) &amp;&amp; (p-&gt;nKeyLength == nKeyLength)) {
            if (!memcmp(p-&gt;arKey, arKey, nKeyLength)) {
                *pData = p-&gt;pData;
                return SUCCESS;
            }
        }
        p = p-&gt;pNext;
    }
    return FAILURE;
}&lt;/pre&gt;
&lt;p&gt;其中zend_hash_index_find用于查找整数key的情况，zend_hash_find用于查找字符串key。逻辑基本一致，只是字符串key会通过zend_inline_hash_func转为整数key，zend_inline_hash_func封装了times33算法，具体代码就不贴出了。&lt;/p&gt;
&lt;h1&gt;攻击&lt;/h1&gt;
&lt;h2&gt;基本攻击&lt;/h2&gt;
&lt;p&gt;知道了PHP内部哈希表的算法，就可以利用其原理构造用于攻击的数据。一种最简单的方法是利用掩码规律制造碰撞。上文提到Zend HashTable的长度nTableSize会被圆整为2的整数次幂，假设我们构造一个2^16的哈希表，则nTableSize的二进制表示为：1 0000 0000 0000 0000，而nTableMask = nTableSize – 1为：0 1111 1111 1111 1111。接下来，可以以0为初始值，以2^16为步长，制造足够多的数据，可以得到如下推测：&lt;/p&gt;
&lt;p&gt;0000 0000 0000 0000 0000 &amp; 0 1111 1111 1111 1111 = 0&lt;/p&gt;
&lt;p&gt;0001 0000 0000 0000 0000 &amp; 0 1111 1111 1111 1111 = 0&lt;/p&gt;
&lt;p&gt;0010 0000 0000 0000 0000 &amp; 0 1111 1111 1111 1111 = 0&lt;/p&gt;
&lt;p&gt;0011 0000 0000 0000 0000 &amp; 0 1111 1111 1111 1111 = 0&lt;/p&gt;
&lt;p&gt;0100 0000 0000 0000 0000 &amp; 0 1111 1111 1111 1111 = 0&lt;/p&gt;
&lt;p&gt;……&lt;/p&gt;
&lt;p&gt;概况来说只要保证后16位均为0，则与掩码位于后得到的哈希值全部碰撞在位置0。&lt;/p&gt;
&lt;p&gt;下面是利用这个原理写的一段攻击代码：&lt;/p&gt;&lt;pre  class=&quot;prettyprint&quot;&gt;&lt;?php

$size = pow(2, 16);

$startTime = microtime(true);

$array = array();
for ($key = 0, $maxKey = ($size - 1) * $size; $key &lt;= $maxKey; $key += $size) {
    $array[$key] = 0;
}

$endTime = microtime(true);

echo $endTime - $startTime, ' seconds', &quot;\n&quot;;&lt;/pre&gt;
&lt;p&gt;这段代码在我的VPS上（单CPU，512M内存）上用了近88秒才完成，并且在此期间CPU资源几乎被用尽：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/hash-collisions-attack-on-php/2.png&quot;/&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/hash-collisions-attack-on-php/3.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;而普通的同样大小的哈希表插入仅用时0.036秒：&lt;/p&gt;&lt;pre  class=&quot;prettyprint&quot;&gt;&lt;?php

$size = pow(2, 16);

$startTime = microtime(true);

$array = array();
for ($key = 0, $maxKey = ($size - 1) * $size; $key &lt;= $size; $key += 1) {
    $array[$key] = 0;
}

$endTime = microtime(true);

echo $endTime - $startTime, ' seconds', &quot;\n&quot;;&lt;/pre&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/hash-collisions-attack-on-php/4.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;可以证明第二段代码插入N个元素的时间在O(N)水平，而第一段攻击代码则需O(N^2)的时间去插入N个元素。&lt;/p&gt;
&lt;h2&gt;POST攻击&lt;/h2&gt;
&lt;p&gt;当然，一般情况下很难遇到攻击者可以直接修改PHP代码的情况，但是攻击者仍可以通过一些方法间接构造哈希表来进行攻击。例如PHP会将接收到的HTTP POST请求中的数据构造为$_POST，而这是一个Array，内部就是通过Zend HashTable表示，因此攻击者只要构造一个含有大量碰撞key的post请求，就可以达到攻击的目的。具体做法不再演示。&lt;/p&gt;
&lt;h1&gt;防护&lt;/h1&gt;
&lt;h2&gt;POST攻击的防护&lt;/h2&gt;
&lt;p&gt;针对POST方式的哈希碰撞攻击，目前PHP的防护措施是控制POST数据的数量。在&gt;=PHP5.3.9的版本中增加了一个配置项max_input_vars，用于标识一次http请求最大接收的参数个数，默认为1000。因此PHP5.3.x的用户可以通过升级至5.3.9来避免哈希碰撞攻击。5.2.x的用户可以使用这个patch：&lt;a href=&quot;http://www.laruence.com/2011/12/30/2440.html&quot;&gt;http://www.laruence.com/2011/12/30/2440.html&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;另外的防护方法是在Web服务器层面进行处理，例如限制http请求body的大小和参数的数量等，这个是现在用的最多的临时处理方案。具体做法与不同Web服务器相关，不再详述。&lt;/p&gt;
&lt;h2&gt;其它防护&lt;/h2&gt;
&lt;p&gt;上面的防护方法只是限制POST数据的数量，而不能彻底解决这个问题。例如，如果某个POST字段是一个json数据类型，会被PHP &lt;a href=&quot;http://cn.php.net/manual/en/function.json-decode.php&quot; target=&quot;_blank&quot;&gt;json_decode&lt;/a&gt;，那么只要构造一个超大的json攻击数据照样可以达到攻击目的。理论上，只要PHP代码中某处构造Array的数据依赖于外部输入，则都可能造成这个问题，因此彻底的解决方案要从Zend底层HashTable的实现动手。一般来说有两种方式，一是限制每个桶链表的最长长度；二是使用其它数据结构如&lt;a href=&quot;http://en.wikipedia.org/wiki/Red%E2%80%93black_tree&quot; target=&quot;_blank&quot;&gt;红黑树&lt;/a&gt;取代链表组织碰撞哈希（并不解决哈希碰撞，只是减轻攻击影响，将N个数据的操作时间从O(N^2)降至O(NlogN)，代价是普通情况下接近O(1)的操作均变为O(logN)）。&lt;/p&gt;
&lt;p&gt;目前使用最多的仍然是POST数据攻击，因此建议生产环境的PHP均进行升级或打补丁。至于从数据结构层面修复这个问题，目前还没有任何方面的消息。&lt;/p&gt;
&lt;h1&gt;参考&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&quot;http://nikic.github.com/2011/12/28/Supercolliding-a-PHP-array.html&quot; target=&quot;_blank&quot;&gt;Supercolliding a PHP array&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;http://www.laruence.com/2011/12/30/2440.html&quot; target=&quot;_blank&quot;&gt;PHP5.2.*防止Hash冲突拒绝服务攻击的Patch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&quot;http://www.laruence.com/2011/12/29/2412.html&quot; target=&quot;_blank&quot;&gt;通过构造Hash冲突实现各种语言的拒绝服务攻击&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a href=&quot;http://www.laruence.com/2011/12/30/2435.html&quot; target=&quot;_blank&quot;&gt;PHP数组的Hash冲突实例&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] &lt;a href=&quot;http://www.php.net/archive/2011.php&quot; target=&quot;_blank&quot;&gt;PHP 5.4.0 RC4 released&lt;/a&gt;&lt;/p&gt;
</description>
</item>
<item>
<title>闭包漫谈（从抽象代数及函数式编程角度）</title>
<link>http://blog.codinglabs.org/articles/closure-perspective-of-abstract-mathematic-and-functional-language.html</link>
<guid>http://blog.codinglabs.org/articles/closure-perspective-of-abstract-mathematic-and-functional-language.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Mon, 05 Dec 2011 00:00:00 GMT</pubDate>
<description>&lt;p&gt;如果Google一下“闭包”这个词，会发现网上关于闭包的文章已经不计其数，甚至很多人将闭包看做面试JavaScript程序员的必考题（虽然闭包和JavaScript没有什么必然联系）。既然如此，我为什么还要写一篇关于闭包的文章呢？&lt;/p&gt;
&lt;p&gt;首先，虽然网上关于闭包的文章甚多，但是很少以较为形式化的角度阐述闭包，而我认为理解闭包的关键之一就是从形式化角度理解其涵义；其次，大多数文章将闭包的概念与JavaScript语言绑定太死，这样容易局限对闭包概念的理解，而难以窥探到其本质。从JavaScript去理解闭包，个人认为这是本末倒置的，应该先理解纯粹意义上的闭包，然后再理解JavaScript中的闭包机制。&lt;/p&gt;
&lt;p&gt;基于以上情况，本文将从较为形式化的角度阐述闭包概念，当做对现有网络的资料的一个补充。&lt;/p&gt;
&lt;h1&gt;一个需要明确的重要事实&lt;/h1&gt;
&lt;p&gt;在开始阐述闭包之前，我需要特别明确一个非常重要的事实，那就是“闭包（closure）”一词被用于定义两个毫不相关的概念，分别是数学领域&lt;a href=&quot;http://en.wikipedia.org/wiki/Abstract_algebra&quot; target=&quot;_blank&quot;&gt;抽象代数&lt;/a&gt;分支下用于描述集合之于运算的一种性质以及计算机科学程序设计语言分支用于描述函数式语言所支持的一种机制。这种不同大约就如“电影《人猿泰山》”和“五岳之尊泰山”中的“泰山”差不多，两个短语中的“泰山”描述的是两个风马牛不相及的概念，虽然是同一个词。&lt;/p&gt;
&lt;p&gt;一般来说，作为程序员我们说的闭包更多是指后者，但是如果你与我一样同时具有一点数学背景，第一次接触“闭包”一词是在抽象数学中的话，那么当刚接触到计算机中的“闭包”时多少会产生困惑，同样，如果你是一个纯粹的程序员，那么当在数学著作中读到“闭包”一词时请小心区分这个“闭包”具体是表述哪一个概念。&lt;/p&gt;
&lt;p&gt;我会在下文中分别阐述数学领域和计算机科学领域闭包的概念。&lt;/p&gt;
&lt;h1&gt;抽象代数中的闭包&lt;/h1&gt;
&lt;p&gt;抽象代数是一门研究代数结构的数学分支，它的研究对象包括群、环、域和向量空间等等。当然我在这里丝毫没有要大谈特谈这些令人头大的概念的意思，我会尽量以一种易懂的半形式化方式去阐述一些概念。&lt;/p&gt;
&lt;h2&gt;集合的定义&lt;/h2&gt;
&lt;p&gt;非正式地，集合是N个对象组成的一个无序、互异且确定的整体。其中N是自然数。这些对象称为集合的元素。&lt;/p&gt;
&lt;p&gt;无序性是指集合中的元素不存在序关系（集合上可以定义序关系，但这些序关系不是集合本身的一部分），每个元素的地位是相同的。&lt;/p&gt;
&lt;p&gt;互异性是指集合中任意两个元素是不同的，即同一集合中任意两个元素间不存在等价关系。&lt;/p&gt;
&lt;p&gt;确定是指对任意一个对象和任意一个集合，这个对象要么属于此集合，要么不属于此集合，二者必居其一，不存在模棱两可的状态（模糊数学中有一种中间隶属状态，本文不考虑模糊数学领域）。&lt;/p&gt;
&lt;h2&gt;运算的定义&lt;/h2&gt;
&lt;p&gt;非正式地，集合上的n元运算是一个映射，这个映射将作用于任意n个集合中的元素，并映射为一个结果（注意结果不一定属于这个集合）。&lt;/p&gt;
&lt;p&gt;例如，设\(N^+\)是正整数集合，那么加法（\(+\)）和减法（\(-\)）都可以看做定义在\(N^+\)上的二元运算，因为任意两个正整数都可以进行加减。&lt;/p&gt;
&lt;h2&gt;封闭的定义&lt;/h2&gt;
&lt;p&gt;有了集合和运算的概念，就可以定义封闭的概念了。&lt;/p&gt;
&lt;p&gt;非正式地，如果定义于集合\(A\)上的运算\(\oplus\)的运算结果仍然属于\(A\)，那么运算\(\oplus\)对于集合\(A\)是封闭的。下面给出“封闭”的一个半形式化定义：&lt;/p&gt;
&lt;p&gt;如果对于任意\(a_1,a_2\in A\)，有\(a_1\oplus a_2\in A\)，那么说二元运算\(\oplus\)对于集合A是封闭的。&lt;/p&gt;
&lt;p&gt;例如“\(+\)”对于\(N^+\)是封闭的，因为任意两个正整数的和结果仍然是正整数；但是“\(-\)”对于\(N^+\)不是封闭的，例如3和5属于\(N^+\)，但是：\(3-5=-2\notin N^+\)。&lt;/p&gt;
&lt;h2&gt;闭包性质&lt;/h2&gt;
&lt;p&gt;一个集合满足闭包性质当且仅当这个集合在某个运算或某些运算的搜集下是封闭的，其中“某些运算的搜集下封闭”是指这个集合单独闭合在每个运算之下。&lt;/p&gt;
&lt;p&gt;值得一提的是，之前这条定义往往被作为一条公理引入一个代数结构，叫做“闭包公理”。但是现代集合论往往将运算形式化的定义为集合间的运算，所以将其作为公理引入代数结构是多余的（因为可以通过其它公理间接定义闭包公理），但是对于子集是否闭合的问题，闭包公理仍然有意义。&lt;/p&gt;
&lt;h2&gt;一个例子 - 存在于形式语言与自动机理论中的闭包&lt;/h2&gt;
&lt;p&gt;上面说了很多东西，我们来看一个抽象代数领域闭包的例子。&lt;/p&gt;
&lt;p&gt;我们回想在形式语言与自动机理论中（或者编译原理中也有提到）在定义语言时做的一些推导。&lt;/p&gt;
&lt;p&gt;一般我们会先定义一个有限集合\(\Sigma\)，叫做字母表，\(\Sigma\)的n阶幂运算定义为形成一个新的集合\(\Sigma ^n\)，一个对象属于\(\Sigma ^n\)当且仅当它是\(\Sigma\)中任意n个字母连接成的串，其中\(\Sigma ^0=\left \{ \varepsilon \right \}\)。而&lt;/p&gt;
&lt;p&gt;\(\Sigma ^*=\Sigma^0 \cup \Sigma^1 \cup ... \cup \Sigma^n \cup ...\)&lt;/p&gt;
&lt;p&gt;此时集合\(\Sigma ^*\)满足闭包性质，因为这个集合对于幂运算是封闭的。有兴趣的读者可以自行证明一下。&lt;/p&gt;
&lt;h1&gt;函数式编程中的闭包&lt;/h1&gt;
&lt;p&gt;在这一章节开始之前，我需要再和大家明确一个比较纠结的事实，就是在函数式编程领域中当说到“闭包”时，也有可能是指数学领域中闭包的概念，这是因为函数式编程在基础理论与抽象代数有一定亲缘性，所以当在函数式语言著作中讨论“闭包”时，有可能是在抽象数学的上下文中讨论的。然而，在表述上可能会有微妙变化。在函数式语言领域对于数学闭包常用的表述是“如果一个运算的结果仍然能被此运算作用，则这个运算是封闭的”，要注意这只不过是上文提到的“闭包”概念的另一种等价表述而已，如果我们将这个运算的所有结果看做一个集合，那么就可以等价表述说这个运算在这个集合上是封闭的。&lt;/p&gt;
&lt;p&gt;而我下面所要阐述的闭包是一种截然不同的概念。所以，当在函数式语言的著作中看到“闭包”时，需要根据上下文环境小心区分其表述哪种概念。&lt;/p&gt;
&lt;h2&gt;Lambda演算与自由变量&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Functional_programming&quot; target=&quot;_blank&quot;&gt;函数式编程语言&lt;/a&gt;的基础是&lt;a href=&quot;http://en.wikipedia.org/wiki/Lambda_calculus&quot; target=&quot;_blank&quot;&gt;lambda演算&lt;/a&gt;，这是一套用于研究函数定义、应用和递归的形式系统，由数学家&lt;a href=&quot;http://en.wikipedia.org/wiki/Alonzo_Church&quot; target=&quot;_blank&quot;&gt;丘奇&lt;/a&gt;在20世纪30年代引入。如果您不太熟悉lambda演算，那么维基百科相关页面是很好的快速入门资料，请原谅我不会完整描述lambda演算（因为已经有&lt;a href=&quot;http://en.wikipedia.org/wiki/Lambda_calculus&quot; target=&quot;_blank&quot;&gt;很多可以参考的资料&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;简单来说lambda演算将计算过程看过一系列的函数代换，例如，下面是加运算的lambda函数（假设+运算已经定义）：&lt;/p&gt;
&lt;p&gt;\(\lambda x.\lambda y.x+y\)&lt;/p&gt;
&lt;p&gt;lambda演算就是反复将函数应用于实际值，并用实际值代替参数，最终得出结果。例如下面是7+2的计算过程：&lt;/p&gt;
&lt;p&gt;\((\lambda x.\lambda y.x+y)7 \hspace{2mm}2 \Rightarrow (\lambda y.7+y)2 \Rightarrow 7+2 \Rightarrow 9\)&lt;/p&gt;
&lt;p&gt;首先用第一个参数（7）代替最外层函数的参数（x），然后用第二个参数（2）代替第二层函数的参数（y），最终得到计算结果。&lt;/p&gt;
&lt;p&gt;鉴于如果下面大量使用lambda演算描述问题大家可能会崩溃（我也会崩溃），我将改用函数式语言&lt;a href=&quot;http://en.wikipedia.org/wiki/Scheme_%28programming_language%29&quot; target=&quot;_blank&quot;&gt;scheme&lt;/a&gt;（&lt;a href=&quot;http://en.wikipedia.org/wiki/Lisp_%28programming_language%29&quot; target=&quot;_blank&quot;&gt;Lisp&lt;/a&gt;的一个方言）来进行问题描述。注意其实scheme在本质上与lambda演算是等价的，只是看起来更好懂，例如不需要遵循lambda演算一个变态的规定：每个函数只允许有一个参数（虽然任何多参数函数式程序都可以通过&lt;a href=&quot;http://en.wikipedia.org/wiki/Currying&quot; target=&quot;_blank&quot;&gt;Currying&lt;/a&gt;过程化归为等价的lambda演算）。&lt;/p&gt;
&lt;p&gt;下面是用scheme程序对上述lambda演算的等价表示：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;(define (f x y) (+ x y))&lt;/pre&gt;
&lt;p&gt;可以这样计算7+2：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;(f 7 2)
;Value: 9&lt;/pre&gt;
&lt;p&gt;下面看一个稍微复杂点的例子：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;(define (f x) (lambda (y) (+ x y)))&lt;/pre&gt;
&lt;p&gt;这里定义了函数f，接受一个参数x，特别要注意它的返回值：不是一个值而是一个匿名函数。如果我们把这个函数单独拿出来：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;(lambda (y) (+ x y))&lt;/pre&gt;
&lt;p&gt;可以看到，这个匿名函数接收一个参数y，但是却没有参数x！也就是说，如果脱离上下文执行这个函数，则x处于未指定状态，我们说对于这个函数，y是绑定的，而x是自由的。&lt;/p&gt;
&lt;p&gt;一般地：x是一个函数的函数体中的变量，如果x被这个函数的参数指定，则x是绑定于这个函数的，否则说x对于此函数是自由的。&lt;/p&gt;
&lt;p&gt;下面可以看到，变量的绑定和自由概念是理解闭包本质的一把钥匙。&lt;/p&gt;
&lt;h2&gt;程序语言的闭包性质&lt;/h2&gt;
&lt;p&gt;继续上面的scheme程序，我们已经定义了函数f：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;(define (f x) (lambda (y) (+ x y)))&lt;/pre&gt;
&lt;p&gt;如果我们运行下面程序：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;(f 7)
;Value 13: #[compound-procedure 13]&lt;/pre&gt;
&lt;p&gt;可以看到，f返回了一个过程（匿名函数），按照函数演算规则，这个函数应该是：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;(lambda (y) (+ 7 y))&lt;/pre&gt;
&lt;p&gt;那么下面的运算就很直观了：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;((f 7) 2)
;Value: 9&lt;/pre&gt;
&lt;p&gt;注意这里有一个非常重要的地方（也是闭包性质的关键），那就是这个运算执行了两个函数：f和匿名函数。而f的作用域为(f 7)，这就是说，其实在(f 7)之后，f这个函数就结束了，而x（这里被赋值为7）是f的私有变量（绑定于f），那么程序设计语言的设计者就有两种选择：第一，在函数超出其作用域后立即销毁其绑定变量，如果是这样的话，((f 7) 2) 是无法得出结果的，因为在外层的f运算结束后，存放数值“7”的变量就被释放了，所以匿名函数无法得到其自由变量x的值；显然scheme的设计者做了第二种选择：如果一个函数返回另一个函数，而被返回函数又需要外层函数的变量时，不会立即释放这个变量，而是允许被返回的函数引用这些变量。支持这种机制的语言称为支持闭包机制，而这个内部函数连同其自由变量就形成了一个闭包。&lt;/p&gt;
&lt;p&gt;上面的这段话不太好理解，但是请务必多读几遍，因为，这就是闭包的全部。&lt;/p&gt;
&lt;h2&gt;从Scheme到JavaScript&lt;/h2&gt;
&lt;p&gt;好的，现在开始讨论JavaScript中的闭包。&lt;/p&gt;
&lt;p&gt;上文说过，闭包是函数式语言才有的机制，或者说支持函数式编程泛型的语言才有的性质。一个语言支持函数式编程泛型，如果它同时具有下列特性：&lt;/p&gt;
&lt;p&gt;可以将一个函数赋值给一个变量。&lt;/p&gt;
&lt;p&gt;函数可以作为参数传递给另一个函数。&lt;/p&gt;
&lt;p&gt;函数的返回值可以是一个函数。&lt;/p&gt;
&lt;p&gt;结合上面关于闭包性质的定义，就很清楚为什么只有支持函数编程泛型的语言才可以谈闭包性质。&lt;/p&gt;
&lt;p&gt;很显然，JavaScript是具有上述三条性质的，所以可以说JavaScript拥有函数式编程泛型。当然，一般我们还是习惯于用命令式的写JavaScript，但是其闭包本质是一样的。为了说明这一点，我将会首先用JavaScript按照函数式泛型重写上面的scheme程序，然后转为命令式。&lt;/p&gt;
&lt;p&gt;上文用scheme所写的函数f，可以用JavaScript等价实现如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;function f(x){ return function(y) { return x + y; }; }&lt;/pre&gt;
&lt;p&gt;可以执行与上述scheme类似的计算（因为脱离了浏览器，我是用nodejs执行这段JavaScript）：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;console.log( (f(7)) (2) );
//9&lt;/pre&gt;
&lt;p&gt;其中f返回的匿名函数与其自由变量x组成了一个闭包系统。&lt;/p&gt;
&lt;p&gt;如果用命令式重写上面的程序，就得到了我们熟悉的闭包：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;function f(x{
    return function(y) {
        return x + y;
    };
}

var lam = f(7);
console.log(lam(2));&lt;/pre&gt;
&lt;h1&gt;总结&lt;/h1&gt;
&lt;p&gt;本文分别讨论了抽象代数和函数式编程中两个截然不同的闭包概念，当然，作为程序员我们更关心的是后者（但前者也不是对程序员一点用也没有，如果学习函数式语言的构造原理，抽象代数中的闭包也是必须理解的概念）。实际上，闭包远没有很多人想象的复杂和神秘，只不过是函数式编程中一个普通的概念，但是可能因为我们大多数人是从命令式编程语言学习编程，很少去写函数式程序，而要理解闭包却是需要结合函数式编程泛型，因此很难看清闭包的本质。希望本文能对您有所帮助，同时我个人也建议可以学习一门函数式语言，这样对很多概念的理解非常有好处。&lt;/p&gt;
</description>
</item>
<item>
<title>深入研究PHP及Zend Engine的线程安全模型</title>
<link>http://blog.codinglabs.org/articles/zend-thread-safety.html</link>
<guid>http://blog.codinglabs.org/articles/zend-thread-safety.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Fri, 04 Nov 2011 00:00:00 GMT</pubDate>
<description>&lt;p&gt;在阅读PHP源码和学习PHP扩展开发的过程中，我接触到大量含有“TSRM”字眼的宏。通过查阅资料，知道这些宏与Zend的线程安全机制有关，而绝大多数资料中都建议按照既定规则使用这些宏就可以，而没有说明这些宏的具体作用。不知道怎么回事总是令人不舒服的，因此我通过阅读源码和查阅有限的资料简要了解一下相关机制，本文是我对研究内容的总结。&lt;/p&gt;
&lt;p&gt;本文首先解释了线程安全的概念及PHP中线程安全的背景，然后详细研究了PHP的线程安全机制ZTS（Zend Thread Safety）及具体的实现TSRM，研究内容包括相关数据结构、实现细节及运行机制，最后研究了Zend对于单线程和多线程环境的选择性编译问题。
&lt;!--more--&gt;
&lt;/p&gt;
&lt;h1&gt;线程安全&lt;/h1&gt;
&lt;p&gt;线程安全问题，一言以蔽之就是多线程环境下如何安全存取公共资源。我们知道，每个线程只拥有一个私有栈，共享所属进程的堆。在C中，当一个变量被声明在任何函数之外时，就成为一个全局变量，这时这个变量会被分配到进程的共享存储空间，不同线程都引用同一个地址空间，因此一个线程如果修改了这个变量，就会影响到全部线程。这看似为线程共享数据提供了便利，但是PHP往往是每个线程处理一个请求，因此希望每个线程拥有一个全局变量的副本，而不希望请求间相互干扰。&lt;/p&gt;
&lt;p&gt;早期的PHP往往用于单线程环境，每个进程只启动一个线程，因此不存在线程安全问题。后来出现了多线程环境下使用PHP的场景，因此Zend引入了Zend线程安全机制（Zend Thread Safety，简称ZTS）用于保证线程的安全。&lt;/p&gt;
&lt;h1&gt;ZTS的基本原理及实现&lt;/h1&gt;
&lt;h2&gt;基本思想&lt;/h2&gt;
&lt;p&gt;说起来ZTS的基本思想是很直观的，不是就是需要每个全局变量在每个线程都拥有一个副本吗？那我就提供这样的机制：&lt;/p&gt;
&lt;p&gt;在多线程环境下，申请全局变量不再是简单声明一个变量，而是整个进程在堆上分配一块内存空间用作“线程全局变量池”，在进程启动时初始化这个内存池，每当有线程需要申请全局变量时，通过相应方法调用TSRM（Thread Safe Resource Manager，ZTS的具体实现）并传递必要的参数（如变量大小等等），TSRM负责在内存池中分配相应内存区块并将这块内存的引用标识返回，这样下次这个线程需要读写此变量时，就可以通过将唯一的引用标识传递给TSRM，TSRM将负责真正的读写操作。这样就实现了线程安全的全局变量。下图给出了ZTS原理的示意图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/zend-thread-safety/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Thread1和Thread2同属一个进程，其中各自需要一个全局变量Global Var，TSRM为两者在线程全局内存池中（黄色部分）各自分配了一个区域，并且通过唯一的ID进行标识，这样两个线程就可以通过TSRM存取自己的变量而互不干扰。&lt;/p&gt;
&lt;p&gt;下面通过具体的代码片段看一下Zend具体是如何实现这个机制的。这里我用的是PHP5.3.8的源码。&lt;/p&gt;
&lt;p&gt;TSRM的实现代码在PHP源码的“TSRM”目录下。&lt;/p&gt;
&lt;h2&gt;数据结构&lt;/h2&gt;
&lt;p&gt;TSRM中比较重要的数据结构有两个：tsrm_tls_entry和tsrm_resource_type。下面先看tsrm_tls_entry。&lt;/p&gt;
&lt;p&gt;tsrm_tls_entry定义在TSRM/TSRM.c中：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;typedef struct _tsrm_tls_entry tsrm_tls_entry;

struct _tsrm_tls_entry {
    void **storage;
    int count;
    THREAD_T thread_id;
    tsrm_tls_entry *next;
}&lt;/pre&gt;
&lt;p&gt;每个tsrm_tls_entry结构负责表示一个线程的所有全局变量资源，其中thread_id存储线程ID，count记录全局变量数，next指向下一个节点。storage可以看做指针数组，其中每个元素是一个指向本节点代表线程的一个全局变量。最终各个线程的tsrm_tls_entry被组成一个链表结构，并将链表头指针赋值给一个全局静态变量tsrm_tls_table。注意，因为tsrm_tls_table是一个货真价实的全局变量，所以所有线程会共享这个变量，这就实现了线程间的内存管理一致性。tsrm_tls_entry和tsrm_tls_table结构的示意图如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/zend-thread-safety/2.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;tsrm_resource_type的内部结构相对简单一些：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;typedef struct {
    size_t size;
    ts_allocate_ctor ctor;
    ts_allocate_dtor dtor;
    int done;
} tsrm_resource_type;&lt;/pre&gt;
&lt;p&gt;上文说过tsrm_tls_entry是以线程为单位的（每个线程一个节点），而tsrm_resource_type以资源（或者说全局变量）为单位，每次一个新的资源被分配时，就会创建一个tsrm_resource_type。所有tsrm_resource_type以数组（线性表）的方式组成tsrm_resource_table，其下标就是这个资源的ID。每个tsrm_resource_type存储了此资源的大小和构造、析构方法指针。某种程度上，tsrm_resource_table可以看做是一个哈希表，key是资源ID，value是tsrm_resource_type结构。&lt;/p&gt;
&lt;h2&gt;实现细节&lt;/h2&gt;
&lt;p&gt;这一小节分析TSRM一些算法的实现细节。因为整个TSRM涉及代码比较多，这里拣其中具有代表性的两个函数分析。&lt;/p&gt;
&lt;p&gt;第一个值得注意的是tsrm_startup函数，这个函数在进程起始阶段被sapi调用，用于初始化TSRM的环境。由于tsrm_startup略长，这里摘录出我认为应该注意的地方：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;/* Startup TSRM (call once for the entire process) */
TSRM_API int tsrm_startup(int expected_threads, int expected_resources, int debug_level, char *debug_filename)
{
    /* code... */

    tsrm_tls_table_size = expected_threads;

    tsrm_tls_table = (tsrm_tls_entry **) calloc(tsrm_tls_table_size, sizeof(tsrm_tls_entry *));
    if (!tsrm_tls_table) {
        TSRM_ERROR((TSRM_ERROR_LEVEL_ERROR, &quot;Unable to allocate TLS table&quot;));
        return 0;
    }
    id_count=0;

    resource_types_table_size = expected_resources;
    resource_types_table = (tsrm_resource_type *) calloc(resource_types_table_size, sizeof(tsrm_resource_type));
    if (!resource_types_table) {
        TSRM_ERROR((TSRM_ERROR_LEVEL_ERROR, &quot;Unable to allocate resource types table&quot;));
        free(tsrm_tls_table);
        tsrm_tls_table = NULL;
        return 0;
    }

    /* code... */

    return 1;
}&lt;/pre&gt;
&lt;p&gt;其实tsrm_startup的主要任务就是初始化上文提到的两个数据结构。第一个比较有意思的是它的前两个参数：expected_threads和expected_resources。这两个参数由sapi传入，表示预计的线程数和资源数，可以看到tsrm_startup会按照这两个参数预先分配空间（通过calloc）。因此TSRM会首先分配可容纳expected_threads个线程和expected_resources个资源的。要看各个sapi默认会传入什么，可以看各个sapi的源码（在sapi目录下），我简单看了一下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/zend-thread-safety/3.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;可以看到比较常用的sapi如mod_php5、php-fpm和cgi都是预分配一个线程和一个资源，这样是因为不愿浪费内存空间，而且多数情况下PHP还是运行于单线程环境。&lt;/p&gt;
&lt;p&gt;这里还可以看到一个id_count变量，这个变量是一个全局静态变量，其作用就是通过自增产生资源ID，这个变量在这里被初始化为0。所以TSRM产生资源ID的方式非常简单：就是一个整形变量的自增。&lt;/p&gt;
&lt;p&gt;第二个需要仔细分析的就是ts_allocate_id，编写过PHP扩展的朋友对这个函数肯定不陌生，这个函数用于在多线程环境下申请一个全局变量并返回资源ID。&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;/* allocates a new thread-safe-resource id */
TSRM_API ts_rsrc_id ts_allocate_id(ts_rsrc_id *rsrc_id, size_t size, ts_allocate_ctor ctor, ts_allocate_dtor dtor)
{
    int i;

    TSRM_ERROR((TSRM_ERROR_LEVEL_CORE, &quot;Obtaining a new resource id, %d bytes&quot;, size));

    tsrm_mutex_lock(tsmm_mutex);

    /* obtain a resource id */
    *rsrc_id = TSRM_SHUFFLE_RSRC_ID(id_count++);
    TSRM_ERROR((TSRM_ERROR_LEVEL_CORE, &quot;Obtained resource id %d&quot;, *rsrc_id));

    /* store the new resource type in the resource sizes table */
    if (resource_types_table_size &lt; id_count) {
        resource_types_table = (tsrm_resource_type *) realloc(resource_types_table, sizeof(tsrm_resource_type)*id_count);
        if (!resource_types_table) {
            tsrm_mutex_unlock(tsmm_mutex);
            TSRM_ERROR((TSRM_ERROR_LEVEL_ERROR, &quot;Unable to allocate storage for resource&quot;));
            *rsrc_id = 0;
            return 0;
        }
        resource_types_table_size = id_count;
    }
    resource_types_table[TSRM_UNSHUFFLE_RSRC_ID(*rsrc_id)].size = size;
    resource_types_table[TSRM_UNSHUFFLE_RSRC_ID(*rsrc_id)].ctor = ctor;
    resource_types_table[TSRM_UNSHUFFLE_RSRC_ID(*rsrc_id)].dtor = dtor;
    resource_types_table[TSRM_UNSHUFFLE_RSRC_ID(*rsrc_id)].done = 0;

    /* enlarge the arrays for the already active threads */
    for (i=0; i&lt;tsrm_tls_table_size; i++) {
        tsrm_tls_entry *p = tsrm_tls_table[i];

        while (p) {
            if (p-&gt;count &lt; id_count) {
                int j;

                p-&gt;storage = (void *) realloc(p-&gt;storage, sizeof(void *)*id_count);
                for (j=p-&gt;count; j&lt;id_count; j++) {
                    p-&gt;storage[j] = (void *) malloc(resource_types_table[j].size);
                    if (resource_types_table[j].ctor) {
                        resource_types_table[j].ctor(p-&gt;storage[j], &amp;p-&gt;storage);
                    }
                }
                p-&gt;count = id_count;
            }
            p = p-&gt;next;
        }
    }
    tsrm_mutex_unlock(tsmm_mutex);

    TSRM_ERROR((TSRM_ERROR_LEVEL_CORE, &quot;Successfully allocated new resource id %d&quot;, *rsrc_id));
    return *rsrc_id;
}&lt;/pre&gt;
&lt;p&gt;rsrc_id最终存放的就是新资源的ID。其实这个函数的一些实现方式让我比较费解，首先是返回ID的方式。因为rsrc_id是按引入传入的，所以最终也就应该包含资源ID，那么最后完全不必在return *rsrc_id，可以返回一个预订整数表示成功或失败（例如1成功，0失败），这里有点费两遍事的意思，而且多了一次寻址。另外“*rsrc_id = TSRM_SHUFFLE_RSRC_ID(id_count++); ”让我感觉很奇怪，因为TSRM_SHUFFLE_RSRC_ID被定义为“((rsrc_id)+1)”，那么这里展开就是：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;*rsrc_id = ((id_count++)+1)&lt;/pre&gt;
&lt;p&gt;为什么不写成这样呢：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;*rsrc_id = ++id_count&lt;/pre&gt;
&lt;p&gt;真是怪哉。&lt;/p&gt;
&lt;p&gt;好的，且不管实现是否合理，我们先继续研究这个函数吧。&lt;/p&gt;
&lt;p&gt;首先要将id_count自增，生成一个新的资源ID，然后为这个新资源创建一个tsrm_resource_type并放入resource_type_table，接着遍历所有线程（注意是所有）为每一个线程的tsrm_tls_entry分配这个线程全局变量需要的内存空间（p-&gt;storage[j] = (void *) malloc(resource_types_table[j].size); ）。&lt;/p&gt;
&lt;p&gt;这里需要注意，对于每一次ts_allocate_id调用，Zend会遍历所有线程并为每一个线程分配相应资源，因为ts_allocate_id实际是在MINIT阶段被调用，而不是在请求处理阶段被调用的。换言之，TSRM会在进程建立时统一分配好线程全局资源，关于这个下文会专门描述。&lt;/p&gt;
&lt;p&gt;抽象来看，可以将整个线程全局资源池看做一个矩阵，一个维度为线程，一个维度为id_count，因此任意时刻所有线程全局变量的数量为“线程数*id_count”。tsrm_tls_entry和tsrm_resource_type可以看做这个矩阵在两个维度上的索引。&lt;/p&gt;
&lt;p&gt;通过分析可以看出，每次调用ts_allocate_id的代价是很大的，由于ts_allocate_id并没有预先分配算法，每次在id_count维度申请一个新的变量，就涉及两次realloc和N次malloc（N为线程数），申请M个全局变量的代价为：&lt;/p&gt;
&lt;p&gt;2 * M * t(realloc) + N * M * t(malloc)&lt;/p&gt;
&lt;p&gt;因此要尽量减少ts_allocate_id的调用次数。正因这个原因，在PHP扩展开发中提倡将一个模块所需的全局变量声明为一个结构体然后一次性申请，而不要分开申请。&lt;/p&gt;
&lt;h1&gt;ZTS与生命周期&lt;/h1&gt;
&lt;p&gt;这里需要简单提一下PHP的生命周期。&lt;/p&gt;
&lt;p&gt;PHP的具体生命周期模式取决于sapi的实现，但一般都会有MINIT、RINIT、SCRIPT、RSHUTDOWN和MSHUTDOWN五个典型阶段，不同的只是各个阶段的执行次数不同。例如在CLI或CGI模式下，这五个阶段顺序执行一次，而在Apache或FastCGI模式下往往一个MINIT和MSHUTDOWN中间对应多个RINIT、SCRIPT、RSHUTDOWN。关于PHP生命周期的话题我回头写文单独研究，这里只是简单说一下。&lt;/p&gt;
&lt;p&gt;MINIT和MSHUTDOWN是PHP Module的初始化和清理阶段，往往在进程起始后和结束前执行，在这两个阶段各个模块的MINIT和MSHUTDOWN方法会被调用。而RINIT、SCRIPT、RSHUTDOWN是每一次请求都会触发的一个小周期。在多线程模式中，PHP的生命周期如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/zend-thread-safety/4.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;在这种模式下，进程启动后仅执行一次MINIT。之所以要强调这一点，是因为TSRM的全局变量资源分配就是在MINIT阶段完成的，后续阶段只获取而不会再请求新的全局变量，这就不难理解为什么在ts_allocate_id中每次id_count加一需要遍历所有线程为每个线程分配相同的资源。到这里，终于可以看清TSRM分配线程全局变量的全貌：&lt;/p&gt;
&lt;p&gt;进程启动后，在MINIT阶段启动TSRM（通过sapi调用tsrm_startup），然后在遍历模块时调用每一个模块的MINIT方法，模块在MINIT中告知TSRM要申请多少全局变量及大小（通过ts_allocate_id），TSRM在内存池中分配并做好登记工作（tsrm_tls_table和resource_types_table），然后将凭证（资源ID）返回给模块，告诉模块以后拿着这个凭证来取你的全局变量。&lt;/p&gt;
&lt;h1&gt;ZTS在单线程和多线程环境的选择性编译&lt;/h1&gt;
&lt;p&gt;上文说过，很多情况下PHP还是被用于单线程环境，这时如果还是遵循上述行为，显然过于折腾。因为在单线程环境下不存在线程安全问题，全局变量只要简单声明使用就好，没必要搞那么一大堆动作。PHP的设计者考虑到的这一点，允许在编译时指定是否开启多线程支持，只有当在configure是指定--enable-maintainer-zts选项或启用多线程sapi时，PHP才会编译线程安全的代码。具体来说，当启用线程安全编译时，一个叫ZTS的常量被定义，PHP代码在每个与线程安全相关的地方通过#ifdef检查是否编译线程安全代码。&lt;/p&gt;
&lt;p&gt;在探究相关细节前我先说一些自己的看法，对于ZTS多线程和单线程环境选择性编译设计上，我个人觉得是非常失败的。因为良好的设计应该隔离变化，换言之ZTS有义务将选择性编译相关的东西隔离起来，而不让其污染到模块的编写，这个机制对模块开发应该是透明的。但是ZTS的设计者仿佛生怕大家不知道有这个东西，让其完全污染了整个PHP，模块开发者不得不面对一堆奇奇怪怪的TSRM宏，着实让人非常不爽。所以下面我就带着悲愤的心情研究一下这块内容。&lt;/p&gt;
&lt;p&gt;为了看看模块是如何实现选择性编译代码的，我们建立一个空的PHP扩展模块。到PHP源码的ext目录下执行如下命令：&lt;/p&gt;
&lt;pre class=&quot;brush:bash&quot;&gt;./ext_skel --extname=zts_research&lt;/pre&gt;
&lt;p&gt;ext_skel是一个脚手架程序，用于创建PHP扩展模块。此时会看到ext目录下多了个zts_research目录。ext_skel为为什么生成了一个模块的架子，并附带了很多提示性注释。在这个目录下找到php_zts_research.h并打开，比较有趣的是一下一段代码：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;/* 
    Declare any global variables you may need between the BEGIN
    and END macros here:     

    ZEND_BEGIN_MODULE_GLOBALS(zts_research)
    long  global_value;
    char *global_string;
    ZEND_END_MODULE_GLOBALS(zts_research)
*/&lt;/pre&gt;
&lt;p&gt;很明显这里提示了定义全局变量的方法：用ZEND_BEGIN_MODULE_GLOBALS和ZEND_END_MODULE_GLOBALS两个宏包住所有全局变量。下面看一下这两个宏，这两个宏定义在Zend/zend_API.h文件里：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;#define ZEND_BEGIN_MODULE_GLOBALS(module_name)     \
typedef struct _zend_##module_name##_globals {
#define ZEND_END_MODULE_GLOBALS(module_name)        \
} zend_##module_name##_globals;&lt;/pre&gt;
&lt;p&gt;原来这两个宏只是将一个模块的所有全局变量封装为一个结构体定义，名称为zend_module_name_globals。关于为什么要封装成结构体，上文有提到。&lt;/p&gt;
&lt;p&gt;php_zts_research.h另外比较有意思的一处就是：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;#ifdef ZTS
#define ZTS_RESEARCH_G(v) TSRMG(zts_research_globals_id, zend_zts_research_globals *, v)
#else
#define ZTS_RESEARCH_G(v) (zts_research_globals.v)
#endif&lt;/pre&gt;
&lt;p&gt;zts_research_globals是zts_research模块全局变量结构的变量名称，类型为zend_module_name_globals，在哪定义的稍后会研究。这里ZTS_RESEARCH_G就是这个模块获取全局变量的宏，如果ZTS没有定义（非线程安全时），就直接从这个结构中获取相应字段，如果线程安全开启时，则使用TSRMG这个宏。&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;#define TSRMG(id, type, element)   (((type) (*((void ***) tsrm_ls))[TSRM_UNSHUFFLE_RSRC_ID(id)])-&gt;element)&lt;/pre&gt;
&lt;p&gt;这个宏就不具体细究了，因为实在太难懂了，基本思想就是使用上文提到的TSRM机制从线程全局变量池中获取对应的数据，其中tsrm_ls可以看作是线程全局变量池的指针，获取变量的凭证就是资源ID。&lt;/p&gt;
&lt;p&gt;看到这里可能还有点晕，例如zts_research_globals这个变量哪来的？zts_research_globals_id又是哪来的？为了弄清这个问题，需要打开ext/zts_research/zts_research.c这个文件，其中有这样的代码：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;/* If you declare any globals in php_zts_research.h uncomment this:
ZEND_DECLARE_MODULE_GLOBALS(zts_research)
*/&lt;/pre&gt;
&lt;p&gt;提示很清楚，如果在php_zts_research.h中定义了任何全局变量则将这段代码的注释消除，看来这个ZEND_DECLARE_MODULE_GLOBALS宏就是关键了。然后在Zend/zend_API中有这样的代码：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;#ifdef ZTS

#define ZEND_DECLARE_MODULE_GLOBALS(module_name)                            \
ts_rsrc_id module_name##_globals_id;

/* code... */

#else

#define ZEND_DECLARE_MODULE_GLOBALS(module_name)                            \
zend_##module_name##_globals module_name##_globals;

/* code... */

#endif&lt;/pre&gt;
&lt;p&gt;当线程安全开启时，这里实际定义了一个整形的资源ID（ts_rsrc_id 被typedef定义为int），而当线程安全不开启时，则直接定义一个结构体。在这个模块中分别对应zts_research_globals_id和zts_research_globals。&lt;/p&gt;
&lt;p&gt;到这里思路基本理顺了：如果ZTS没有被启用，则直接声明一个全局变量结构体，并直接通过存取其字段实现全局变量存取；如果ZTS开启，则定义一个整形变量作为资源ID，然后通过ts_allocate_id函数向TSRM申请一块内存放置结构体（需要程序员手工在MINIT函数中实现，脚手架生成的程序中没有），并通过TSRM存取数据。&lt;/p&gt;
&lt;p&gt;最后一个疑问：tsrm_ls在哪里？如果通过上述方法从TSRM中取数据，那么一定要知道线程全局变量内存池的指针tsrm_ls。这就是我说过的最污染PHP的地方，如果你阅读过PHP源码或编写过模块，对以下四个宏肯定眼熟：TSRMLS_D，TSRMLS_DC，TSRMLS_DTSRMLS_C，TSRMLS_CC。实际在PHP内部每次定义方法或调用方法时都要在参数列表最后加上其中的一个宏，其实就是为了将tsrm_ls传给函数以便存取全局变量，这四个宏的定义如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;#ifdef ZTS

#define TSRMLS_D    void ***tsrm_ls
#define TSRMLS_DC   , TSRMLS_D
#define TSRMLS_C    tsrm_ls
#define TSRMLS_CC   , TSRMLS_C

#else

#define TSRMLS_D    void
#define TSRMLS_DC
#define TSRMLS_C
#define TSRMLS_CC

#endif&lt;/pre&gt;
&lt;p&gt;在没有开启ZTS时，四个宏被定义为空，但这时在定义PHP方法或调用方法时依旧将宏加在参数列表后，这是为了保持代码的一致性，当然，因为在非ZTS环境下根本不会用到tsrm_ls，所以没有任何问题。&lt;/p&gt;
&lt;h1&gt;总结&lt;/h1&gt;
&lt;p&gt;本文研究了PHP和Zend的线程安全模型，应该说我个人觉得Zend内核中ZTS的实现巧妙但不够优雅，但目前在开发PHP模块时总免不了常与之打交道。这块内容相对偏门，几乎没有资料对ZTS和TSRM进行详细解释，但是透彻了解ZTS机制对于在PHP模块开发中正确合理使用全局变量是很重要的。希望本文对读者有所帮助。&lt;/p&gt;
</description>
</item>
<item>
<title>如何使用PHP编写daemon process</title>
<link>http://blog.codinglabs.org/articles/write-daemon-with-php.html</link>
<guid>http://blog.codinglabs.org/articles/write-daemon-with-php.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Mon, 24 Oct 2011 00:00:00 GMT</pubDate>
<description>&lt;p&gt;今天下午在&lt;a href=&quot;http://segmentfault.com&quot;&gt;segmentfault.com&lt;/a&gt;看到一个&lt;a href=&quot;http://segmentfault.com/question/585/php%E6%80%8E%E4%B9%88%E5%81%9A%E6%9C%8D%E5%8A%A1%E5%8C%96&quot; target=&quot;_blank&quot;&gt;提问&lt;/a&gt;，提问标题是“PHP怎么做服务化”，其中问道php是不是只能以web方式调用。其实很多人对PHP的使用场景都有误解，认为php只能用于编写web脚本，实际上，从PHP4开始，php的使用场景早已不限于处理web请求。&lt;/p&gt;
&lt;p&gt;从php的架构体系来说，php分为三个层次：sapi、php core和zend engine。php core本身和web没有任何耦合，php通过sapi与其它应用程序通信，例如mod_php就是为apache编写的sapi实现，同样，fpm是一个基于fastcgi协议的sapi实现，这些sapi都是与web server配合用于处理web请求的。但是也有许多sapi与web无关，例如cli sapi可以使得在命令行环境下直接执行php，embed sapi可以将php嵌入其它语言（如Lua）那样。这里我并不打算详细讨论php的架构体系和sapi的话题，只是说明从架构体系角度目前的php早已被设计为支持各种环境，而非为web独有。&lt;/p&gt;
&lt;p&gt;除了架构体系的支持外，php丰富的扩展模块也为php在不同环境发挥作用提供了后盾，例如本文要提到的&lt;a href=&quot;http://cn.php.net/manual/en/book.pcntl.php&quot; target=&quot;_blank&quot;&gt;pcntl模块&lt;/a&gt;和&lt;a href=&quot;http://cn.php.net/manual/en/book.posix.php&quot; target=&quot;_blank&quot;&gt;posix模块&lt;/a&gt;配合可以实现基本的进程管理、信号处理等操作系统级别的功能，而&lt;a href=&quot;http://cn.php.net/manual/en/book.sockets.php&quot; target=&quot;_blank&quot;&gt;sockets模块&lt;/a&gt;可以使php具有socket通信的能力。因此php完全可以用于编写类似于shell或perl常做的工具性脚本，甚至是具有server性质的daemon process。&lt;/p&gt;
&lt;p&gt;为了展示php如何编写daemon server，我用php编写了一个简单的http server，这个server以daemon process的形式运行。当然，为了把重点放在如何使用php编写daemon，我没有为这个http server实现具体业务逻辑，但它可以监听指定端口，接受http请求并返回给客户端一条固定的文本，整个过程通过socket实现，全部由php编写而成。&lt;/p&gt;
&lt;!--more--&gt;
&lt;h1&gt;代码实例&lt;/h1&gt;
&lt;p&gt;下面是这个程序的完整代码：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;&lt;?php

//Accpet the http client request and generate response content.
//As a demo, this function just send &quot;PHP HTTP Server&quot; to client.
function handle_http_request($address, $port)
{
    $max_backlog = 16;
    $res_content = &quot;HTTP/1.1 200 OK
        Content-Length: 15
        Content-Type: text/plain; charset=UTF-8

        PHP HTTP Server&quot;;
    $res_len = strlen($res_content);

    //Create, bind and listen to socket
    if(($socket = socket_create(AF_INET, SOCK_STREAM, SOL_TCP)) === FALSE)
    {
        echo &quot;Create socket failed!\n&quot;;
        exit;
    }   

    if((socket_bind($socket, $address, $port)) === FALSE)
    {
        echo &quot;Bind socket failed!\n&quot;;
        exit;
    }

    if((socket_listen($socket, $max_backlog)) === FALSE)
    {
        echo &quot;Listen to socket failed!\n&quot;;
        exit;
    }

    //Loop
    while(TRUE)
    {
        if(($accept_socket = socket_accept($socket)) === FALSE)
        {
            continue;
        }
        else
        {
            socket_write($accept_socket, $res_content, $res_len);   
            socket_close($accept_socket);
        }
    }
}

//Run as daemon process.
function run()
{
    if(($pid1 = pcntl_fork()) === 0)
    //First child process
    {
        posix_setsid(); //Set first child process as the session leader.

        if(($pid2 = pcntl_fork()) === 0)
        //Second child process, which run as daemon.
        {
            //Replaced with your own domain or address.
            handle_http_request('www.codinglabs.org', 9999); 
        }
        else
        {
            //First child process exit;
            exit;
        }
    }
    else
    {
        //Wait for first child process exit;
        pcntl_wait($status);
    }
}

//Entry point.
run();

?&gt;&lt;/pre&gt;
&lt;p&gt;这里我假设各位对Unix环境编程都比较了解，所以不做太多细节的解释，只梳理一下。简单来看，这个程序主要由两个部分组成，handle_http_request函数负责处理http请求，其编写方法与用C编写的tcp server类似：创建socket、绑定、监听，然后通过一个循环处理每个connect过来的客户端，一旦accept到一个连接，则输出固定的文本“PHP HTTP Server”（当然http头需要首先构建好），这里没有考虑多路复用和非阻塞等情况，而只是一个简单的同步阻塞tcp server。&lt;/p&gt;
&lt;p&gt;run函数负责将整个程序变为daemon process，方法和Unix环境下C的方法很类似，通过两次fork，第一次fork后调用setsid将子进程1变为session leader，这样就可以让子进程2与其祖先detach，即使祖先进程结束了它也会继续运行（托孤给init进程）。相关细节我不再赘述，对Unix进程相关不熟悉的朋友可以参考《&lt;a href=&quot;http://www.kohala.com/start/apue.html&quot; target=&quot;_blank&quot;&gt;Advanced Programming in the UNIX Environment&lt;/a&gt;》一书。&lt;/p&gt;
&lt;p&gt;注意，在这里&lt;a href=&quot;http://cn.php.net/manual/en/function.pcntl-fork.php&quot; target=&quot;_blank&quot;&gt;pcntl_fork&lt;/a&gt;对应Unix中的fork，&lt;a href=&quot;http://cn.php.net/manual/en/function.pcntl-wait.php&quot; target=&quot;_blank&quot;&gt;pcntl_wait&lt;/a&gt;对应wait，而&lt;a href=&quot;http://cn.php.net/manual/en/function.posix-setsid.php&quot; target=&quot;_blank&quot;&gt;posix_setsid&lt;/a&gt;对应setsid，更多函数可以参考PHP Manual中的pcntl和fork模块相关内容。&lt;/p&gt;
&lt;h1&gt;检验&lt;/h1&gt;
&lt;p&gt;下面在命令行下启动这个脚本：&lt;/p&gt;&lt;pre class=&quot;prettyprint&quot;&gt;php httpserver.php&lt;/pre&gt;
&lt;p&gt;用ps命令可以看到我们已经启动了一个daemon进程：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/write-daemon-with-php/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;这里我绑定的是我博客的域名www.codinglabs.org，端口是9999，可以按需要进行修改。&lt;/p&gt;
&lt;p&gt;下面我先用curl命令看下这个http server是否正常运行：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/write-daemon-with-php/2.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;看来是没问题，再到浏览器中看一下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/write-daemon-with-php/3.png&quot;/&gt;&lt;/p&gt;
&lt;h1&gt;结语&lt;/h1&gt;
&lt;p&gt;当然，这个程序算不上真正的http server，即使作为一个daemon process，也是不完善的，很多必要的事情如修改执行目录（php中可以通过chroot实现）、信号绑定、日志功能等等都没有去做，不过作为一个demo，它已经足够说明php不只是可以编写动态网页处理脚本。如果有的朋友有兴趣，可以使用php将我上面说的功能为这个的http server加上。&lt;/p&gt;
&lt;p&gt;还有一点要说明的是，pcntl和sockets模块默认是不安装的，如果在安装php时没有通过参数指定安装，则需要单独安装这两个扩展模块。&lt;/p&gt;
</description>
</item>
<item>
<title>PHP Extension开发基础</title>
<link>http://blog.codinglabs.org/articles/php-extension-dev-guide.html</link>
<guid>http://blog.codinglabs.org/articles/php-extension-dev-guide.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Fri, 21 Oct 2011 00:00:00 GMT</pubDate>
<description>&lt;p&gt;PHP是当前应用非常广泛的一门语言，从国外的Facebook、Twitter到国内的淘宝、腾讯、百度再到互联网上林林总总的各种大中小型网站都能见到它的身影。PHP的成功，应该说很大程度上依赖于其开放的扩展API机制和丰富的扩展组件（PHP Extension），正是这些扩展组件使得PHP从各种数据库操作到XML、JSON、加密、文件处理、图形处理、Socket等领域无所不能。有时候开发人员可能需要开发自己的PHP扩展，当前PHP5的扩展机制是基于Zend API的，Zend API提供了丰富的接口和宏定义，加上一些实用工具，使得PHP扩展开发起来难度并不算特别大。本文将介绍关于PHP扩展组件开发的基本知识，并通过一个实例展示开发PHP扩展的基本过程。&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;PHP扩展组件的开发过程在Unix和Windows环境下有所不同，但基本是互通的，本文将基于Unix环境（具体使用Linux）。阅读本文需要简单了解Unix环境、PHP和C语言的一些基础知识，只要简单了解就行，我会尽量不涉及太过具体的操作系统和语言特性，并在必要的地方加以解释，以便读者阅读。&lt;/p&gt;
&lt;p&gt;本文的具体开发环境为Ubuntu 10.04 + PHP 5.3.3。&lt;/p&gt;
&lt;h1&gt;下载PHP源代码&lt;/h1&gt;
&lt;p&gt;要开发PHP扩展，第一步要下载PHP源代码，因为里面有开发扩展需要的工具。我下载的是PHP最新版本5.3.3，格式为tar.bz2压缩包。下载地址为：&lt;a href=&quot;http://cn.php.net/get/php-5.3.3.tar.bz2/from/a/mirror&quot;&gt;http://cn.php.net/get/php-5.3.3.tar.bz2/from/a/mirror&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;下载后，将源代码移动到合适的目录并解压。解压命令为：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;tar -jxvf 源码包名称&lt;/pre&gt;
&lt;p&gt;若下载的是tar.gz压缩包，解压命令为&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;tar -zxvf 源码包名称&lt;/pre&gt;
&lt;p&gt;解压后，在源代码目录中有个ext目录，这里便是和PHP扩展有关的目录。进入目录后用ls查看，可以看到许多已经存在的扩展。下图是在我的环境下查看的结果：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/php-extension-dev-guide/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;其中蓝色的均是扩展包目录，其中可以看到我们很熟悉的mysql、iconv和gd等等。而ext_skel是Unix环境下用于自动生成PHP扩展框架的脚本工具，后面我们马上会用到，ext_skel_win32.php是windows下对应的脚本。&lt;/p&gt;
&lt;h1&gt;开发自己的PHP扩展——say_hello&lt;/h1&gt;
&lt;p&gt;下面我们开发一个PHP扩展：say_hello。这个扩展很简单，只是接受一个字符串参数，然后输出“Hello xxx!”。这个例子只是为了介绍PHP扩展组件的开发流程，不承担实际功能。&lt;/p&gt;
&lt;h2&gt;生成扩展组件框架&lt;/h2&gt;
&lt;p&gt;PHP的扩展组件开发目录和文件是有固定组织结构的，你可以随便进入一个已有扩展组件目录，查看其所有文件，我想你一定眼花缭乱了。当然你可以选择手工完成框架的搭建，不过我相信你更希望有什么东西来帮你完成。上文提到的ext_skel脚本就是用来自动构建扩展包框架的工具。ext_skel的完整命令为：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;ext_skel --extname=module [--proto=file] [--stubs=file] [--xml[=file]] [--skel=dir] [--full-xml] [--no-help]&lt;/pre&gt;
&lt;p&gt;作为初学者，我们不必了解所有命令参数，实际上，大多数情况下只需要提供第一个参数就可以了，也就是扩展模块的名字。因此，我们在ext目录中键入如下命令：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;./ext_skel --extname=say_hello&lt;/pre&gt;
&lt;p&gt;（如果你希望详细了解ext_skel的各项命令参数，请&lt;a href=&quot;http://www.php.net/manual/en/internals2.buildsys.skeleton.php&quot; target=&quot;_blank&quot;&gt;参考这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;这时再用ls查看，会发现多了一个“say_hello”目录，进入这个目录，会发现ext_skel已经为我们建立好了say_hello的基本框架，如下图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/php-extension-dev-guide/2.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;如果你懒得弄清楚PHP扩展包目录结构的全部内容，那么里面有三个文件你必须注意：&lt;/p&gt;
&lt;p&gt;config.m4：这是Unix环境下的Build System配置文件，后面将会通过它生成配置和安装。&lt;/p&gt;
&lt;p&gt;php_say_hello.h：这个文件是扩展模块的头文件。遵循C语言一贯的作风，这个里面可以放置一些自定义的结构体、全局变量等等。&lt;/p&gt;
&lt;p&gt;say_hello.c：这个就是扩展模块的主程序文件了，最终的扩展模块各个函数入口都在这里。当然，你可以将所有程序代码都塞到这里面，也可以遵循模块化思想，将各个功能模块放到不同文件中。&lt;/p&gt;
&lt;p&gt;下面的内容主要围绕这三个文件展开。&lt;/p&gt;
&lt;h2&gt;Unix Build System配置&lt;/h2&gt;
&lt;p&gt;开发PHP扩展组件的第一步不是写实现代码，而是要先配置好Build System选项。由于我们是在Linux下开发，所以这里的配置主要与config.m4有关。&lt;/p&gt;
&lt;p&gt;关于Build System配置这一块，要是写起来能写一大堆，而且与Unix系统很多东西相关，就算我有兴趣写估计大家也没兴趣看，所以这里我们从略，只拣关键地方说一下，关于config.m4更多细节可以&lt;a href=&quot;http://www.php.net/manual/en/internals2.buildsys.configunix.php&quot; target=&quot;_blank&quot;&gt;参考这里&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;打开生成的config.m4文件，内容大致如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;dnl $Id$
dnl config.m4 for extension say_hello
dnl Comments in this file start with the string 'dnl'.
dnl Remove where necessary. This file will not work
dnl without editing.

dnl If your extension references something external, use with:
dnl PHP_ARG_WITH(say_hello, for say_hello support,
dnl Make sure that the comment is aligned:
dnl [ --with-say_hello Include say_hello support])

dnl Otherwise use enable:
dnl PHP_ARG_ENABLE(say_hello, whether to enable say_hello support,
dnl Make sure that the comment is aligned:
dnl [ --enable-say_hello Enable say_hello support])

if test &quot;$PHP_SAY_HELLO&quot; != &quot;no&quot;; then
dnl Write more examples of tests here...
dnl # --with-say_hello -&gt; check with-path
dnl SEARCH_PATH=&quot;/usr/local /usr&quot; # you might want to change this
dnl SEARCH_FOR=&quot;/include/say_hello.h&quot; # you most likely want to change this
dnl if test -r $PHP_SAY_HELLO/$SEARCH_FOR; then # path given as parameter
dnl SAY_HELLO_DIR=$PHP_SAY_HELLO
dnl else # search default path list
dnl AC_MSG_CHECKING([for say_hello files in default path])
dnl for i in $SEARCH_PATH ; do
dnl if test -r $i/$SEARCH_FOR; then
dnl SAY_HELLO_DIR=$i
dnl AC_MSG_RESULT(found in $i)
dnl fi
dnl done
dnl fi
dnl
dnl if test -z &quot;$SAY_HELLO_DIR&quot;; then
dnl AC_MSG_RESULT([not found])
dnl AC_MSG_ERROR([Please reinstall the say_hello distribution])
dnl fi
dnl # --with-say_hello -&gt; add include path
dnl PHP_ADD_INCLUDE($SAY_HELLO_DIR/include)
dnl # --with-say_hello -&gt; check for lib and symbol presence
dnl LIBNAME=say_hello # you may want to change this
dnl LIBSYMBOL=say_hello # you most likely want to change this
dnl PHP_CHECK_LIBRARY($LIBNAME,$LIBSYMBOL,
dnl [
dnl PHP_ADD_LIBRARY_WITH_PATH($LIBNAME, $SAY_HELLO_DIR/lib, SAY_HELLO_SHARED_LIBADD)
dnl AC_DEFINE(HAVE_SAY_HELLOLIB,1,[ ])
dnl ],[
dnl AC_MSG_ERROR([wrong say_hello lib version or lib not found])
dnl ],[
dnl -L$SAY_HELLO_DIR/lib -lm
dnl ])
dnl
dnl PHP_SUBST(SAY_HELLO_SHARED_LIBADD)
PHP_NEW_EXTENSION(say_hello, say_hello.c, $ext_shared)
fi
&lt;/pre&gt;
&lt;p&gt;不要看这么多，因为所有以“dnl”开头的全是注释，所以真正起作用没几行。这里需要配置的只有下面几行：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;dnl If your extension references something external, use with:
dnl PHP_ARG_WITH(say_hello, for say_hello support,
dnl Make sure that the comment is aligned:
dnl [ --with-say_hello Include say_hello support])

dnl Otherwise use enable:
dnl PHP_ARG_ENABLE(say_hello, whether to enable say_hello support,
dnl Make sure that the comment is aligned:
dnl [ --enable-say_hello Enable say_hello support])&lt;/pre&gt;
&lt;p&gt;我想大家也都能看明白，意思就是“如果你的扩展引用了外部组件，使用…，否则使用…”。我们的say_hello扩展并没有引用外部组件，所以将“Otherwise use enable”下面三行的“dnl”去掉，改为：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;dnl Otherwise use enable:
PHP_ARG_ENABLE(say_hello, whether to enable say_hello support,
Make sure that the comment is aligned:
[ --enable-say_hello Enable say_hello support])&lt;/pre&gt;
&lt;p&gt;保存，这样关于Build System配置就大功告成了。&lt;/p&gt;
&lt;h2&gt;PHP Extension及Zend_Module结构分析&lt;/h2&gt;
&lt;p&gt;以上可以看成是为开发PHP扩展而做的准备工作，下面就要编写核心代码了。上文说过，编写PHP扩展是基于Zend API和一些宏的，所以如果要编写核心代码，我们首先要弄清楚PHP Extension的结构。因为一个PHP Extension在C语言层面实际上就是一个zend_module_entry结构体，这点可以从“php_say_hello.h”中得到证实。打开“php_say_hello.h”，会看到里面有怎么一行：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;extern zend_module_entry say_hello_module_entry;&lt;/pre&gt;
&lt;p&gt;say_hello_module_entry就是say_hello扩展的C语言对应元素，而关于其类型zend_module_entry的定义可以在PHP源代码的“Zend/zend_modules.h”文件里找到，下面代码是zend_module_entry的定义：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;typedef struct _zend_module_entry zend_module_entry;

struct _zend_module_entry {
    unsigned short size;
    unsigned int zend_api;
    unsigned char zend_debug;
    unsigned char zts;
    const struct _zend_ini_entry *ini_entry;
    const struct _zend_module_dep *deps;
    const char *name;
    const struct _zend_function_entry *functions;
    int (*module_startup_func)(INIT_FUNC_ARGS);
    int (*module_shutdown_func)(SHUTDOWN_FUNC_ARGS);
    int (*request_startup_func)(INIT_FUNC_ARGS);
    int (*request_shutdown_func)(SHUTDOWN_FUNC_ARGS);
    void (*info_func)(ZEND_MODULE_INFO_FUNC_ARGS);
    const char *version;
    size_t globals_size;

    #ifdef ZTS
    ts_rsrc_id* globals_id_ptr;
    #else
    void* globals_ptr;
    #endif

    void (*globals_ctor)(void *global TSRMLS_DC);
    void (*globals_dtor)(void *global TSRMLS_DC);
    int (*post_deactivate_func)(void);
    int module_started;
    unsigned char type;
    void *handle;
    int module_number;
    char *build_id;
};&lt;/pre&gt;
&lt;p&gt;这个结构体可能看起来会让人有点头疼，不过我还是要解释一下里面的内容。因为这就是PHP Extension的原型，如果不搞清楚，就没法开发PHP Extension了。当然，我就不一一对每个字段进行解释了，只拣关键的、这篇文章会用到的字段说，因为许多字段并不需要我们手工填写，而是可以使用某些预定义的宏填充。&lt;/p&gt;
&lt;p&gt;第7个字段“name”，这个字段是此PHP Extension的名字，在本例中就是“say_hello”。&lt;/p&gt;
&lt;p&gt;第8个字段“functions”，这个将存放我们在此扩展中定义的函数的引用，具体结构不再分析，有兴趣的朋友可以阅读_zend_function_entry的源代码。具体编写代码时这里会有相应的宏。&lt;/p&gt;
&lt;p&gt;第9-12个字段分别是四个函数指针，这四个函数会在相应时机被调用，分别是“扩展模块加载时”、“扩展模块卸载时”、“每个请求开始时”和“每个请求结束时”。这四个函数可以看成是一种拦截机制，主要用于相应时机的资源分配、释放等相关操作。&lt;/p&gt;
&lt;p&gt;第13个字段“info_func”也是一个函数指针，这个指针指向的函数会在执行phpinfo()时被调用，用于显示自定义模块信息。&lt;/p&gt;
&lt;p&gt;第14个字段“version”是模块的版本。&lt;/p&gt;
&lt;p&gt;（关于zend_module_entry更详尽的介绍请&lt;a href=&quot;http://www.php.net/manual/en/internals2.structure.modstruct.php&quot; target=&quot;_blank&quot;&gt;参考这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;介绍完以上字段，我们可以看看“say_hello.c”中自动生成的“say_hello_module_entry”框架代码了。&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;/* {{{ say_hello_module_entry
*/
zend_module_entry say_hello_module_entry = {
    #if ZEND_MODULE_API_NO &gt;= 20010901
    STANDARD_MODULE_HEADER,
    #endif
    &quot;say_hello&quot;,
    say_hello_functions,
    PHP_MINIT(say_hello),
    PHP_MSHUTDOWN(say_hello),
    PHP_RINIT(say_hello), /* Replace with NULL if there's nothing to do at request start */
    PHP_RSHUTDOWN(say_hello), /* Replace with NULL if there's nothing to do at request end */
    PHP_MINFO(say_hello),
    #if ZEND_MODULE_API_NO &gt;= 20010901
    &quot;0.1&quot;, /* Replace with version number for your extension */
    #endif
    STANDARD_MODULE_PROPERTIES
};
/* }}} */&lt;/pre&gt;
&lt;p&gt;首先，宏“STANDARD_MODULE_HEADER”会生成前6个字段，“STANDARD_MODULE_PROPERTIES ”会生成“version”后的字段，所以现在我们还不用操心。而我们关心的几个字段，也都填写好或由宏生成好了，并且在“say_hello.c”的相应位置也生成了几个函数的框架。这里要注意，几个宏的参数均为“say_hello”，但这并不表示几个函数的名字全为“say_hello”，C语言中也不可能存在函数名重载机制。实际上，在开发PHP Extension的过程中，几乎处处都要用到Zend里预定义的各种宏，从全局变量到函数的定义甚至返回值，都不能按照“裸写”的方式来编写C语言，这是因为PHP的运行机制可能会导致命名冲突等问题，而这些宏会将函数等元素变换成一个内部名称，但这些对程序员都是透明的（除非你去阅读那些宏的代码），我们通过各种宏进行编程，而宏则为我们处理很多内部的东西。&lt;/p&gt;
&lt;p&gt;写到这里，我们的任务就明了了：第一，如果需要在相应时机处理一些东西，那么需要填充各个拦截函数内容；第二，编写say_hello的功能函数，并将引用添加到say_hello_functions中。&lt;/p&gt;
&lt;h2&gt;编写phpinfo()回调函数&lt;/h2&gt;
&lt;p&gt;因为say_hello扩展在各个生命周期阶段并不需要做操作，所以我们只编写info_func的内容，上文说过，这个函数将在phpinfo()执行时被自动调用，用于显示扩展的信息。编写这个函数会用到四个函数：&lt;/p&gt;
&lt;p&gt;php_info_print_table_start()——开始phpinfo表格。无参数。&lt;/p&gt;
&lt;p&gt;php_info_print_table_header()——输出表格头。第一个参数是整形，指明头的列数，然后后面的参数是与列数等量的(char*)类型参数用于指定显示的文字。&lt;/p&gt;
&lt;p&gt;php_info_print_table_row()——输出表格内容。第一个参数是整形，指明这一行的列数，然后后面的参数是与列数等量的(char*)类型参数用于指定显示的文字。&lt;/p&gt;
&lt;p&gt;php_info_print_table_end()——结束phpinfo表格。无参数。&lt;/p&gt;
&lt;p&gt;下面是“say_hello.c”中需要编写的info_func的具体代码：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;/* {{{ PHP_MINFO_FUNCTION
*/
PHP_MINFO_FUNCTION(say_hello)
{
    php_info_print_table_start();
    php_info_print_table_header(2, &quot;say_hello support&quot;, &quot;enabled&quot;);
    php_info_print_table_row(2, &quot;author&quot;, &quot;Zhang Yang&quot;); /* Replace with your name */
    php_info_print_table_end();
    /* Remove comments if you have entries in php.ini
    DISPLAY_INI_ENTRIES();
    */
}
/* }}} */&lt;/pre&gt;
&lt;p&gt;可以看到我们编写了两行内容、组件是否可用以及作者信息。&lt;/p&gt;
&lt;h2&gt;编写核心函数&lt;/h2&gt;
&lt;p&gt;编写核心函数，总共分为三步：1、使用宏PHP_FUNCTION定义函数体；2、使用宏ZEND_BEGIN_ARG_INFO和ZEND_END_ARG_INFO定义参数信息；3、使用宏PHP_FE将函数加入到say_hello_functions中。下面分步说明。&lt;/p&gt;
&lt;h3&gt;使用宏PHP_FUNCTION定义函数体&lt;/h3&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;PHP_FUNCTION(say_hello_func)
{
    char *name;
    int name_len;
    if (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, &quot;s&quot;, &amp;name, &amp;name_len) == FAILURE)
    {
        return;
    }
    php_printf(&quot;Hello %s!&quot;, name);

    RETURN_TRUE;
}&lt;/pre&gt;
&lt;p&gt;上文说过，编写PHP扩展时几乎所有东西都不能裸写，而是必须使用相应的宏。从上面代码可以清楚看到这一点。总体来说，核心函数代码一般由如下几部分构成：&lt;/p&gt;
&lt;p&gt;定义函数，这一步通过宏PHP_FUNCTION实现，函数的外部名称就是宏后面括号里面的名称。&lt;/p&gt;
&lt;p&gt;声明并定义局部变量。&lt;/p&gt;
&lt;p&gt;解析参数，这一步通过zend_parse_parameters函数实现，这个函数的作用是从函数用户的输入栈中读取数据，然后转换成相应的函数参数填入变量以供后面核心功能代码使用。zend_parse_parameters的第一个参数是用户传入参数的个数，可以由宏“ZEND_NUM_ARGS() TSRMLS_CC”生成；第二个参数是一个字符串，其中每个字母代表一个变量类型，我们只有一个字符串型变量，所以第二个参数是“s”；最后各个参数需要一些必要的局部变量指针用于存储数据，下表给出了不同变量类型的字母代表及其所需要的局部变量指针。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/php-extension-dev-guide/3.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;参数解析完成后就是核心功能代码，我们这里只是输出一行字符，php_printf是Zend版本的printf。&lt;/p&gt;
&lt;p&gt;最后的返回值也是通过宏实现的。RETURN_TRUE宏是返回布尔值“true”。&lt;/p&gt;
&lt;h3&gt;使用宏ZEND_BEGIN_ARG_INFO和ZEND_END_ARG_INFO定义参数信息&lt;/h3&gt;
&lt;p&gt;参数信息是函数所必要部分，这里不做深究，直接给出相应代码：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;ZEND_BEGIN_ARG_INFO(arginfo_say_hello_func, 0) ZEND_END_ARG_INFO()&lt;/pre&gt;
&lt;p&gt;如需了解具体信息请阅读相关宏定义。&lt;/p&gt;
&lt;h3&gt;使用宏PHP_FE将函数加入到say_hello_functions中&lt;/h3&gt;
&lt;p&gt;最后，我们需要将刚才定义的函数和参数信息加入到say_hello_functions数组里，代码如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;const zend_function_entry say_hello_functions[] = {
PHP_FE(say_hello_func, arginfo_say_hello_func)
    {NULL, NULL, NULL}
};&lt;/pre&gt;
&lt;p&gt;这一步就是通过PHP_EF宏实现，注意这个数组最后一行必须是{NULL, NULL, NULL} ，请不要删除。&lt;/p&gt;
&lt;p&gt;下面是编写完成后的say_hello.c全部代码：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;/*
+----------------------------------------------------------------------+
| PHP Version 5                                                        |
+----------------------------------------------------------------------+
| Copyright (c) 1997-2010 The PHP Group                                |
+----------------------------------------------------------------------+
| This source file is subject to version 3.01 of the PHP license,      |
| that is bundled with this package in the file LICENSE, and is        |
| available through the world-wide-web at the following url:           |
| http://www.php.net/license/3_01.txt                                  |
| If you did not receive a copy of the PHP license and are unable to   |
| obtain it through the world-wide-web, please send a note to          |
| license@php.net so we can mail you a copy immediately.               |
+----------------------------------------------------------------------+
| Author: ZhangYang &lt;ericzhang.buaa@gmail.com&gt;                         |
+----------------------------------------------------------------------+
*/

/* $Id: header 297205 2010-03-30 21:09:07Z johannes $ */

#ifdef HAVE_CONFIG_H
#include &quot;config.h&quot;
#endif

#include &quot;php.h&quot;
#include &quot;php_ini.h&quot;
#include &quot;ext/standard/info.h&quot;

#include &quot;php_say_hello.h&quot;

/* If you declare any globals in php_say_hello.h uncomment this:
ZEND_DECLARE_MODULE_GLOBALS(say_hello)
*/

/* True global resources - no need for thread safety here */
static int le_say_hello;

/* {{{ PHP_FUNCTION
*/
PHP_FUNCTION(say_hello_func)
{
    char *name;
    int name_len;

    if (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, &quot;s&quot;, &amp;name, &amp;name_len) == FAILURE)
    {
        return;
    }

    php_printf(&quot;Hello %s!&quot;, name);
    RETURN_TRUE;
}

ZEND_BEGIN_ARG_INFO(arginfo_say_hello_func, 0)
ZEND_END_ARG_INFO()
/* }}} */

/* {{{ say_hello_functions[]
*
* Every user visible function must have an entry in say_hello_functions[].
*/
const zend_function_entry say_hello_functions[] = {
    PHP_FE(say_hello_func, arginfo_say_hello_func)
    {NULL, NULL, NULL} /* Must be the last line in say_hello_functions[] */
};
/* }}} */

/* {{{ say_hello_module_entry
*/
zend_module_entry say_hello_module_entry = {
    #if ZEND_MODULE_API_NO &gt;= 20010901
    STANDARD_MODULE_HEADER,
    #endif
    &quot;say_hello&quot;,
    say_hello_functions,
    NULL,
    NULL,
    NULL,
    NULL,
    PHP_MINFO(say_hello),

    #if ZEND_MODULE_API_NO &gt;= 20010901
    &quot;0.1&quot;, /* Replace with version number for your extension */
    #endif

    STANDARD_MODULE_PROPERTIES
};
/* }}} */

#ifdef COMPILE_DL_SAY_HELLO
ZEND_GET_MODULE(say_hello)
#endif

/* {{{ PHP_MINFO_FUNCTION
*/
PHP_MINFO_FUNCTION(say_hello)
{
    php_info_print_table_start();
    php_info_print_table_header(2, &quot;say_hello support&quot;, &quot;enabled&quot;);
    php_info_print_table_row(2, &quot;author&quot;, &quot;Zhang Yang&quot;); /* Replace with your name */
    php_info_print_table_end();

    /* Remove comments if you have entries in php.ini
    DISPLAY_INI_ENTRIES();
    */
}
/* }}} */&lt;/pre&gt;
&lt;h2&gt;编译并安装扩展&lt;/h2&gt;
&lt;p&gt;在say_hello目录下输入下面命令：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;/usr/bin/phpize
./configure
make
make install&lt;/pre&gt;
&lt;p&gt;这样就完成了say_hello扩展的安装（如果没有报错的话）。&lt;/p&gt;
&lt;p&gt;这时如果你去放置php扩展的目录下，会发现多了一个say_hello.so的文件。如下图所示：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/php-extension-dev-guide/4.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;下面就是将其加入到php.ini配置中，然后重启Apache（如果需要的话）。这些都是PHP基本配置的内容，我就不详述了。&lt;/p&gt;
&lt;h2&gt;扩展测试&lt;/h2&gt;
&lt;p&gt;如果上面顺利完成，这时运行phpinfo()，应该能看到如下信息：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/php-extension-dev-guide/5.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;这说明扩展已经安装成功了。然后我们编写一个测试用PHP脚本：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;&lt;?php say_hello_func('Zhang Yang'); ?&gt;;&lt;/pre&gt;
&lt;p&gt;执行这个脚本，结果如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/php-extension-dev-guide/6.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;说明扩展已经正常工作了。&lt;/p&gt;
&lt;h1&gt;总结&lt;/h1&gt;
&lt;p&gt;这篇文章主要用示例方法介绍PHP Extension的开发基础。在PHP的使用中，也许是因为需要支持新的组件（如新的数据库），又或是业务需要或性能需要，几乎都会遇到需要开发PHP扩展的地方。后续如果有机会，我会写文章介绍一些关于扩展开发较为深入的东西，如扩展模块生命周期、INI使用以及编写面向对象的扩展模块等等。&lt;/p&gt;
</description>
</item>
<item>
<title>使用memc-nginx和srcache-nginx模块构建高效透明的缓存机制</title>
<link>http://blog.codinglabs.org/articles/nginx-memc-and-srcache.html</link>
<guid>http://blog.codinglabs.org/articles/nginx-memc-and-srcache.html</guid>
<author>ericzhang.buaa@gmail.com 张洋</author>
<pubDate>Tue, 18 Oct 2011 00:00:00 GMT</pubDate>
<description>&lt;p&gt;为了提高性能，几乎所有互联网应用都有缓存机制，其中&lt;a href=&quot;http://memcached.org&quot; target=&quot;_blank&quot;&gt;Memcache&lt;/a&gt;是使用非常广泛的一个分布式缓存系统。众所周知，LAMP是非常经典的Web架构方式，但是随着&lt;a href=&quot;http://wiki.nginx.org/&quot; target=&quot;_blank&quot;&gt;Nginx&lt;/a&gt;的成熟，越来越多的系统开始转型为LNMP（Linux+Nginx+MySQL+PHP with fpm），这是因为Nginx采用基于事件机制的I/O多路复用思想设计，在高并发情况下其性能远远优于默认采用prefork模式的Apache，另外，相对于Apache，Nginx更轻量，同时拥有大量优秀的扩展模块，使得在Nginx上可以实现一些美妙的功能。&lt;/p&gt;
&lt;p&gt;传统上，PHP中使用memcache的方法是使用&lt;a href=&quot;http://pecl.php.net/package/memcache&quot; target=&quot;_blank&quot;&gt;php-memcache&lt;/a&gt;或&lt;a href=&quot;http://pecl.php.net/package/memcached&quot; target=&quot;_blank&quot;&gt;php-memached&lt;/a&gt;扩展操作memcache，然而在Nginx上有构建更高效缓存机制的方法，本文将首先介绍这种机制，然后介绍具体的操作步骤方法，最后将对这种机制和传统的PHP操作memcache的性能进行一个benchmark。
&lt;!--more--&gt;
&lt;/p&gt;
&lt;h1&gt;Nginx的Memc和SR Cache模块&lt;/h1&gt;
&lt;h2&gt;缓存策略的改进&lt;/h2&gt;
&lt;p&gt;我们知道，Nginx的核心设计思想是事件驱动的非阻塞I/O。Nginx被设计为可以配置I/O多路复用策略，在Unix系统中传统的多路复用是采用select或poll，但是这两个方法的问题是随着监听socket的增加，性能会下降，因为在linux内核中是采用轮询的方式判断是否可以触发事件，换句话说算法的复杂度为O(N)，而在较新的linux内核中引入了复杂度为O(1)的epoll，因此Nginx在Linux下默认采用epoll，而在FreeBSD下默认采用kqueue作为I/O策略。&lt;/p&gt;
&lt;p&gt;即便是这样，传统的缓存策略仍可能造成效率低下，因为传统上是通过PHP操作memcache的，要执行PHP代码，Nginx就必然要和FastCGI通信，同时也要进入PHP的生命周期，因此SAPI、PHP Core和Zend Engine的一系列逻辑会被执行。&lt;strike&gt;更糟糕的是，fpm和PHP可能会阻塞，因此破坏了Nginx的非阻塞性。&lt;/strike&gt;（原文中此处表述有误，fastcgi与nginx进行同步通信，但并不会破坏nginx i/o的非阻塞性，多谢&lt;a target=&quot;_blank&quot; href=&quot;http://agentzh.org&quot;&gt;agentzh&lt;/a&gt;给予指正）下图展示了在memcache命中时整个处理过程。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/nginx-memc-and-srcache/1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;可以看到，即使memcache命中，还是要进入PHP的生命周期。我们知道，目前很多互联网应用都使用RESTful规范进行设计，在RESTful应用下，普遍使用uri和查询参数作为缓存的key，因此一种更高效的缓存策略是Nginx直接访问memcache，并用$uri和$args等Nginx内置变量设定缓存key规则，这样，当缓存命中时，Nginx可以跳过通过fastcgi和PHP通信的过程，直接从memcache中获取数据并返回。memc-nginx和srcache-nginx正是利用这种策略提高了缓存的效率。下图是这种高效缓存策略的示意图（当memcache命中时）。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/nginx-memc-and-srcache/2.png&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;模块介绍&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://wiki.nginx.org/HttpMemcModule&quot; target=&quot;_blank&quot;&gt;memc-nginx&lt;/a&gt;和&lt;a href=&quot;http://wiki.nginx.org/HttpSRCacheModule&quot; target=&quot;_blank&quot;&gt;srcache-nginx&lt;/a&gt;模块均为前淘宝工程师agentzh（章亦春）开发。其中memc模块扩展了Nginx标准的memcache模块，增加了set、add、delete等memcache命令，而srcache则是为location增加了透明的基于subrequest的缓存层。两者配合使用，可以实现上一节提到的高效缓存机制。关于两个模块的详细信息可以参考它们Nginx官网的wiki（&lt;a href=&quot;http://wiki.nginx.org/HttpMemcModule&quot; target=&quot;_blank&quot;&gt;memc wiki&lt;/a&gt;，&lt;a href=&quot;http://wiki.nginx.org/HttpSRCacheModule&quot; target=&quot;_blank&quot;&gt;srcache wiki&lt;/a&gt;）页。&lt;/p&gt;
&lt;h2&gt;安装及配置&lt;/h2&gt;
&lt;p&gt;下面以LNMP环境介绍如何使用这两个模块构建缓存层。&lt;/p&gt;
&lt;p&gt;因为Nginx并不支持模块动态加载，所以要安装新的模块，必须重新编译Nginx。首先下载两个模块（&lt;a href=&quot;https://github.com/agentzh/memc-nginx-module/downloads&quot; target=&quot;_blank&quot;&gt;memc下载地址&lt;/a&gt;，&lt;a href=&quot;https://github.com/agentzh/srcache-nginx-module/downloads&quot; target=&quot;_blank&quot;&gt;srcache下载地址&lt;/a&gt;），另外，为了发挥出缓存的最大性能，建议将memcache的upstream配置为keep-alive，为了支持upstream的keep-alive需要同时安装&lt;a href=&quot;http://wiki.nginx.org/HttpUpstreamKeepaliveModule&quot; target=&quot;_blank&quot;&gt;http-upstream-keepalive-module&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;将模块下载并解压到合适的目录，这里我Nginx使用的版本是1.0.4，与相关模块一起解压到了/home/zhangyang/downloads，如下图所示。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/nginx-memc-and-srcache/3.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;其中红框框起来的是我们需要用到的模块。进入nginx目录，执行下列命令：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;./configure --prefix=/usr/local/nginx \
--add-module=../memc-nginx-module \
--add-module=../srcache-nginx-module \
--add-module=../ngx_http_upstream_keepalive
make
make install&lt;/pre&gt;
&lt;p&gt;这里我将nginx安装到/usr/local/nginx下，你可以根据自己的需要更改安装路径，另外，我只列出了本文必要的configure命令，你也可以增加需要的configure选项。&lt;/p&gt;
&lt;p&gt;然后需要对nginx进行配置，nginx默认主配置文件放在安装目录的conf下，例如我的主配置文件为/usr/local/nginx/conf/nginx.conf。&lt;/p&gt;
&lt;p&gt;这里我只贴出相关的配置：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;#Memcache服务upstream
upstream memcache {
    server localhost:11211;
    keepalive 512 single;
}
server {
    listen       80;
    server_name  localhost;
    #memc-nginx-module
    location /memc {
        internal;
        memc_connect_timeout 100ms;
        memc_send_timeout 100ms;
        memc_read_timeout 100ms;
        set $memc_key $query_string;
        set $memc_exptime 300;
        memc_pass memcache;
    }
    location / {
        root   /var/www;
        index  index.html index.htm index.php;
    }
    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
    #
    location ~ \.php$ {
        charset        utf-8;
        default_type   text/html;
        #srcache-nginx-module
        set $key $uri$args;
        srcache_fetch GET /memc $key;
        srcache_store PUT /memc $key;
        root           /var/www;
        fastcgi_pass   127.0.0.1:9000;
        fastcgi_index  index.php;
        include        fastcgi_params;
        fastcgi_param  SCRIPT_FILENAME $document_root$fastcgi_script_name;
    }
}&lt;/pre&gt;
&lt;p&gt;下面解释一下其中几个点。&lt;/p&gt;
&lt;p&gt;上文说过，memc-nginx是一个标准的upstream模块，因此首先需要定义memcache的upstream。这里我在本机上启动了一个memcache服务，端口为默认的11211，keepalive指令是http-upsteram-keepalive-module提供的功能，这里我们最大保持512个不立即关闭的连接用于提升性能。&lt;/p&gt;
&lt;p&gt;下面是为memc-nginx-module配置location，我们配置为/memc，所有请求都通过请求这个location来操作memcache，memc-nginx-module存取memcache是基于http method语义的，使用http的GET方法表示get、PUT方法表示set、DELETE方法表示delete。这里我们将/memc设为&lt;a href=&quot;http://wiki.nginx.org/NginxHttpCoreModule#internal&quot; target=&quot;_blank&quot;&gt;internal&lt;/a&gt;表示只接受内部访问，不接收外部http请求，这是为了安全考虑，当然如果需要通过http协议开放外部访问，可以去掉internal然后使用&lt;a href=&quot;http://wiki.nginx.org/NginxHttpAccessModule#deny&quot; target=&quot;_blank&quot;&gt;deny&lt;/a&gt;和&lt;a href=&quot;http://wiki.nginx.org/NginxHttpAccessModule#allow&quot; target=&quot;_blank&quot;&gt;allow&lt;/a&gt;指令控制权限。比较重要的是$memc_key这个变量，它表示以什么作为key，这里我们直接使用Nginx内置的$query_string来作为key，$memc_exptime表示缓存失效时间，以秒记。这里统一设为300（5分钟），在实际应用中可以根据具体情况为不同的内容设置不同的过期时间。&lt;/p&gt;
&lt;p&gt;最后我们为“~ \.php$”这个location配置了缓存，这表示所有以“.php”结尾的请求都会结果被缓存，当然这里只是示例需要，实际中一般不会这么配，而是为特定需要缓存的location配置缓存。&lt;/p&gt;
&lt;p&gt;srcache_fetch表示注册一个输入拦截处理器到location，这个配置将在location进入时被执行；而srcache_store表示注册一个输出拦截器到location，当location执行完成并输出时会被执行。注意srcache模块实际可以与任何缓存模块进行配合使用，而不必一定是memc。这里我们以$uri$args作为缓存的key。&lt;/p&gt;
&lt;p&gt;经过上述配置后，相当于对Nginx增加了如下逻辑：当所请求的uri以“.php”结尾时，首先到memcache中查询有没有以$uri$args为key的数据，如果有则直接返回；否则，执行location的逻辑，如果返回的http状态码为200，则在输出前以$uri$args为key，将输入结果存入memcache。&lt;/p&gt;
&lt;h2&gt;更多配置&lt;/h2&gt;
&lt;p&gt;上一节给出了使用memc和srcache构建缓存层的最基本方法，实际应用中可能需要更多灵活的配置，例如为不同的location配置不同的缓存参数，根据返回内容而不是返回的http状态码确定是否缓存等等。可以有很多的方法实现这些需求，例如，srcache还支持两个指令：srcache_fetch_skip和srcache_fetch_skip，这两个指令接受一个参数，当参数已定义且非0时，则进行相应操作，否则不进行。例如，如果配置了srcache_fetch_skip $skip，这条指令，那么只有当$skip的值为非0时，才将结果缓存，如果配合&lt;a href=&quot;http://wiki.nginx.org/HttpLuaModule&quot; target=&quot;_blank&quot;&gt;ngx_lua&lt;/a&gt;模块的&lt;a href=&quot;http://wiki.nginx.org/HttpLuaModule#set_by_lua&quot; target=&quot;_blank&quot;&gt;set_by_lua&lt;/a&gt;指令，则可以实现复杂的缓存控制。如：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;location /xxxx {
    set $key ...;
    set_by_lua $skip '
        if ngx.var.cookie_foo == &quot;bar&quot; then
            return 1
        end
        return 0
    ';
    srcache_fetch_skip $skip;
    srcache_store_skip $skip;
    srcache_fetch GET /memc $key;
    srcache_store GET /memc $key;
    # proxy_pass/fastcgi_pass/...
}&lt;/pre&gt;
&lt;p&gt;这表示对/xxxx这个location的访问，只有存在cookie “foo”且值为“bar”时缓存机制才起作用。关于ngx_lua的更多内容请参考其&lt;a href=&quot;http://wiki.nginx.org/HttpLuaModule&quot; target=&quot;_blank&quot;&gt;主页&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;另外，我最近在春哥（章亦春在淘宝的昵称）的微博上看到他目前正在完善srcache的功能，为其实现更多&lt;a href=&quot;http://tools.ietf.org/pdf/rfc2616.pdf&quot; target=&quot;_blank&quot;&gt;RFC2616&lt;/a&gt;的缓存行为标准。关于这个模块的最新动态可以关注其&lt;a href=&quot;https://github.com/agentzh/srcache-nginx-module&quot; target=&quot;_blank&quot;&gt;github&lt;/a&gt;主页。&lt;/p&gt;
&lt;h1&gt;Benchmark&lt;/h1&gt;
&lt;p&gt;下面对使用memc和srcache构建的缓存机制进行一个简单的benchmark，并与使用PHP操作memcache的策略进行一个对比。为了简单起见，我们的测试PHP脚本不去访问I/O，而仅仅是调用phpinfo函数输出PHP相关信息。&lt;/p&gt;
&lt;p&gt;测试一共分三组进行：第一组在Nginx和PHP中均不开启缓存，第二组仅使用PHP memcache缓存，第三组仅使用Nginx memcache缓存。三组都用&lt;a href=&quot;http://httpd.apache.org/docs/2.0/programs/ab.html&quot; target=&quot;_blank&quot;&gt;ab&lt;/a&gt;程序去压，并发数为20，请求次数为10000。&lt;/p&gt;
&lt;p&gt;这里的测试环境是我的一个虚拟机，操作系统为Ubuntu10，内存512M。Nginx采用epoll，单worker进程，memcache最大并发数为1024，最大使用内存64m。&lt;/p&gt;
&lt;h2&gt;不开启缓存&lt;/h2&gt;
&lt;p&gt;这一组我们不开启缓存，PHP程序非常简单：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;&lt;?php
phpinfo();
?&gt;&lt;/pre&gt;
&lt;p&gt;测试结果如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/nginx-memc-and-srcache/4.png&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;PHP memcache缓存策略&lt;/h2&gt;
&lt;p&gt;第二组我们用PHP操作缓存，测试脚本为：&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;&lt;?php
$memc = new Memcached;
$memc-&gt;addServer('localhost', 11211) or die('Connect to memcache server failed!');
$output = $memc-&gt;get('my_key');
if(empty($output)) {
    ob_start();
    phpinfo();
    $output = ob_get_contents();
    ob_end_clean();
    $memc-&gt;set('my_key', $output, 300);
}
echo $output; 
?&gt;&lt;/pre&gt;
&lt;p&gt;测试结果如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/nginx-memc-and-srcache/5.png&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;Nginx memcache缓存策略&lt;/h2&gt;
&lt;p&gt;最后，我们将PHP脚本回归到不使用缓存的版本，并配置好memc和srcache缓存机制。测试结果如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/nginx-memc-and-srcache/6.png&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;结果对比分析&lt;/h2&gt;
&lt;p&gt;为了直观，我取“每秒处理请求数”、“平均每个请求处理时间”和“吞吐率”作为评价指标，制作了一张图表。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/nginx-memc-and-srcache/7.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;我想看到图表，结论已毋需我多言。在各项指标上使用memc和srcache构建的缓存机制都大大优于使用PHP操作memcache。其中每秒处理请求数（并发度）和吞吐率都是其9倍左右，而平均个请求所用时间仅有传统策略的1/8。&lt;/p&gt;
&lt;p&gt;这里要特别说明一下，这里之所以PHP memcache策略比不使用缓存优势不明显，是因为我们的PHP脚本不涉及I/O操作，如果其中存在如数据库存取，PHP memcache的优势还是有的，但不论如何，Nginx memcache策略在性能上的优势是其无法比拟的。&lt;/p&gt;
&lt;p&gt;另外，除了性能优势外，使用这种策略还可以简化PHP逻辑，因为缓存这一层都放在Nginx中了，PHP就从缓存操作中解放了出来，因此是一举多得。&lt;/p&gt;
&lt;p&gt;如果你的系统也构建在LNMP上（或LAMP）上，不妨使用本文提到的方法替代传统的缓存策略，尽情享受性能上的提升。&lt;/p&gt;
</description>
</item>

    </channel>
</rss>
